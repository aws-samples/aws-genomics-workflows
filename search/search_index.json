{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Genomics Workflows on AWS DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Introduction Welcome! This guide walks through how to use Amazon Web Services ( AWS ), such as Amazon S3 and AWS Batch , to run large scale genomics analyses. Here you will learn how to: Use S3 buckets to stage large genomics datasets as inputs and outputs from analysis pipelines Create job queues in AWS Batch to use for scalable parallel job execution Orchestrate individual jobs into analysis workflows using native AWS services like AWS Step Functions and 3rd party workflow engines If you're impatient and want to get something up and running immediately, head straight to the Quick Start section. Otherwise, continue on for the full details. Prerequisites Throughout this guide we'll assume that you: Are familiar with the Linux command line Can use SSH to access a Linux server Have access to an AWS account If you are completely new to AWS, we highly recommend going through the following AWS 10-Minute Tutorials that will demonstrate the basics of AWS, as well as set up your development machine for working with AWS. Launch a Linux Virtual Machine - A tutorial which walks users through the process of starting a host on AWS, and configuring your own computer to connect over SSH. Batch upload files to the cloud - A tutorial on using the AWS Command Line Interface (CLI) to access Amazon S3. AWS Account Access AWS has many services that can be used for genomics. Here, we will build core architecture with AWS Batch , a managed service that is built on top of other AWS services, such as Amazon EC2 and Amazon Elastic Container Service (ECS) . Along the way, we'll leverage some advanced capabilities that need escalated (administrative) privileges to implement. For example, you will need to be able to create Roles via AWS Identity and Access Management (IAM) , a service that helps you control who is authenticated (signed in) and authorized (has permissions) to use AWS resources. Tip We strongly recommend following the IAM Security Best Practices for securing your root AWS account and IAM users. Note If you are using an institutional account, it is likely you do not have administrative privileges, i.e. the IAM AdministratorAccess managed policy is not attached to your IAM User or Role, and you won't be able to attach it yourself. If this is the case, you will need to work with your account administrator to get things set up for you. Refer them to this guide, and have them provide you with an AWS Batch Job Queue ARN , and an Amazon S3 Bucket that you can write results to. Contribution This site is a living document, created for and by the genomics community at AWS and around the world. We encourage you to contribute new content and make improvements to existing content via pull request to the GitHub repo that hosts the source code for this site.","title":"Overview"},{"location":"index.html#genomics-workflows-on-aws","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows.","title":"Genomics Workflows on AWS"},{"location":"index.html#introduction","text":"Welcome! This guide walks through how to use Amazon Web Services ( AWS ), such as Amazon S3 and AWS Batch , to run large scale genomics analyses. Here you will learn how to: Use S3 buckets to stage large genomics datasets as inputs and outputs from analysis pipelines Create job queues in AWS Batch to use for scalable parallel job execution Orchestrate individual jobs into analysis workflows using native AWS services like AWS Step Functions and 3rd party workflow engines If you're impatient and want to get something up and running immediately, head straight to the Quick Start section. Otherwise, continue on for the full details.","title":"Introduction"},{"location":"index.html#prerequisites","text":"Throughout this guide we'll assume that you: Are familiar with the Linux command line Can use SSH to access a Linux server Have access to an AWS account If you are completely new to AWS, we highly recommend going through the following AWS 10-Minute Tutorials that will demonstrate the basics of AWS, as well as set up your development machine for working with AWS. Launch a Linux Virtual Machine - A tutorial which walks users through the process of starting a host on AWS, and configuring your own computer to connect over SSH. Batch upload files to the cloud - A tutorial on using the AWS Command Line Interface (CLI) to access Amazon S3.","title":"Prerequisites"},{"location":"index.html#aws-account-access","text":"AWS has many services that can be used for genomics. Here, we will build core architecture with AWS Batch , a managed service that is built on top of other AWS services, such as Amazon EC2 and Amazon Elastic Container Service (ECS) . Along the way, we'll leverage some advanced capabilities that need escalated (administrative) privileges to implement. For example, you will need to be able to create Roles via AWS Identity and Access Management (IAM) , a service that helps you control who is authenticated (signed in) and authorized (has permissions) to use AWS resources. Tip We strongly recommend following the IAM Security Best Practices for securing your root AWS account and IAM users. Note If you are using an institutional account, it is likely you do not have administrative privileges, i.e. the IAM AdministratorAccess managed policy is not attached to your IAM User or Role, and you won't be able to attach it yourself. If this is the case, you will need to work with your account administrator to get things set up for you. Refer them to this guide, and have them provide you with an AWS Batch Job Queue ARN , and an Amazon S3 Bucket that you can write results to.","title":"AWS Account Access"},{"location":"index.html#contribution","text":"This site is a living document, created for and by the genomics community at AWS and around the world. We encourage you to contribute new content and make improvements to existing content via pull request to the GitHub repo that hosts the source code for this site.","title":"Contribution"},{"location":"disclaimer.html","text":"Disclaimer DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The architectures and solutions presented in this guide are provided \"as is\" per the underlying LICENSE . Before implementing anything described here in a production setting we recommended that you consult with your AWS account team regarding your specific requirements for performance, scalability, and security via a Well Architected Review .","title":"Disclaimer"},{"location":"disclaimer.html#disclaimer","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The architectures and solutions presented in this guide are provided \"as is\" per the underlying LICENSE . Before implementing anything described here in a production setting we recommended that you consult with your AWS account team regarding your specific requirements for performance, scalability, and security via a Well Architected Review .","title":"Disclaimer"},{"location":"quick-start.html","text":"Quick Start - For rapid deployment DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Step 0: Amazon VPC While you can use an existing \"default\" VPC to implement deployment of your genomics environment, we strongly recommend utilizing a VPC with private subnets for processing sensitive data with AWS Batch. Doing so will restrict access to the instances from the internet, and help meet security and compliance requirements, such as dbGaP . NOTE , these private subnets must have a route to the secure route to the internet. A typical method would be to use a NAT Gateway although other options are possible. Tip You may also want to review the HIPAA on AWS Enterprise Accelerator and the AWS Biotech Blueprint for additional security best practices such as: Basic AWS Identity and Access Management (IAM) configuration with custom (IAM) policies, with associated groups, roles, and instance profiles Standard, external-facing Amazon Virtual Private Cloud (Amazon VPC) Multi-AZ architecture with separate subnets for different application tiers and private (back-end) subnets for application and database Amazon Simple Storage Service (Amazon S3) buckets for encrypted web content, logging, and backup data Standard Amazon VPC security groups for Amazon Elastic Compute Cloud (Amazon EC2) instances and load balancers used in the sample application stack A secured bastion login host to facilitate command-line Secure Shell (SSH) access to Amazon EC2 instances for troubleshooting and systems administration activities Logging, monitoring, and alerts using AWS CloudTrail, Amazon CloudWatch, and AWS Config rules Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow The above template uses the AWS Quickstart reference for a Modular and Scalable VPC Architecture and provides a networking foundation for AWS Cloud infrastructures, deploying an Amazon Virtual Private Cloud (Amazon VPC) according to AWS best-practices and guidelines. For architectural details, best practices, step-by-step instructions, and customization options, see the deployment guide . The VPC quick start template will deploy a VPC with 2 private subnets (the minimum recommended for the core environment) in two Availability Zones (AZs). For production environments we recommend using as many AZs as are available in your region. This allows the AWS Batch compute environments to source workers from more AZs potentially resulting in better pricing and fewer interruptions when using Spot Instances. A simple way to create a CloudFormation template for a complete VPC stack with multiple AZs is to use the AWS CDK . This example Java app will synthesize a CloudFormation template that can be used to generate a VPC with subnets in upto 6 AZs (or the maximum for the region if there are less than 6): public class CdkVpcApp { public static void main(final String[] args) { App app = new App(); Environment env = Environment .builder() .account(\"my-account-number\") .region(\"us-east-1\") .build(); new CdkVpcStack(app, \"CdkVpcStack\", StackProps.builder().env(cromwell).build()); app.synth(); } } public class CdkVpcStack extends Stack { public CdkVpcStack(final Construct scope, final String id) { this(scope, id, null); } public CdkVpcStack(final Construct scope, final String id, final StackProps props) { super(scope, id, props); // The code that defines your stack goes here Vpc vpc = Vpc.Builder.create(this, \"vpc\") .maxAzs(6) .build(); } } For a full set of options check out the VPC section of the CDK API Guide Step 1: Core Environment The CloudFormation template below will create all the AWS resources required - S3 Bucket, EC2 Launch Templates, IAM Roles, Batch Compute Environments, Batch Job Queues - needed for a genomics workflow core execution environment. It should be deployed into an existing VPC. You can use your Default VPC for testing. For production, you should use (or create) a VPC with at least two private subnets. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow When launching the stack you can supply a Namespace as an optional parameter. The core can be installed multiple times in your account if needed (e.g. for use by different projects) grouped by the provided Namespace . By default, the Namespace is set to the stack name, which must be unique within an AWS region. Prior to the final create button, be sure to acknowledge \"IAM CAPABILITIES\". The template will take about 15-20 minutes to finish creating resources. Step 2: Worklow Orchestrators The CloudFormation templates below will create resources specific to a workflow orchestrator. All assume that you have already installed the core environment described above. When launching these stacks, you must provide a Namespace parameter. This is used to associate a workflow orchestrator stack to a specific core environment. Multiple workflow orchestrators can share a single core environment. Name Description Source Launch Stack AWS Step Functions Create a Step Functions State Machine, Batch Job Definitions, and container images to run an example genomics workflow cloud_download play_arrow Cromwell Create resources needed to run Cromwell on AWS: an RDS Aurora database, an EC2 instance with Cromwell installed as a server, and an IAM instance profile cloud_download play_arrow Nextflow Create resources needed to run Nextflow on AWS: an S3 Bucket for nextflow logs and metadata, AWS Batch Job Definition for a Nextflow head node, and an IAM role for the nextflow head node job cloud_download play_arrow","title":"Quick Start"},{"location":"quick-start.html#quick-start-for-rapid-deployment","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows.","title":"Quick Start - For rapid deployment"},{"location":"quick-start.html#step-0-amazon-vpc","text":"While you can use an existing \"default\" VPC to implement deployment of your genomics environment, we strongly recommend utilizing a VPC with private subnets for processing sensitive data with AWS Batch. Doing so will restrict access to the instances from the internet, and help meet security and compliance requirements, such as dbGaP . NOTE , these private subnets must have a route to the secure route to the internet. A typical method would be to use a NAT Gateway although other options are possible. Tip You may also want to review the HIPAA on AWS Enterprise Accelerator and the AWS Biotech Blueprint for additional security best practices such as: Basic AWS Identity and Access Management (IAM) configuration with custom (IAM) policies, with associated groups, roles, and instance profiles Standard, external-facing Amazon Virtual Private Cloud (Amazon VPC) Multi-AZ architecture with separate subnets for different application tiers and private (back-end) subnets for application and database Amazon Simple Storage Service (Amazon S3) buckets for encrypted web content, logging, and backup data Standard Amazon VPC security groups for Amazon Elastic Compute Cloud (Amazon EC2) instances and load balancers used in the sample application stack A secured bastion login host to facilitate command-line Secure Shell (SSH) access to Amazon EC2 instances for troubleshooting and systems administration activities Logging, monitoring, and alerts using AWS CloudTrail, Amazon CloudWatch, and AWS Config rules Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow The above template uses the AWS Quickstart reference for a Modular and Scalable VPC Architecture and provides a networking foundation for AWS Cloud infrastructures, deploying an Amazon Virtual Private Cloud (Amazon VPC) according to AWS best-practices and guidelines. For architectural details, best practices, step-by-step instructions, and customization options, see the deployment guide . The VPC quick start template will deploy a VPC with 2 private subnets (the minimum recommended for the core environment) in two Availability Zones (AZs). For production environments we recommend using as many AZs as are available in your region. This allows the AWS Batch compute environments to source workers from more AZs potentially resulting in better pricing and fewer interruptions when using Spot Instances. A simple way to create a CloudFormation template for a complete VPC stack with multiple AZs is to use the AWS CDK . This example Java app will synthesize a CloudFormation template that can be used to generate a VPC with subnets in upto 6 AZs (or the maximum for the region if there are less than 6): public class CdkVpcApp { public static void main(final String[] args) { App app = new App(); Environment env = Environment .builder() .account(\"my-account-number\") .region(\"us-east-1\") .build(); new CdkVpcStack(app, \"CdkVpcStack\", StackProps.builder().env(cromwell).build()); app.synth(); } } public class CdkVpcStack extends Stack { public CdkVpcStack(final Construct scope, final String id) { this(scope, id, null); } public CdkVpcStack(final Construct scope, final String id, final StackProps props) { super(scope, id, props); // The code that defines your stack goes here Vpc vpc = Vpc.Builder.create(this, \"vpc\") .maxAzs(6) .build(); } } For a full set of options check out the VPC section of the CDK API Guide","title":"Step 0: Amazon VPC"},{"location":"quick-start.html#step-1-core-environment","text":"The CloudFormation template below will create all the AWS resources required - S3 Bucket, EC2 Launch Templates, IAM Roles, Batch Compute Environments, Batch Job Queues - needed for a genomics workflow core execution environment. It should be deployed into an existing VPC. You can use your Default VPC for testing. For production, you should use (or create) a VPC with at least two private subnets. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow When launching the stack you can supply a Namespace as an optional parameter. The core can be installed multiple times in your account if needed (e.g. for use by different projects) grouped by the provided Namespace . By default, the Namespace is set to the stack name, which must be unique within an AWS region. Prior to the final create button, be sure to acknowledge \"IAM CAPABILITIES\". The template will take about 15-20 minutes to finish creating resources.","title":"Step 1: Core Environment"},{"location":"quick-start.html#step-2-worklow-orchestrators","text":"The CloudFormation templates below will create resources specific to a workflow orchestrator. All assume that you have already installed the core environment described above. When launching these stacks, you must provide a Namespace parameter. This is used to associate a workflow orchestrator stack to a specific core environment. Multiple workflow orchestrators can share a single core environment. Name Description Source Launch Stack AWS Step Functions Create a Step Functions State Machine, Batch Job Definitions, and container images to run an example genomics workflow cloud_download play_arrow Cromwell Create resources needed to run Cromwell on AWS: an RDS Aurora database, an EC2 instance with Cromwell installed as a server, and an IAM instance profile cloud_download play_arrow Nextflow Create resources needed to run Nextflow on AWS: an S3 Bucket for nextflow logs and metadata, AWS Batch Job Definition for a Nextflow head node, and an IAM role for the nextflow head node job cloud_download play_arrow","title":"Step 2: Worklow Orchestrators"},{"location":"containers/container-examples.html","text":"","title":"Container examples"},{"location":"containers/container-introduction.html","text":"","title":"Container introduction"},{"location":"core-env/build-custom-distribution.html","text":"Building Custom Resources DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. This section describes how to build and upload templates and artifacts to use in a customized deployment. Once uploaded, the locations of the templates and artifacts are used when deploying the Nextflow on AWS Batch solution (see Customized Deployment ) Building a Custom Distribution This step involves building a distribution of templates and artifacts from the solution's source code. First, create a local clone of the Genomics Workflows on AWS source code. The code base contains several directories: _scripts/ : Shell scripts for building and uploading the customized distribution of templates and artifacts docs/ : Source code for the documentation, written in MarkDown for the MkDocs publishing platform. This documentation may be modified, expanded, and contributed in the same way as source code. src/ : Source code for the components of the solution: containers/ : CodeBuild buildspec files for building AWS-specific container images and pushing them to ECR _common/ build.sh : A generic build script that first builds a base image for a container, then builds an AWS specific image entrypoint.aws.sh : A generic entrypoint script that wraps a call to a binary tool in the container with handlers data staging from/to S3 nextflow/ Dockerfile nextflow.aws.sh : Docker entrypoint script to execute the Nextflow workflow on AWS Batch ebs-autoscale/ get-amazon-ebs-autoscale.sh : Script to retrieve and install Amazon EBS Autoscale ecs-additions/ : Scripts to be installed on ECS host instances to support the distribution awscli-shim.sh : Installed as /opt/aws-cli/bin/aws and mounted onto the container, allows container images without full glibc to use the AWS CLI v2 through supplied shared libraries (especially libz) and LD_LIBRARY_PATH . ecs-additions-common.sh : Utility script to install fetch_and_run.sh , Nextflow and Cromwell shims, and swap space ecs-additions-cromwell-linux2-worker.sh : ecs-additions-cromwell.sh : ecs-additions-nextflow.sh : ecs-additions-step-functions.sh : fetch_and_run.sh : Uses AWS CLI to download and run scripts and zip files from S3 provision.sh : Appended to the userdata in the launch template created by gwfcore-launch-template : Starts SSM Agent, ECS Agent, Docker; runs get-amazon-ebs-autoscale.sh , ecs-additions-common.sh and orchestrator-specific ecs-additions- scripts. lambda/ : Lambda functions to create, modify or delete ECR registries or CodeBuild jobs templates/ : CloudFormation templates for the solution stack, as described in Customized Deployment Deploying a Custom Distribution The script _scripts/deploy.sh will create a custom distribution of artifacts and templates from files in the source tree, then upload this distribution to an S3 bucket. It will optionally also build and deploy a static documentation site from the Markdown documentation files. Its usage is: deploy.sh [--site-bucket BUCKET] [--asset-bucket BUCKET] [--asset-profile PROFILE] [--deploy-region REGION] [--public] [--verbose] STAGE --site-bucket BUCKET Deploy documentation site to BUCKET --asset-bucket BUCKET Deploy assets to BUCKET --asset-profile PROFILE Use PROFILE for AWS CLI commands --deploy-region REGION Deploy in region REGION --public Deploy to public bucket with '--acl public-read' (Default false) --verbose Display more output STAGE 'test' or 'production' When running this script from the command line, use the value test for the stage. This will deploy the templates and artifacts into a directory test in your deployment bucket: $ aws s3 ls s3://my-deployment-bucket/test/ PRE artifacts/ PRE templates/ Use these values when deploying a customized installation, as described in Customized Deployment , sections 'Artifacts and Nested Stacks' and 'Nextflow'. In the example from above, the values to use would be: Artifact S3 Bucket Name: my-deployment-bucket Artifact S3 Prefix: test/artifacts Template Root URL: https://my-deployment-bucket.s3.amazonaws.com/test/templates The use of production for stage is reserved for deployments from a Travis CI/CD environment; this usage will deploy into a subdirectory named after the current release tag.","title":"Building a Custom Distribution"},{"location":"core-env/build-custom-distribution.html#building-custom-resources","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. This section describes how to build and upload templates and artifacts to use in a customized deployment. Once uploaded, the locations of the templates and artifacts are used when deploying the Nextflow on AWS Batch solution (see Customized Deployment )","title":"Building Custom Resources"},{"location":"core-env/build-custom-distribution.html#building-a-custom-distribution","text":"This step involves building a distribution of templates and artifacts from the solution's source code. First, create a local clone of the Genomics Workflows on AWS source code. The code base contains several directories: _scripts/ : Shell scripts for building and uploading the customized distribution of templates and artifacts docs/ : Source code for the documentation, written in MarkDown for the MkDocs publishing platform. This documentation may be modified, expanded, and contributed in the same way as source code. src/ : Source code for the components of the solution: containers/ : CodeBuild buildspec files for building AWS-specific container images and pushing them to ECR _common/ build.sh : A generic build script that first builds a base image for a container, then builds an AWS specific image entrypoint.aws.sh : A generic entrypoint script that wraps a call to a binary tool in the container with handlers data staging from/to S3 nextflow/ Dockerfile nextflow.aws.sh : Docker entrypoint script to execute the Nextflow workflow on AWS Batch ebs-autoscale/ get-amazon-ebs-autoscale.sh : Script to retrieve and install Amazon EBS Autoscale ecs-additions/ : Scripts to be installed on ECS host instances to support the distribution awscli-shim.sh : Installed as /opt/aws-cli/bin/aws and mounted onto the container, allows container images without full glibc to use the AWS CLI v2 through supplied shared libraries (especially libz) and LD_LIBRARY_PATH . ecs-additions-common.sh : Utility script to install fetch_and_run.sh , Nextflow and Cromwell shims, and swap space ecs-additions-cromwell-linux2-worker.sh : ecs-additions-cromwell.sh : ecs-additions-nextflow.sh : ecs-additions-step-functions.sh : fetch_and_run.sh : Uses AWS CLI to download and run scripts and zip files from S3 provision.sh : Appended to the userdata in the launch template created by gwfcore-launch-template : Starts SSM Agent, ECS Agent, Docker; runs get-amazon-ebs-autoscale.sh , ecs-additions-common.sh and orchestrator-specific ecs-additions- scripts. lambda/ : Lambda functions to create, modify or delete ECR registries or CodeBuild jobs templates/ : CloudFormation templates for the solution stack, as described in Customized Deployment","title":"Building a Custom Distribution"},{"location":"core-env/build-custom-distribution.html#deploying-a-custom-distribution","text":"The script _scripts/deploy.sh will create a custom distribution of artifacts and templates from files in the source tree, then upload this distribution to an S3 bucket. It will optionally also build and deploy a static documentation site from the Markdown documentation files. Its usage is: deploy.sh [--site-bucket BUCKET] [--asset-bucket BUCKET] [--asset-profile PROFILE] [--deploy-region REGION] [--public] [--verbose] STAGE --site-bucket BUCKET Deploy documentation site to BUCKET --asset-bucket BUCKET Deploy assets to BUCKET --asset-profile PROFILE Use PROFILE for AWS CLI commands --deploy-region REGION Deploy in region REGION --public Deploy to public bucket with '--acl public-read' (Default false) --verbose Display more output STAGE 'test' or 'production' When running this script from the command line, use the value test for the stage. This will deploy the templates and artifacts into a directory test in your deployment bucket: $ aws s3 ls s3://my-deployment-bucket/test/ PRE artifacts/ PRE templates/ Use these values when deploying a customized installation, as described in Customized Deployment , sections 'Artifacts and Nested Stacks' and 'Nextflow'. In the example from above, the values to use would be: Artifact S3 Bucket Name: my-deployment-bucket Artifact S3 Prefix: test/artifacts Template Root URL: https://my-deployment-bucket.s3.amazonaws.com/test/templates The use of production for stage is reserved for deployments from a Travis CI/CD environment; this usage will deploy into a subdirectory named after the current release tag.","title":"Deploying a Custom Distribution"},{"location":"core-env/create-custom-compute-resources.html","text":"Core: Custom Compute Resources DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Genomics is a data-heavy workload and requires some modification to the defaults used by AWS Batch for job processing. To efficiently use resources, AWS Batch places multiple jobs on an worker instance. The data requirements for individual jobs can range from a few MB to 100s of GB. Instances running workflow jobs will not know beforehand how much space is required, and need scalable storage to meet unpredictable runtime demands. To handle this use case, we can use a process that monitors a mountpoint on an instance and expands free space as needed based on capacity thresholds. This can be done using logical volume management and attaching EBS volumes as needed to the instance like so: The above process - \"EBS autoscaling\" - requires a few small dependencies and a simple daemon installed on the host instance. By default, AWS Batch uses the Amazon ECS-Optimized AMI to launch instances for running jobs. This is sufficient in most cases, but specialized needs, such as the large storage requirements noted above, require customization of the base AMI. Because the provisioning requirements for EBS autoscaling are fairly simple and light weight, one can use an EC2 Launch Template to customize instances. EC2 Launch Template The simplest method for customizing an instance is to use an EC2 Launch Template. This works best if your customizations are relatively light - such as installing a few small utilities or making specific configuration changes. This is because Launch Templates run a UserData script when an instance first launches. The longer these scripts / customizations take to complete, the longer it will be before your instance is ready for work. Launch Templates are capable of pre-configuring a lot of EC2 instance options. Since this will be working with AWS Batch, which already does a lot of automatic instance configuration on its own, you only need to supply a UserData script like the one below: MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==BOUNDARY==\" --==BOUNDARY== Content-Type: text/cloud-config; charset=\"us-ascii\" packages: - jq - btrfs-progs - sed - wget - unzip # add more package names here if you need them runcmd: - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" - unzip -q /tmp/awscliv2.zip -d /tmp - /tmp/aws/install ### # add provisioning commands here ### --==BOUNDARY==-- Info The MIME boundaries are required in the UserData script if the Launch Template is to be used with AWS Batch. See AWS Batch's Launch Template Support documentation to learn more. In the set of provisioning commands you can add steps that retrieve and install Amazon EBS Autoscale . For example, this documentation comes with a tarball that is publicly hosted on S3, allowing you to run the following to provision your instances: cd /opt && wget https://aws-genomics-workflows.s3.amazonaws.com/artifacts/amazon-ebs-autoscale.tgz && tar -xzf amazon-ebs-autoscale.tgz sh /opt/ebs-autoscale/install.sh The above will install an ebs-autoscale daemon on an instance. By default it will add a 100GB EBS volume to the logical volume mounted at /scratch . The mount point is specific to your use case, and /scratch is considered a generic default. For instances launched by AWS Batch for running containerized jobs, a good option is to apply Amazon EBS Autoscaling to /var/lib/docker - the location where Docker stores container volumes and metadata and is the location that containers use for their filesystems. Making this auto-expand allows containers to pull in any amount of data they need. Flexible provisioning Provisioning needs can change over time. For instance, updrades to existing software or swapping out tools altogether. While you could explicitly write all the commands you initially need in UserData , any future changes will require creating a new revision of the Launch Template and subsequently require rebuilding AWS Batch resources. For more flexibility, you can store your provisioning scripts on S3 and have the Launch Template retrieve and run them. This allows you to create a generic and relatively immutable Launch Template, and update provisioning steps by uploading new scripts to S3. You can automate this further using a Git repository like AWS CodeCommit to track versions of your provisioning scripts and use CI/CD pipelines with AWS CodeBuild and AWS CodePipeline to deploy new releases to S3. All of the above can be create in an automated fashion with CloudFormation. Look at the source code for the template below to see how this is done. Name Description Source Launch Stack Provisioning Code Creates and installs code and artifacts used to run subsequent templates and provision EC2 instances cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Creating an EC2 Launch Template Instructions on how to create a launch template are below. Once your Launch Template is created, you can reference it when you setup resources in AWS Batch to ensure that jobs run therein have your customizations available to them. Automated via CloudFormation You can use the following CloudFormation template to create a Launch Template suitable for your needs. Name Description Source Launch Stack EC2 Launch Template Creates an EC2 Launch Template that provisions instances on first boot for processing genomics workflow tasks. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Manually via the AWS Console Go to the EC2 Console Click on \"Launch Templates\" (under \"Instances\") Click on \"Create launch template\" Under \"Launch template name and description\" Use \"genomics-workflow-template\" for \"Launch template name\" Under \"Storage volumes\" Click \"Add new volume\" - this will add an entry called \"Volume 3 (custom)\" Set Size to 100 GiB Set Delete on termination to Yes Set Device name to /dev/xvdba Set Volume type to General purpose SSD (gp2) Set Encrypted to Yes Info Volume 1 is used for the root filesystem. The default size of 8GB is typically sufficient. Volume 2 is the default volume used by Amazon ECS Optimized AMIs for Docker image and metadata volume using Docker's devicemapper driver. We'll be replacing this volume with the custom one you created, but need to keep it for compatibility. Volume 3 (the one you created above) will be used for job scratch space. This will be mapped to /var/lib/docker which is used for container storage - i.e. what each running container will use to create its internal filesystem. Expand the \"Advanced details\" section Add the following script to User data MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==BOUNDARY==\" --==BOUNDARY== Content-Type: text/cloud-config; charset=\"us-ascii\" packages: - jq - btrfs-progs - sed - wget - unzip # add more package names here if you need them runcmd: - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" - unzip -q /tmp/awscliv2.zip -d /tmp - /tmp/aws/install ### # add provisioning commands here ### --==BOUNDARY==-- Important You will need to replace # add provisioning commands here with something appropriate Click on \"Create launch template\" Custom AMIs A slightly more involved method for customizing an instance is to create a new AMI based on the ECS Optimized AMI. This is good if you have a lot of customization to do - lots of software to install and/or need large datasets preloaded that will be needed by all your jobs. You can learn more about how to create your own AMIs in the EC2 userguide . If needed, you can use Custom AMIs and Lauch Templates together - e.g. for a case where you need to ship preloaded datesets or use AMI packaged software from the AWS Marketplace and use the flexible provisioning options Launch Templates provide. Note This is considered advanced use. All documentation and CloudFormation templates hereon assumes use of EC2 Launch Templates.","title":"Compute Resources"},{"location":"core-env/create-custom-compute-resources.html#core-custom-compute-resources","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Genomics is a data-heavy workload and requires some modification to the defaults used by AWS Batch for job processing. To efficiently use resources, AWS Batch places multiple jobs on an worker instance. The data requirements for individual jobs can range from a few MB to 100s of GB. Instances running workflow jobs will not know beforehand how much space is required, and need scalable storage to meet unpredictable runtime demands. To handle this use case, we can use a process that monitors a mountpoint on an instance and expands free space as needed based on capacity thresholds. This can be done using logical volume management and attaching EBS volumes as needed to the instance like so: The above process - \"EBS autoscaling\" - requires a few small dependencies and a simple daemon installed on the host instance. By default, AWS Batch uses the Amazon ECS-Optimized AMI to launch instances for running jobs. This is sufficient in most cases, but specialized needs, such as the large storage requirements noted above, require customization of the base AMI. Because the provisioning requirements for EBS autoscaling are fairly simple and light weight, one can use an EC2 Launch Template to customize instances.","title":"Core: Custom Compute Resources"},{"location":"core-env/create-custom-compute-resources.html#ec2-launch-template","text":"The simplest method for customizing an instance is to use an EC2 Launch Template. This works best if your customizations are relatively light - such as installing a few small utilities or making specific configuration changes. This is because Launch Templates run a UserData script when an instance first launches. The longer these scripts / customizations take to complete, the longer it will be before your instance is ready for work. Launch Templates are capable of pre-configuring a lot of EC2 instance options. Since this will be working with AWS Batch, which already does a lot of automatic instance configuration on its own, you only need to supply a UserData script like the one below: MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==BOUNDARY==\" --==BOUNDARY== Content-Type: text/cloud-config; charset=\"us-ascii\" packages: - jq - btrfs-progs - sed - wget - unzip # add more package names here if you need them runcmd: - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" - unzip -q /tmp/awscliv2.zip -d /tmp - /tmp/aws/install ### # add provisioning commands here ### --==BOUNDARY==-- Info The MIME boundaries are required in the UserData script if the Launch Template is to be used with AWS Batch. See AWS Batch's Launch Template Support documentation to learn more. In the set of provisioning commands you can add steps that retrieve and install Amazon EBS Autoscale . For example, this documentation comes with a tarball that is publicly hosted on S3, allowing you to run the following to provision your instances: cd /opt && wget https://aws-genomics-workflows.s3.amazonaws.com/artifacts/amazon-ebs-autoscale.tgz && tar -xzf amazon-ebs-autoscale.tgz sh /opt/ebs-autoscale/install.sh The above will install an ebs-autoscale daemon on an instance. By default it will add a 100GB EBS volume to the logical volume mounted at /scratch . The mount point is specific to your use case, and /scratch is considered a generic default. For instances launched by AWS Batch for running containerized jobs, a good option is to apply Amazon EBS Autoscaling to /var/lib/docker - the location where Docker stores container volumes and metadata and is the location that containers use for their filesystems. Making this auto-expand allows containers to pull in any amount of data they need.","title":"EC2 Launch Template"},{"location":"core-env/create-custom-compute-resources.html#flexible-provisioning","text":"Provisioning needs can change over time. For instance, updrades to existing software or swapping out tools altogether. While you could explicitly write all the commands you initially need in UserData , any future changes will require creating a new revision of the Launch Template and subsequently require rebuilding AWS Batch resources. For more flexibility, you can store your provisioning scripts on S3 and have the Launch Template retrieve and run them. This allows you to create a generic and relatively immutable Launch Template, and update provisioning steps by uploading new scripts to S3. You can automate this further using a Git repository like AWS CodeCommit to track versions of your provisioning scripts and use CI/CD pipelines with AWS CodeBuild and AWS CodePipeline to deploy new releases to S3. All of the above can be create in an automated fashion with CloudFormation. Look at the source code for the template below to see how this is done. Name Description Source Launch Stack Provisioning Code Creates and installs code and artifacts used to run subsequent templates and provision EC2 instances cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack.","title":"Flexible provisioning"},{"location":"core-env/create-custom-compute-resources.html#creating-an-ec2-launch-template","text":"Instructions on how to create a launch template are below. Once your Launch Template is created, you can reference it when you setup resources in AWS Batch to ensure that jobs run therein have your customizations available to them.","title":"Creating an EC2 Launch Template"},{"location":"core-env/create-custom-compute-resources.html#automated-via-cloudformation","text":"You can use the following CloudFormation template to create a Launch Template suitable for your needs. Name Description Source Launch Stack EC2 Launch Template Creates an EC2 Launch Template that provisions instances on first boot for processing genomics workflow tasks. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack.","title":"Automated via CloudFormation"},{"location":"core-env/create-custom-compute-resources.html#manually-via-the-aws-console","text":"Go to the EC2 Console Click on \"Launch Templates\" (under \"Instances\") Click on \"Create launch template\" Under \"Launch template name and description\" Use \"genomics-workflow-template\" for \"Launch template name\" Under \"Storage volumes\" Click \"Add new volume\" - this will add an entry called \"Volume 3 (custom)\" Set Size to 100 GiB Set Delete on termination to Yes Set Device name to /dev/xvdba Set Volume type to General purpose SSD (gp2) Set Encrypted to Yes Info Volume 1 is used for the root filesystem. The default size of 8GB is typically sufficient. Volume 2 is the default volume used by Amazon ECS Optimized AMIs for Docker image and metadata volume using Docker's devicemapper driver. We'll be replacing this volume with the custom one you created, but need to keep it for compatibility. Volume 3 (the one you created above) will be used for job scratch space. This will be mapped to /var/lib/docker which is used for container storage - i.e. what each running container will use to create its internal filesystem. Expand the \"Advanced details\" section Add the following script to User data MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\"==BOUNDARY==\" --==BOUNDARY== Content-Type: text/cloud-config; charset=\"us-ascii\" packages: - jq - btrfs-progs - sed - wget - unzip # add more package names here if you need them runcmd: - curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" - unzip -q /tmp/awscliv2.zip -d /tmp - /tmp/aws/install ### # add provisioning commands here ### --==BOUNDARY==-- Important You will need to replace # add provisioning commands here with something appropriate Click on \"Create launch template\"","title":"Manually via the AWS Console"},{"location":"core-env/create-custom-compute-resources.html#custom-amis","text":"A slightly more involved method for customizing an instance is to create a new AMI based on the ECS Optimized AMI. This is good if you have a lot of customization to do - lots of software to install and/or need large datasets preloaded that will be needed by all your jobs. You can learn more about how to create your own AMIs in the EC2 userguide . If needed, you can use Custom AMIs and Lauch Templates together - e.g. for a case where you need to ship preloaded datesets or use AMI packaged software from the AWS Marketplace and use the flexible provisioning options Launch Templates provide. Note This is considered advanced use. All documentation and CloudFormation templates hereon assumes use of EC2 Launch Templates.","title":"Custom AMIs"},{"location":"core-env/create-iam-roles.html","text":"Core: Permissions DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. IAM is used to control access to your AWS resources. This includes access by users and groups in your account, as well as access by AWS services such as AWS Batch operating on your behalf. Services use IAM Roles which provide temporary access to AWS resources when needed. IMPORTANT You need to have Administrative access to your AWS account to make changes in IAM. A recommended way to do this is to create a user and add that user to a group with the AdministratorAccess managed policy attached. This makes it easier to revoke these privileges if necessary. Create IAM Resources IAM Policies For the EC2 instance role described in the next section, it is recommended to restrict access to just the resources and permissions it needs to use. In this case, it will be: Access to the specific buckets used for input and output data The ability to create and add EBS volumes to the instance (more on this later) These policies could be used by other roles, so it will be easier to manage them if each are stand alone documents. Bucket Access Policy (required) : This policy specifies full access to a single S3 bucket named <bucket-name> which physically resides in <region> . { \"PolicyName\": \"s3bucket-access-<region>\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::<bucket-name>\", \"arn:aws:s3:::<bucket-name>/*\" ] } ] } } If needed, the policy can be made more granular - i.e. only allowing access to a prefix within the bucket - by modifying the second Resource item to include the prefix path before the * . EBS Autoscale Policy (required) : This policy allows job instance to attach EBS volumes to create extra scratch space for genomic data using Amazon EBS Autoscale . { \"PolicyName\": \"ebs-autoscale-<region>\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:DescribeVolumeStatus\", \"ec2:DescribeVolumes\", \"ec2:ModifyInstanceAttribute\", \"ec2:DescribeVolumeAttribute\", \"ec2:CreateVolume\", \"ec2:DeleteVolume\", \"ec2:CreateTags\" ], \"Resource\": \"*\" } ] } } IAM Roles IAM roles that your job execution environment in AWS Batch will use include: Batch Service Role (required) : Role used by AWS Batch to call other AWS services on its behalf. AWS Batch makes calls to other AWS services on your behalf to manage the resources that you use with the service. Before you can use the service, you must have an IAM policy and role that provides the necessary permissions to AWS Batch. (Learn More) Batch Instance Profile (required) : Role that defines service permissions for EC2 instances launched by AWS Batch. This role should also have attached policies (see above) that allow access to specific S3 buckets and the ability to modify storage (e.g. EBS volumes) on the instance. (Learn More) Batch SpotFleet Role (depends) : This role is needed if you intend to launch spot instances from AWS Batch. If you create a managed compute environment that uses Amazon EC2 Spot Fleet Instances with a BEST_FIT allocation strategy, you must create a role that grants the Spot Fleet permission to set a cost threshold, launch, tag, and terminate instances on your behalf. (Learn More) Batch Job Role (optional) : Role used to provide specific service permissions to individual jobs. Jobs can run without an IAM role. In that case, they inherit the permissions of the instance they run on. Job roles are useful if you have jobs that utilize additional AWS resources such as buckets with supplementary data or need to interact with other AWS services like databases. Automated via CloudFormation The CloudFormation template below creates all of the above roles and policies. Name Description Source Launch Stack Amazon IAM Roles Create the necessary IAM Roles. This is useful to hand to someone with the right permissions to create these on your behalf. You will need to provide a S3 bucket name . cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Administrative Access Required In order run this CloudFormation template you you will need privileged access to your account either through an IAM user, STS assumed role, or CloudFormation Stack role. Manually via the AWS Console Create a bucket access policy Go to the IAM Console Click on \"Policies\" Click on \"Create Policy\" Repeat the following for as many buckets as you will use (e.g. if you have one bucket for nextflow logs and another for nextflow workDir, you will need to do this twice) Select \"S3\" as the service Select \"All Actions\" Under Resources select \"Specific\" Under Resources > bucket, click \"Add ARN\" Type in the name of the bucket Click \"Add\" Under Resources > object, click \"Add ARN\" For \"Bucket Name\", type in the name of the bucket For \"Object Name\", select \"Any\" Click \"Add additional permissions\" if you have additional buckets you are using Click \"Review Policy\" Name the policy \"bucket-access-policy\" Click \"Create Policy\" Create an EBS autoscale policy Go to the IAM Console Click on \"Policies\" Click on \"Create Policy\" Switch to the \"JSON\" tab Paste the following into the editor: { \"Version\": \"2012-10-17\", \"Statement\": { \"Action\": [ \"ec2:*Volume\", \"ec2:modifyInstanceAttribute\", \"ec2:describeVolumes\" ], \"Resource\": \"*\", \"Effect\": \"Allow\" } } Click \"Review Policy\" Name the policy \"ebs-autoscale-policy\" Click \"Create Policy\" Create a Batch Service Role This is a role used by AWS Batch to launch EC2 instances on your behalf. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose \"Batch\" as the service to use the role Click \"Next: Permissions\" In Attached permissions policies, the \"AWSBatchServiceRole\" will already be attached Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"AWSBatchServiceRole\" Click \"Create role\" Create an EC2 Instance Role This is a role that controls what AWS Resources EC2 instances launched by AWS Batch have access to. In this case, you will limit S3 access to just the bucket you created earlier. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose EC2 from the larger services list Choose \"EC2 - Allows EC2 instances to call AWS services on your behalf\" as the use case. Click \"Next: Permissions\" Type \"ContainerService\" in the search field for policies Click the checkbox next to \"AmazonEC2ContainerServiceforEC2Role\" to attach the policy Type \"S3\" in the search field for policies Click the checkbox next to \"AmazonS3ReadOnlyAccess\" to attach the policy Note Enabling Read-Only access to all S3 resources is required if you use publicly available datasets such as the 1000 Genomes dataset , and others, available in the AWS Registry of Open Datasets Type \"bucket-access-policy\" in the search field for policies Click the checkbox next to \"bucket-access-policy\" to attach the policy Type \"ebs-autoscale-policy\" in the search field for policies Click the checkbox next to \"ebs-autoscale-policy\" to attach the policy Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"ecsInstanceRole\" Click \"Create role\" Create an EC2 SpotFleet Role This is a role that allows creation and launch of Spot fleets - Spot instances with similar compute capabilities (i.e. vCPUs and RAM). This is for using Spot instances when running jobs in AWS Batch. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose EC2 from the larger services list Choose \"EC2 - Spot Fleet Tagging\" as the use case In Attached permissions policies, the \"AmazonEC2SpotFleetTaggingRole\" will already be attached Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"AWSSpotFleetTaggingRole\" Click \"Create role\" Create a Job Role This is a role used by individual Batch Jobs to specify permissions to AWS resources in addition to permissions allowed by the Instance Role above. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose Elastic Container Service from the larger services list Choose \"Elastic Container Service Task\" as the use case. Click \"Next: Permissions\" Attach AWS managed and user defined policies as needed. Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"BatchJobRole\" Click \"Create role\"","title":"Permissions"},{"location":"core-env/create-iam-roles.html#core-permissions","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. IAM is used to control access to your AWS resources. This includes access by users and groups in your account, as well as access by AWS services such as AWS Batch operating on your behalf. Services use IAM Roles which provide temporary access to AWS resources when needed. IMPORTANT You need to have Administrative access to your AWS account to make changes in IAM. A recommended way to do this is to create a user and add that user to a group with the AdministratorAccess managed policy attached. This makes it easier to revoke these privileges if necessary.","title":"Core: Permissions"},{"location":"core-env/create-iam-roles.html#create-iam-resources","text":"","title":"Create IAM Resources"},{"location":"core-env/create-iam-roles.html#iam-policies","text":"For the EC2 instance role described in the next section, it is recommended to restrict access to just the resources and permissions it needs to use. In this case, it will be: Access to the specific buckets used for input and output data The ability to create and add EBS volumes to the instance (more on this later) These policies could be used by other roles, so it will be easier to manage them if each are stand alone documents. Bucket Access Policy (required) : This policy specifies full access to a single S3 bucket named <bucket-name> which physically resides in <region> . { \"PolicyName\": \"s3bucket-access-<region>\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::<bucket-name>\", \"arn:aws:s3:::<bucket-name>/*\" ] } ] } } If needed, the policy can be made more granular - i.e. only allowing access to a prefix within the bucket - by modifying the second Resource item to include the prefix path before the * . EBS Autoscale Policy (required) : This policy allows job instance to attach EBS volumes to create extra scratch space for genomic data using Amazon EBS Autoscale . { \"PolicyName\": \"ebs-autoscale-<region>\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:AttachVolume\", \"ec2:DescribeVolumeStatus\", \"ec2:DescribeVolumes\", \"ec2:ModifyInstanceAttribute\", \"ec2:DescribeVolumeAttribute\", \"ec2:CreateVolume\", \"ec2:DeleteVolume\", \"ec2:CreateTags\" ], \"Resource\": \"*\" } ] } }","title":"IAM Policies"},{"location":"core-env/create-iam-roles.html#iam-roles","text":"IAM roles that your job execution environment in AWS Batch will use include: Batch Service Role (required) : Role used by AWS Batch to call other AWS services on its behalf. AWS Batch makes calls to other AWS services on your behalf to manage the resources that you use with the service. Before you can use the service, you must have an IAM policy and role that provides the necessary permissions to AWS Batch. (Learn More) Batch Instance Profile (required) : Role that defines service permissions for EC2 instances launched by AWS Batch. This role should also have attached policies (see above) that allow access to specific S3 buckets and the ability to modify storage (e.g. EBS volumes) on the instance. (Learn More) Batch SpotFleet Role (depends) : This role is needed if you intend to launch spot instances from AWS Batch. If you create a managed compute environment that uses Amazon EC2 Spot Fleet Instances with a BEST_FIT allocation strategy, you must create a role that grants the Spot Fleet permission to set a cost threshold, launch, tag, and terminate instances on your behalf. (Learn More) Batch Job Role (optional) : Role used to provide specific service permissions to individual jobs. Jobs can run without an IAM role. In that case, they inherit the permissions of the instance they run on. Job roles are useful if you have jobs that utilize additional AWS resources such as buckets with supplementary data or need to interact with other AWS services like databases.","title":"IAM Roles"},{"location":"core-env/create-iam-roles.html#automated-via-cloudformation","text":"The CloudFormation template below creates all of the above roles and policies. Name Description Source Launch Stack Amazon IAM Roles Create the necessary IAM Roles. This is useful to hand to someone with the right permissions to create these on your behalf. You will need to provide a S3 bucket name . cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Administrative Access Required In order run this CloudFormation template you you will need privileged access to your account either through an IAM user, STS assumed role, or CloudFormation Stack role.","title":"Automated via CloudFormation"},{"location":"core-env/create-iam-roles.html#manually-via-the-aws-console","text":"","title":"Manually via the AWS Console"},{"location":"core-env/create-iam-roles.html#create-a-bucket-access-policy","text":"Go to the IAM Console Click on \"Policies\" Click on \"Create Policy\" Repeat the following for as many buckets as you will use (e.g. if you have one bucket for nextflow logs and another for nextflow workDir, you will need to do this twice) Select \"S3\" as the service Select \"All Actions\" Under Resources select \"Specific\" Under Resources > bucket, click \"Add ARN\" Type in the name of the bucket Click \"Add\" Under Resources > object, click \"Add ARN\" For \"Bucket Name\", type in the name of the bucket For \"Object Name\", select \"Any\" Click \"Add additional permissions\" if you have additional buckets you are using Click \"Review Policy\" Name the policy \"bucket-access-policy\" Click \"Create Policy\"","title":"Create a bucket access policy"},{"location":"core-env/create-iam-roles.html#create-an-ebs-autoscale-policy","text":"Go to the IAM Console Click on \"Policies\" Click on \"Create Policy\" Switch to the \"JSON\" tab Paste the following into the editor: { \"Version\": \"2012-10-17\", \"Statement\": { \"Action\": [ \"ec2:*Volume\", \"ec2:modifyInstanceAttribute\", \"ec2:describeVolumes\" ], \"Resource\": \"*\", \"Effect\": \"Allow\" } } Click \"Review Policy\" Name the policy \"ebs-autoscale-policy\" Click \"Create Policy\"","title":"Create an EBS autoscale policy"},{"location":"core-env/create-iam-roles.html#create-a-batch-service-role","text":"This is a role used by AWS Batch to launch EC2 instances on your behalf. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose \"Batch\" as the service to use the role Click \"Next: Permissions\" In Attached permissions policies, the \"AWSBatchServiceRole\" will already be attached Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"AWSBatchServiceRole\" Click \"Create role\"","title":"Create a Batch Service Role"},{"location":"core-env/create-iam-roles.html#create-an-ec2-instance-role","text":"This is a role that controls what AWS Resources EC2 instances launched by AWS Batch have access to. In this case, you will limit S3 access to just the bucket you created earlier. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose EC2 from the larger services list Choose \"EC2 - Allows EC2 instances to call AWS services on your behalf\" as the use case. Click \"Next: Permissions\" Type \"ContainerService\" in the search field for policies Click the checkbox next to \"AmazonEC2ContainerServiceforEC2Role\" to attach the policy Type \"S3\" in the search field for policies Click the checkbox next to \"AmazonS3ReadOnlyAccess\" to attach the policy Note Enabling Read-Only access to all S3 resources is required if you use publicly available datasets such as the 1000 Genomes dataset , and others, available in the AWS Registry of Open Datasets Type \"bucket-access-policy\" in the search field for policies Click the checkbox next to \"bucket-access-policy\" to attach the policy Type \"ebs-autoscale-policy\" in the search field for policies Click the checkbox next to \"ebs-autoscale-policy\" to attach the policy Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"ecsInstanceRole\" Click \"Create role\"","title":"Create an EC2 Instance Role"},{"location":"core-env/create-iam-roles.html#create-an-ec2-spotfleet-role","text":"This is a role that allows creation and launch of Spot fleets - Spot instances with similar compute capabilities (i.e. vCPUs and RAM). This is for using Spot instances when running jobs in AWS Batch. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose EC2 from the larger services list Choose \"EC2 - Spot Fleet Tagging\" as the use case In Attached permissions policies, the \"AmazonEC2SpotFleetTaggingRole\" will already be attached Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"AWSSpotFleetTaggingRole\" Click \"Create role\"","title":"Create an EC2 SpotFleet Role"},{"location":"core-env/create-iam-roles.html#create-a-job-role","text":"This is a role used by individual Batch Jobs to specify permissions to AWS resources in addition to permissions allowed by the Instance Role above. Go to the IAM Console Click on \"Roles\" Click on \"Create role\" Select \"AWS service\" as the trusted entity Choose Elastic Container Service from the larger services list Choose \"Elastic Container Service Task\" as the use case. Click \"Next: Permissions\" Attach AWS managed and user defined policies as needed. Click \"Next: Tags\". (adding tags is optional) Click \"Next: Review\" Set the Role Name to \"BatchJobRole\" Click \"Create role\"","title":"Create a Job Role"},{"location":"core-env/create-s3-bucket.html","text":"Core: Data Storage DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. You will need a robust location to store your input and output data. Genomics data files often equal or exceed 100GB per file. In addition to input sample files, genomics data processing typically relies on additional items like reference sequences or annotation databases that can be equally large. The following are key criteria for storing data for genomics workflows accessible to compute secure durable capable of handling large files Amazon S3 buckets meet all of the above conditions. S3 also makes it easy to collaboratively work on such large datasets because buckets and the data stored in them are globally available. You can use an S3 bucket to store both your input data and workflow results. Create an S3 Bucket You can use an existing bucket for your workflows, or you can create a new one using the methods below. Automated via Cloudformation Name Description Source Launch Stack Amazon S3 Bucket Creates a secure Amazon S3 bucket to read from and write results to. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Manually via the AWS Console Go to the S3 Console Click on the \"Create Bucket\" button In the dialog that opens: Provide a \"Bucket Name\". This needs to be globally unique. Select the region for the bucket. Buckets are globally accessible, but the data resides on physical hardware within a specific region. It is best to choose a region that is closest to where you are and where you will launch compute resources to reduce network latency and avoid inter-region transfer costs. The default options for bucket configuration are sufficient for the marjority of use cases. Click the \"Create\" button to accept defaults and create the bucket.","title":"Data Storage"},{"location":"core-env/create-s3-bucket.html#core-data-storage","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. You will need a robust location to store your input and output data. Genomics data files often equal or exceed 100GB per file. In addition to input sample files, genomics data processing typically relies on additional items like reference sequences or annotation databases that can be equally large. The following are key criteria for storing data for genomics workflows accessible to compute secure durable capable of handling large files Amazon S3 buckets meet all of the above conditions. S3 also makes it easy to collaboratively work on such large datasets because buckets and the data stored in them are globally available. You can use an S3 bucket to store both your input data and workflow results.","title":"Core: Data Storage"},{"location":"core-env/create-s3-bucket.html#create-an-s3-bucket","text":"You can use an existing bucket for your workflows, or you can create a new one using the methods below.","title":"Create an S3 Bucket"},{"location":"core-env/create-s3-bucket.html#automated-via-cloudformation","text":"Name Description Source Launch Stack Amazon S3 Bucket Creates a secure Amazon S3 bucket to read from and write results to. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack.","title":"Automated via Cloudformation"},{"location":"core-env/create-s3-bucket.html#manually-via-the-aws-console","text":"Go to the S3 Console Click on the \"Create Bucket\" button In the dialog that opens: Provide a \"Bucket Name\". This needs to be globally unique. Select the region for the bucket. Buckets are globally accessible, but the data resides on physical hardware within a specific region. It is best to choose a region that is closest to where you are and where you will launch compute resources to reduce network latency and avoid inter-region transfer costs. The default options for bucket configuration are sufficient for the marjority of use cases. Click the \"Create\" button to accept defaults and create the bucket.","title":"Manually via the AWS Console"},{"location":"core-env/custom-deploy.html","text":"Customized Deployment DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Deployments of the 'Nextflow on AWS Batch' solution are based on nested CloudFormation templates, and on artifacts comprising scripts, software packages, and configuration files. The templates and artifacts are stored in S3 buckets, and their S3 URLs are used when launching the top-level template and as parameters to that template's deployment. VPC The quick start link deploys the AWS VPC Quickstart , which creates a VPC with up to 4 Availability Zones, each with a public subnet and a private subnet with NAT Gateway access to the Internet. Genomics Workflow Core This quick start link deploys the CloudFormation template gwfcore-root.template.yaml for the Genomics Workflow Core (GWFCore) from the Genomics Workflows on AWS solution. This template launches a number of nested templates, as shown below: Root Stack gwfcore-root - Top level template for Genomics Workflow Core S3 Stack gwfcore-s3 - S3 bucket (new or existing) for storing analysis results IAM Stack gwfcore-iam - Creates IAM roles to use with AWS Batch scalable genomics workflow environment Code Stack gwfcore-code - Creates AWS CodeCommit repos and CodeBuild projects for Genomics Workflows Core assets and artifacts Launch Template Stack gwfcore-launch-template - Creates an EC2 Launch Template for AWS Batch based genomics workflows Batch Stack gwfcore-batch - Deploys resource for a AWS Batch environment that is suitable for genomics, including default and high-priority JobQueues Root Stack The quick start solution links to the CloudFormation console, where the 'Amazon S3 URL' field is prefilled with the S3 URL of a copy of the root stack template, hosted in the public S3 bucket aws-genomics-workflows . To use a customized root stack, upload your modified stack template to an S3 bucket (see Building a Custom Distribution ), and specify that template's URL in 'Amazon S3 URL'. Artifacts and Nested Stacks The subsequent screen, 'Specify Stack Details', allows for customization of the deployed resources in the 'Distribution Configuration' section. Artifact S3 Bucket Name and Artifact S3 Prefix define the location of the artifacts uploaded prior to this deployment. By default, pre-prepared artifacts are stored in the aws-genomics-workflows bucket. Template Root URL defines the bucket and prefix used to store nested templates, called by the root template. To use your own modified artifacts or nested templates, build and upload as described in Building a Custom Distribution , and specify the bucket and prefix in the fields above. Workflow Orchestrators Nextflow This quick start deploys the Nextflow template nextflow-resources.template.yaml , which launches one nested stack: Root Stack nextflow-resources - Creates resources specific to running Nextflow on AWS Container Build Stack container-build - Creates resources for building a Docker container image using CodeBuild, storing the image in ECR, and optionally creating a corresponding Batch Job Definition The nextflow root stack is specified in the same way as the GWFCore root stack, above, and a location for a modified root stack may be specified as with the Core stack. The subsequent 'Specify Stack Details' screen has fields allowing the customization of the Nextflow deployment. S3NextflowPrefix , S3LogsDirPrefix , and S3WorkDirPrefix specify the path within the GWFCore bucket in which to store per-run data and log files. TemplateRootUrl specifies the path to the nested templates called by the Nextflow root template, as with the GWFCore root stack.","title":"Customized Deployment"},{"location":"core-env/custom-deploy.html#customized-deployment","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Deployments of the 'Nextflow on AWS Batch' solution are based on nested CloudFormation templates, and on artifacts comprising scripts, software packages, and configuration files. The templates and artifacts are stored in S3 buckets, and their S3 URLs are used when launching the top-level template and as parameters to that template's deployment.","title":"Customized Deployment"},{"location":"core-env/custom-deploy.html#vpc","text":"The quick start link deploys the AWS VPC Quickstart , which creates a VPC with up to 4 Availability Zones, each with a public subnet and a private subnet with NAT Gateway access to the Internet.","title":"VPC"},{"location":"core-env/custom-deploy.html#genomics-workflow-core","text":"This quick start link deploys the CloudFormation template gwfcore-root.template.yaml for the Genomics Workflow Core (GWFCore) from the Genomics Workflows on AWS solution. This template launches a number of nested templates, as shown below: Root Stack gwfcore-root - Top level template for Genomics Workflow Core S3 Stack gwfcore-s3 - S3 bucket (new or existing) for storing analysis results IAM Stack gwfcore-iam - Creates IAM roles to use with AWS Batch scalable genomics workflow environment Code Stack gwfcore-code - Creates AWS CodeCommit repos and CodeBuild projects for Genomics Workflows Core assets and artifacts Launch Template Stack gwfcore-launch-template - Creates an EC2 Launch Template for AWS Batch based genomics workflows Batch Stack gwfcore-batch - Deploys resource for a AWS Batch environment that is suitable for genomics, including default and high-priority JobQueues","title":"Genomics Workflow Core"},{"location":"core-env/custom-deploy.html#root-stack","text":"The quick start solution links to the CloudFormation console, where the 'Amazon S3 URL' field is prefilled with the S3 URL of a copy of the root stack template, hosted in the public S3 bucket aws-genomics-workflows . To use a customized root stack, upload your modified stack template to an S3 bucket (see Building a Custom Distribution ), and specify that template's URL in 'Amazon S3 URL'.","title":"Root Stack"},{"location":"core-env/custom-deploy.html#artifacts-and-nested-stacks","text":"The subsequent screen, 'Specify Stack Details', allows for customization of the deployed resources in the 'Distribution Configuration' section. Artifact S3 Bucket Name and Artifact S3 Prefix define the location of the artifacts uploaded prior to this deployment. By default, pre-prepared artifacts are stored in the aws-genomics-workflows bucket. Template Root URL defines the bucket and prefix used to store nested templates, called by the root template. To use your own modified artifacts or nested templates, build and upload as described in Building a Custom Distribution , and specify the bucket and prefix in the fields above.","title":"Artifacts and Nested Stacks"},{"location":"core-env/custom-deploy.html#workflow-orchestrators","text":"","title":"Workflow Orchestrators"},{"location":"core-env/custom-deploy.html#nextflow","text":"This quick start deploys the Nextflow template nextflow-resources.template.yaml , which launches one nested stack: Root Stack nextflow-resources - Creates resources specific to running Nextflow on AWS Container Build Stack container-build - Creates resources for building a Docker container image using CodeBuild, storing the image in ECR, and optionally creating a corresponding Batch Job Definition The nextflow root stack is specified in the same way as the GWFCore root stack, above, and a location for a modified root stack may be specified as with the Core stack. The subsequent 'Specify Stack Details' screen has fields allowing the customization of the Nextflow deployment. S3NextflowPrefix , S3LogsDirPrefix , and S3WorkDirPrefix specify the path within the GWFCore bucket in which to store per-run data and log files. TemplateRootUrl specifies the path to the nested templates called by the Nextflow root template, as with the GWFCore root stack.","title":"Nextflow"},{"location":"core-env/introduction.html","text":"Core: Introduction DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. A high level view of the architecture you will need to run workflows is shown is below. This section of the guide details the common components required for job execution and data storage. This includes the following: A place to store your input data and generated results Access controls to your data and compute resources Code and artifacts used to provision compute resources Containerized task scheduling and execution The above is referred to here as the \"Genomics Workflows Core\". To launch this core in your AWS account, use the Cloudformation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. Info To create all of the resources described, the Cloudformation template above uses Nested Stacks . This is a way to modularize complex stacks and enable reuse. The individual nested stack templates are intended to be run from a parent or \"root\" template. On the following pages, the individual nested stack templates are available for viewing only.","title":"Introduction"},{"location":"core-env/introduction.html#core-introduction","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. A high level view of the architecture you will need to run workflows is shown is below. This section of the guide details the common components required for job execution and data storage. This includes the following: A place to store your input data and generated results Access controls to your data and compute resources Code and artifacts used to provision compute resources Containerized task scheduling and execution The above is referred to here as the \"Genomics Workflows Core\". To launch this core in your AWS account, use the Cloudformation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. Info To create all of the resources described, the Cloudformation template above uses Nested Stacks . This is a way to modularize complex stacks and enable reuse. The individual nested stack templates are intended to be run from a parent or \"root\" template. On the following pages, the individual nested stack templates are available for viewing only.","title":"Core: Introduction"},{"location":"core-env/setup-aws-batch.html","text":"Core: AWS Batch DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. AWS Batch is a managed service that helps you efficiently run batch computing workloads on the AWS Cloud. Users submit jobs to job queues, specifying the application to be run and the compute resources (CPU and memory) required by the job. AWS Batch is responsible for launching the appropriate quantity and types of instances needed to run your jobs. AWS Batch manages the following resources: Job Definitions Job Queues Compute Environments A job definition specifies how jobs are to be run. For example, which Docker image to use for your job, how many vCPUs and how much memory is required, the IAM role to be used, and more. Jobs are submitted to job queues where they reside until they can be scheduled to run on Amazon EC2 instances within a compute environment. An AWS account can have multiple job queues, each with varying priority. This gives you the ability to closely align the consumption of compute resources with your organizational requirements. Compute environments are effectively autoscaling clusters of EC2 instances that are launched to run your jobs. Unlike traditional HPC clusters, compute environments can be configured to use a variety of instance types and sizes. The AWS Batch job scheduler will do the heavy lifting of placing jobs on the most appropriate instance type based on the jobs resource requirements. Compute environments can also use either On-demand instances, or Spot instances for maximum cost savings. Job queues are mapped to one or more compute environments and a given environment can also be mapped to one or more job queues. This many-to-many relationship is defined by the compute environment order and job queue priority properties. The following diagram shows a general overview of how the AWS Batch resources interact. For more information, watch the How AWS Batch Works video. Job Requirements AWS Batch does not make assumptions on the structure and requirements that Jobs take with respect to inputs and outputs. Batch Jobs may take data streams, files, or only parameters as input, and produce the same variety for output, inclusive of files, metadata changes, updates to databases, etc. Batch assumes that each application handles their own input/output requirements. A common pattern for bioinformatics tooling is that files such as genomic sequence data are both inputs and outputs to/from a process. Many bioinformatics tools have also been developed to run in traditional Linux-based compute clusters with shared filesystems and are not necessarily optimized for cloud computing. When using AWS Batch for genomics workflows, there are a couple key considerations: Independent execution: To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow will run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. Multitenancy: Multiple container jobs may run concurrently on the same instance. In these situations, it is essential that your job writes to a unique subdirectory. Data cleanup: As your jobs complete and write the output back to S3, it is a good idea to delete the scratch data generated by that job on the instance. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances. Creating an AWS Batch Environment A complete AWS Batch environment consists of the following: A Compute Environment that utilizes EC2 Spot instances for cost-effective computing A Compute Environment that utilizes EC2 on-demand (e.g. public pricing ) instances for high-priority work that can't risk job interruptions or delays due to insufficient Spot capacity. A default Job Queue that solely utilizes the Spot compute environment. This is for jobs where timeliness isn't a constraint, and can wait for the right instances to become available, as well has handle interruption. It also ensures the most cost savings. A priority Job Queue that leverages the on-demand, and optionally Spot, CE's (in that order) and has higher priority than the default queue. This is for jobs that cannot handle interruption, and need to be executed immediately. Automated via CloudFormation The CloudFormation template below will create all of the above. Name Description Source Launch Stack AWS Batch Creates AWS Batch Job Queues and Compute Environments. You will need to provide the details for your Launch Template ID, IAM roles and instance profiles, and the IDs for a VPC and subnets. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack. Manually via the AWS Console Compute Environments You can create several compute environments to suit your needs. Below we'll create the following: An \"optimal\" compute environment using on-demand instances An \"optimal\" compute environment using spot instances \"Optimal\" is a default grouping of EC2 instance types used for compute environments. It includes M4 (general purpose), C4 (compute-optimized), and R4 (memory-optimized) instance families which should be suitable for a wide range of computing cases. Create an \"optimal\" on-demand compute environment Go to the AWS Batch Console Click on \"Compute environments\" Click on \"Create environment\" Select \"Managed\" as the \"Compute environment type\" For \"Compute environment name\" type: \"ondemand\" In the \"Service role\" drop down, select the AWSBatchServiceRole you created previously In the \"Instance role\" drop down, select the ecsInstanceRole you created previously For \"Provisioning model\" select \"On-Demand\" \"Allowed instance types\" will be already populated with \"optimal\" - which is a mixture of M4, C4, and R4 instances. This should be sufficient for demonstration purposes. In a production setting, it is recommended to specify the instance famimlies and sizes most apprioriate for the jobs the CE will support. For the On-Demand CE, selecting newer instance types is beneficial as they tend to have better price per performance. \"Allocation strategy\" will already be set to BEST_FIT . This is recommended for on-demand based compute environments as it ensures the most cost efficiency. In the \"Launch template\" drop down, select the genomics-workflow-template you created previously Set Minimum and Desired vCPUs to 0. Info Minimum vCPUs is the lowest number of active vCPUs (i.e. instances) your compute environment will keep running and available for placing jobs when there are no jobs queued. Setting this to 0 means that AWS Batch will terminate all instances when all queued jobs are complete. Desired vCPUs is the number of active vCPUs (i.e. instances) that are currently needed in the compute environment to process queued jobs. Setting this to 0 implies that there are currently no queued jobs. AWS Batch will adjust this number based on the number of jobs queued and their resource requirements. Maximum vCPUs is the highest number of active vCPUs (i.e. instances) your compute environment will launch. This places a limit on the number of jobs the compute environment can process in parallel. For networking, the options are populated with your account's default VPC, public subnets, and security group. This should be sufficient for the purposes of this workshop. In a production setting, it is recommended to use a separate VPC, private subnets therein, and associated security groups. Optional: (Recommended) Add EC2 tags. These will help identify which EC2 instances were launched by AWS Batch. At minimum: Key: \"Name\" Value: \"batch-ondemand-worker\" Click on \"Create\" Create an \"optimal\" spot compute environment Go to the AWS Batch Console Click on \"Compute environments\" Click on \"Create environment\" Select \"Managed\" as the \"Compute environment type\" For \"Compute environment name\" type: \"spot\" In the \"Service role\" drop down, select the AWSBatchServiceRole you created previously In the \"Instance role\" drop down, select the ecsInstanceRole you created previously For \"Provisioning model\" select \"Spot\" \"Allowed instance types\" will be already populated with \"optimal\" - which is a mixture of M4, C4, and R4 instances. This should be sufficient for demonstration purposes. In a production setting, it is recommended to specify the instance families and sizes most appropriate for the jobs the CE will support. For the SPOT CE a wider diversity of instance types is recommended to maximize the pools from which capacity can be drawn from. Limiting the size of instances is also recommended to avoid scheduling too many jobs on a SPOT instance that could be interrupted. \"Allocation strategy\" will already be set to SPOT_CAPACITY_OPTIMIZED . This is recommended for Spot based compute environments as it ensures the most compute capacity is available for your jobs. In the \"Launch template\" drop down, select the genomics-workflow-template you created previously Set Minimum and Desired vCPUs to 0. For networking, the options are populated with your account's default VPC, public subnets, and security group. This should be sufficient for the purposes of this workshop. In a production setting, it is recommended to use a separate VPC, private subnets therein, and associated security groups. Optional: (Recommended) Add EC2 tags. These will help identify which EC2 instances were launched by AWS Batch. At minimum: Key: \"Name\" Value: \"batch-spot-worker\" Click on \"Create\" Job Queues AWS Batch job queues, are where you submit and monitor the status of jobs. Job queues can be associated with one or more compute environments in a preferred order. Multiple job queues can be associated with the same compute environment. Thus to handle scheduling, job queues also have a priority weight as well. Below we'll create two job queues: A \"Default\" job queue A \"Priority\" job queue Both job queues will use both compute environments you created previously. Create a \"default\" job queue This queue is intended for jobs that do not require urgent completion, and can handle potential interruption. This queue will schedule jobs to only the \"spot\" compute environment. Note It is not recommended to configure a job queue to \"spillover\" from Spot to On-Demand. Doing so could lead Insufficient Capacity Errors, resulting in Batch unable to schedule jobs, leaving them stuck in \"RUNNABLE\" Because it leverages Spot instances, it will also be the most cost effective job queue. Go to the AWS Batch Console Click on \"Job queues\" Click on \"Create queue\" For \"Queue name\" use \"default\" Set \"Priority\" to 1 Under \"Connected compute environments for this queue\", using the drop down menu: Select the \"spot\" compute environment you created previously Click on \"Create Job Queue\" Create a \"priority\" job queue This queue is intended for jobs that are urgent and cannot handle potential interruption. This queue will schedule jobs to: The \"ondemand\" compute environment The \"spot\" compute environment in that order. In this queue configuration, Batch will schedule jobs to the \"ondemand\" compute environment first. When the number of Max vCPUs for that environment is reached, Batch will begin scheduling jobs to the \"spot\" compute environment. The use of the \"spot\" compute environment is optional, and is used to help drain pending jobs from the queue faster. Go to the AWS Batch Console Click on \"Job queues\" Click on \"Create queue\" For \"Queue name\" use \"priority\" Set \"Priority\" to 100 (higher values mean higher priority) Under \"Connected compute environments for this queue\", using the drop down menu: Select the \"ondemand\" compute environment you created previously, then Select the \"spot\" compute environment you created previously Click on \"Create Job Queue\"","title":"AWS Batch"},{"location":"core-env/setup-aws-batch.html#core-aws-batch","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. AWS Batch is a managed service that helps you efficiently run batch computing workloads on the AWS Cloud. Users submit jobs to job queues, specifying the application to be run and the compute resources (CPU and memory) required by the job. AWS Batch is responsible for launching the appropriate quantity and types of instances needed to run your jobs. AWS Batch manages the following resources: Job Definitions Job Queues Compute Environments A job definition specifies how jobs are to be run. For example, which Docker image to use for your job, how many vCPUs and how much memory is required, the IAM role to be used, and more. Jobs are submitted to job queues where they reside until they can be scheduled to run on Amazon EC2 instances within a compute environment. An AWS account can have multiple job queues, each with varying priority. This gives you the ability to closely align the consumption of compute resources with your organizational requirements. Compute environments are effectively autoscaling clusters of EC2 instances that are launched to run your jobs. Unlike traditional HPC clusters, compute environments can be configured to use a variety of instance types and sizes. The AWS Batch job scheduler will do the heavy lifting of placing jobs on the most appropriate instance type based on the jobs resource requirements. Compute environments can also use either On-demand instances, or Spot instances for maximum cost savings. Job queues are mapped to one or more compute environments and a given environment can also be mapped to one or more job queues. This many-to-many relationship is defined by the compute environment order and job queue priority properties. The following diagram shows a general overview of how the AWS Batch resources interact. For more information, watch the How AWS Batch Works video.","title":"Core: AWS Batch"},{"location":"core-env/setup-aws-batch.html#job-requirements","text":"AWS Batch does not make assumptions on the structure and requirements that Jobs take with respect to inputs and outputs. Batch Jobs may take data streams, files, or only parameters as input, and produce the same variety for output, inclusive of files, metadata changes, updates to databases, etc. Batch assumes that each application handles their own input/output requirements. A common pattern for bioinformatics tooling is that files such as genomic sequence data are both inputs and outputs to/from a process. Many bioinformatics tools have also been developed to run in traditional Linux-based compute clusters with shared filesystems and are not necessarily optimized for cloud computing. When using AWS Batch for genomics workflows, there are a couple key considerations: Independent execution: To make your workflow as flexible as possible, each job should run independently. As a result, you cannot necessarily guarantee that different jobs in the same overall workflow will run on the same instance. Using S3 as the location to exchange data between containers enables you to decouple storage of your intermediate files from compute. Multitenancy: Multiple container jobs may run concurrently on the same instance. In these situations, it is essential that your job writes to a unique subdirectory. Data cleanup: As your jobs complete and write the output back to S3, it is a good idea to delete the scratch data generated by that job on the instance. This allows you to optimize for cost by reusing EC2 instances if there are jobs remaining in the queue, rather than terminating the EC2 instances.","title":"Job Requirements"},{"location":"core-env/setup-aws-batch.html#creating-an-aws-batch-environment","text":"A complete AWS Batch environment consists of the following: A Compute Environment that utilizes EC2 Spot instances for cost-effective computing A Compute Environment that utilizes EC2 on-demand (e.g. public pricing ) instances for high-priority work that can't risk job interruptions or delays due to insufficient Spot capacity. A default Job Queue that solely utilizes the Spot compute environment. This is for jobs where timeliness isn't a constraint, and can wait for the right instances to become available, as well has handle interruption. It also ensures the most cost savings. A priority Job Queue that leverages the on-demand, and optionally Spot, CE's (in that order) and has higher priority than the default queue. This is for jobs that cannot handle interruption, and need to be executed immediately.","title":"Creating an AWS Batch Environment"},{"location":"core-env/setup-aws-batch.html#automated-via-cloudformation","text":"The CloudFormation template below will create all of the above. Name Description Source Launch Stack AWS Batch Creates AWS Batch Job Queues and Compute Environments. You will need to provide the details for your Launch Template ID, IAM roles and instance profiles, and the IDs for a VPC and subnets. cloud_download play_arrow Info The launch button has been disabled above since this template is part of a set of nested templates. It is not recommended to launch it independently of its intended parent stack.","title":"Automated via CloudFormation"},{"location":"core-env/setup-aws-batch.html#manually-via-the-aws-console","text":"","title":"Manually via the AWS Console"},{"location":"core-env/setup-aws-batch.html#compute-environments","text":"You can create several compute environments to suit your needs. Below we'll create the following: An \"optimal\" compute environment using on-demand instances An \"optimal\" compute environment using spot instances \"Optimal\" is a default grouping of EC2 instance types used for compute environments. It includes M4 (general purpose), C4 (compute-optimized), and R4 (memory-optimized) instance families which should be suitable for a wide range of computing cases.","title":"Compute Environments"},{"location":"core-env/setup-aws-batch.html#create-an-optimal-on-demand-compute-environment","text":"Go to the AWS Batch Console Click on \"Compute environments\" Click on \"Create environment\" Select \"Managed\" as the \"Compute environment type\" For \"Compute environment name\" type: \"ondemand\" In the \"Service role\" drop down, select the AWSBatchServiceRole you created previously In the \"Instance role\" drop down, select the ecsInstanceRole you created previously For \"Provisioning model\" select \"On-Demand\" \"Allowed instance types\" will be already populated with \"optimal\" - which is a mixture of M4, C4, and R4 instances. This should be sufficient for demonstration purposes. In a production setting, it is recommended to specify the instance famimlies and sizes most apprioriate for the jobs the CE will support. For the On-Demand CE, selecting newer instance types is beneficial as they tend to have better price per performance. \"Allocation strategy\" will already be set to BEST_FIT . This is recommended for on-demand based compute environments as it ensures the most cost efficiency. In the \"Launch template\" drop down, select the genomics-workflow-template you created previously Set Minimum and Desired vCPUs to 0. Info Minimum vCPUs is the lowest number of active vCPUs (i.e. instances) your compute environment will keep running and available for placing jobs when there are no jobs queued. Setting this to 0 means that AWS Batch will terminate all instances when all queued jobs are complete. Desired vCPUs is the number of active vCPUs (i.e. instances) that are currently needed in the compute environment to process queued jobs. Setting this to 0 implies that there are currently no queued jobs. AWS Batch will adjust this number based on the number of jobs queued and their resource requirements. Maximum vCPUs is the highest number of active vCPUs (i.e. instances) your compute environment will launch. This places a limit on the number of jobs the compute environment can process in parallel. For networking, the options are populated with your account's default VPC, public subnets, and security group. This should be sufficient for the purposes of this workshop. In a production setting, it is recommended to use a separate VPC, private subnets therein, and associated security groups. Optional: (Recommended) Add EC2 tags. These will help identify which EC2 instances were launched by AWS Batch. At minimum: Key: \"Name\" Value: \"batch-ondemand-worker\" Click on \"Create\"","title":"Create an \"optimal\" on-demand compute environment"},{"location":"core-env/setup-aws-batch.html#create-an-optimal-spot-compute-environment","text":"Go to the AWS Batch Console Click on \"Compute environments\" Click on \"Create environment\" Select \"Managed\" as the \"Compute environment type\" For \"Compute environment name\" type: \"spot\" In the \"Service role\" drop down, select the AWSBatchServiceRole you created previously In the \"Instance role\" drop down, select the ecsInstanceRole you created previously For \"Provisioning model\" select \"Spot\" \"Allowed instance types\" will be already populated with \"optimal\" - which is a mixture of M4, C4, and R4 instances. This should be sufficient for demonstration purposes. In a production setting, it is recommended to specify the instance families and sizes most appropriate for the jobs the CE will support. For the SPOT CE a wider diversity of instance types is recommended to maximize the pools from which capacity can be drawn from. Limiting the size of instances is also recommended to avoid scheduling too many jobs on a SPOT instance that could be interrupted. \"Allocation strategy\" will already be set to SPOT_CAPACITY_OPTIMIZED . This is recommended for Spot based compute environments as it ensures the most compute capacity is available for your jobs. In the \"Launch template\" drop down, select the genomics-workflow-template you created previously Set Minimum and Desired vCPUs to 0. For networking, the options are populated with your account's default VPC, public subnets, and security group. This should be sufficient for the purposes of this workshop. In a production setting, it is recommended to use a separate VPC, private subnets therein, and associated security groups. Optional: (Recommended) Add EC2 tags. These will help identify which EC2 instances were launched by AWS Batch. At minimum: Key: \"Name\" Value: \"batch-spot-worker\" Click on \"Create\"","title":"Create an \"optimal\" spot compute environment"},{"location":"core-env/setup-aws-batch.html#job-queues","text":"AWS Batch job queues, are where you submit and monitor the status of jobs. Job queues can be associated with one or more compute environments in a preferred order. Multiple job queues can be associated with the same compute environment. Thus to handle scheduling, job queues also have a priority weight as well. Below we'll create two job queues: A \"Default\" job queue A \"Priority\" job queue Both job queues will use both compute environments you created previously.","title":"Job Queues"},{"location":"core-env/setup-aws-batch.html#create-a-default-job-queue","text":"This queue is intended for jobs that do not require urgent completion, and can handle potential interruption. This queue will schedule jobs to only the \"spot\" compute environment. Note It is not recommended to configure a job queue to \"spillover\" from Spot to On-Demand. Doing so could lead Insufficient Capacity Errors, resulting in Batch unable to schedule jobs, leaving them stuck in \"RUNNABLE\" Because it leverages Spot instances, it will also be the most cost effective job queue. Go to the AWS Batch Console Click on \"Job queues\" Click on \"Create queue\" For \"Queue name\" use \"default\" Set \"Priority\" to 1 Under \"Connected compute environments for this queue\", using the drop down menu: Select the \"spot\" compute environment you created previously Click on \"Create Job Queue\"","title":"Create a \"default\" job queue"},{"location":"core-env/setup-aws-batch.html#create-a-priority-job-queue","text":"This queue is intended for jobs that are urgent and cannot handle potential interruption. This queue will schedule jobs to: The \"ondemand\" compute environment The \"spot\" compute environment in that order. In this queue configuration, Batch will schedule jobs to the \"ondemand\" compute environment first. When the number of Max vCPUs for that environment is reached, Batch will begin scheduling jobs to the \"spot\" compute environment. The use of the \"spot\" compute environment is optional, and is used to help drain pending jobs from the queue faster. Go to the AWS Batch Console Click on \"Job queues\" Click on \"Create queue\" For \"Queue name\" use \"priority\" Set \"Priority\" to 100 (higher values mean higher priority) Under \"Connected compute environments for this queue\", using the drop down menu: Select the \"ondemand\" compute environment you created previously, then Select the \"spot\" compute environment you created previously Click on \"Create Job Queue\"","title":"Create a \"priority\" job queue"},{"location":"install-cromwell/index.html","text":"Installing the Genomics Workflow Core and Cromwell Summary The purpose of this document is to demonstrate how an AWS user can provision the infrastructure necessary to run Cromwell versions 52 and beyond on AWS Batch using S3 as an object store using CloudFormation. The instructions cover deployment into an existing VPC. There are two main steps: deploying the genomics workflow core infrastructure which can be used with Cromwell, Nextflow and AWS Step Functions, and the deployment of the Cromwell server and related artifacts. Assumptions The instructions assume you have an existing AWS account with sufficient credentials to deploy the infrastructure or that you will use a role with CloudFormation that has sufficient privileges (admin role is recommended). You have an existing VPC to deploy artifacts into. This VPC should have a minimum of two subnets with routes to the public internet. Private subnet routes may be through a NAT Gateway. Deployment of Genomics Workflow Core into an existing VPC. Take note of the id of the VPC that you will use and the ids of the subnets of the VPC that you will use for the Batch worker nodes. We recommend using two or more private subnets. Open the CloudFormation consoles and select \u201c Create stack \u201d with new resources. Enter https://aws-genomics-workflows.s3.amazonaws.com/latest/templates/gwfcore/gwfcore-root.template.yaml as the Amazon S3 URL. Select appropriate values for your environment including the VPC and subnets you recorded above. It is recommended to leave the Default and High Priority Min vCPU values at 0 so that the AWS Batch cluster will not have any instances running when there are no workflows running. Max vCPU values may be increased if you expect to run large workloads utilizing many CPUs. Leave the Distribution Configuration values with the preset defaults. Optionally add tags and click Next Review the parameters, acknowledge the Capabilities notifications and click \u201c Create Stack \u201d The template will now create several nested stacks to deploy the required resources. This step will take approximately 10 minutes to complete. When this is complete you can proceed with the \u201c Deploy Cromwell Resources \u201d section below. Deploy Cromwell Resources Ensure all steps of the CloudFormation deployment of the Genomics Workflow Core have successfully completed before proceeding any further. From the CloudFormation console select \u201c Create Stack \u201d and if prompted select \u201c With new resources (Standard) \u201d Fill in the Amazon S3 URL with https://aws-genomics-workflows.s3.amazonaws.com/latest/templates/cromwell/cromwell-resources.template.yaml Fill in appropriate values for the template. For GWFCoreNamespace use the names space value you used in the section above . You should use the same VPC as you used in the previous step above. To secure your Cromwell server you should change the SSH Address Range and HTTP Address Range to trusted values, these will be used when creating the servers security group. You may either use the latest version of Cromwell (recommended) or specify a version 52 or greater. Select a MySQL compliant Cromwell Database Password that will be used for Cromwell\u2019s metadata database. Select \u201c Next\u201d . On the remaining two screens keep the defaults, acknowledge the IAM capabilities and then click \u201c Create Stack \u201d Once the stack completes an EC2 will be deployed and it will be running an instance of the Cromwell server. You can now proceed with \" Testing your deployment \" Testing your Deployment The following WDL file is a very simple workflow that can be used to test that all the components of the deployment are working together. Add the code block below to a file named workflow.wdl workflow helloWorld { call sayHello } task sayHello { command { echo \"hello world\" } output { String out = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" memory: \"1 GB\" cpu: 1 } } This task can be submitted to the servers REST endpoint using curl either from a client that has access to the servers elastic IP or from within the server itself using localhost. The hostname of the server is also emitted as an output from the cromwell-resources CloudFormation template. curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" It can take a few minutes for AWS Batch to realize there is a job in the work queue and provision a worker to run it. You can monitor this in the AWS Batch console. You can also monitor the Cromwell server logs in CloudWatch. There will be a log group called cromwell-server. Once the run is completed you will see output similar to: If the run is successful subsequent runs will be \u201ccall cached\u201d meaning that the results of the previous run will be copied for all successful steps. If you resubmit the job you will very quickly see the workflow success in the server logs and no additional jobs will be seen in the AWS Batch console. You can disable call caching for the job by adding an options file and submitting it with the run. This will cause the workflow to be re-executed in full. { \"write_to_cache\": false, \"read_from_cache\": false } curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowOptions=@options.json\" For a more realistic workflow, a WDL for simple variant calling using bwa-mem, samtools, and bcftools is available here : Clone the repo, and submit the WDL file to cromwell. The workflow uses default inputs from public data sources. If you want to override these inputs, modify the inputs.json file accordingly and submit it along with the workflow.","title":"Installing the Genomics Workflow Core and Cromwell"},{"location":"install-cromwell/index.html#installing-the-genomics-workflow-core-and-cromwell","text":"","title":"Installing the Genomics Workflow Core and Cromwell"},{"location":"install-cromwell/index.html#summary","text":"The purpose of this document is to demonstrate how an AWS user can provision the infrastructure necessary to run Cromwell versions 52 and beyond on AWS Batch using S3 as an object store using CloudFormation. The instructions cover deployment into an existing VPC. There are two main steps: deploying the genomics workflow core infrastructure which can be used with Cromwell, Nextflow and AWS Step Functions, and the deployment of the Cromwell server and related artifacts.","title":"Summary"},{"location":"install-cromwell/index.html#assumptions","text":"The instructions assume you have an existing AWS account with sufficient credentials to deploy the infrastructure or that you will use a role with CloudFormation that has sufficient privileges (admin role is recommended). You have an existing VPC to deploy artifacts into. This VPC should have a minimum of two subnets with routes to the public internet. Private subnet routes may be through a NAT Gateway.","title":"Assumptions"},{"location":"install-cromwell/index.html#deployment-of-genomics-workflow-core-into-an-existing-vpc","text":"Take note of the id of the VPC that you will use and the ids of the subnets of the VPC that you will use for the Batch worker nodes. We recommend using two or more private subnets. Open the CloudFormation consoles and select \u201c Create stack \u201d with new resources. Enter https://aws-genomics-workflows.s3.amazonaws.com/latest/templates/gwfcore/gwfcore-root.template.yaml as the Amazon S3 URL. Select appropriate values for your environment including the VPC and subnets you recorded above. It is recommended to leave the Default and High Priority Min vCPU values at 0 so that the AWS Batch cluster will not have any instances running when there are no workflows running. Max vCPU values may be increased if you expect to run large workloads utilizing many CPUs. Leave the Distribution Configuration values with the preset defaults. Optionally add tags and click Next Review the parameters, acknowledge the Capabilities notifications and click \u201c Create Stack \u201d The template will now create several nested stacks to deploy the required resources. This step will take approximately 10 minutes to complete. When this is complete you can proceed with the \u201c Deploy Cromwell Resources \u201d section below.","title":"Deployment of Genomics Workflow Core into an existing VPC."},{"location":"install-cromwell/index.html#deploy-cromwell-resources","text":"Ensure all steps of the CloudFormation deployment of the Genomics Workflow Core have successfully completed before proceeding any further. From the CloudFormation console select \u201c Create Stack \u201d and if prompted select \u201c With new resources (Standard) \u201d Fill in the Amazon S3 URL with https://aws-genomics-workflows.s3.amazonaws.com/latest/templates/cromwell/cromwell-resources.template.yaml Fill in appropriate values for the template. For GWFCoreNamespace use the names space value you used in the section above . You should use the same VPC as you used in the previous step above. To secure your Cromwell server you should change the SSH Address Range and HTTP Address Range to trusted values, these will be used when creating the servers security group. You may either use the latest version of Cromwell (recommended) or specify a version 52 or greater. Select a MySQL compliant Cromwell Database Password that will be used for Cromwell\u2019s metadata database. Select \u201c Next\u201d . On the remaining two screens keep the defaults, acknowledge the IAM capabilities and then click \u201c Create Stack \u201d Once the stack completes an EC2 will be deployed and it will be running an instance of the Cromwell server. You can now proceed with \" Testing your deployment \"","title":"Deploy Cromwell Resources"},{"location":"install-cromwell/index.html#testing-your-deployment","text":"The following WDL file is a very simple workflow that can be used to test that all the components of the deployment are working together. Add the code block below to a file named workflow.wdl workflow helloWorld { call sayHello } task sayHello { command { echo \"hello world\" } output { String out = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" memory: \"1 GB\" cpu: 1 } } This task can be submitted to the servers REST endpoint using curl either from a client that has access to the servers elastic IP or from within the server itself using localhost. The hostname of the server is also emitted as an output from the cromwell-resources CloudFormation template. curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" It can take a few minutes for AWS Batch to realize there is a job in the work queue and provision a worker to run it. You can monitor this in the AWS Batch console. You can also monitor the Cromwell server logs in CloudWatch. There will be a log group called cromwell-server. Once the run is completed you will see output similar to: If the run is successful subsequent runs will be \u201ccall cached\u201d meaning that the results of the previous run will be copied for all successful steps. If you resubmit the job you will very quickly see the workflow success in the server logs and no additional jobs will be seen in the AWS Batch console. You can disable call caching for the job by adding an options file and submitting it with the run. This will cause the workflow to be re-executed in full. { \"write_to_cache\": false, \"read_from_cache\": false } curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowOptions=@options.json\" For a more realistic workflow, a WDL for simple variant calling using bwa-mem, samtools, and bcftools is available here : Clone the repo, and submit the WDL file to cromwell. The workflow uses default inputs from public data sources. If you want to override these inputs, modify the inputs.json file accordingly and submit it along with the workflow.","title":"Testing your Deployment"},{"location":"orchestration/orchestration-intro.html","text":"Workflow Orchestration DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Having deployed a way to execute individual tasks via AWS Batch, we turn to orchestration of complete workflows. In order to process data, we will need to handle the cases for serial and parallel task execution, and retry logic when a task fails. The logic for workflows should live outside of the code for any individual task. There are a couple of options that researchers can use to define and execute repeatable data analysis pipelines on AWS Batch: AWS Step Functions , a native AWS service for workflow orchestration. 3rd party alternatives: Cromwell , a workflow execution system from the Broad Institute Nextflow , a reactive workflow framework and domain specific language (DSL) from the Comparative Bioinformatics group at the Barcelona Centre for Genomic Regulation (CRG) Help There are many more 3rd party alternatives. We are actively seeking out help to document them here!","title":"Introduction"},{"location":"orchestration/orchestration-intro.html#workflow-orchestration","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Having deployed a way to execute individual tasks via AWS Batch, we turn to orchestration of complete workflows. In order to process data, we will need to handle the cases for serial and parallel task execution, and retry logic when a task fails. The logic for workflows should live outside of the code for any individual task. There are a couple of options that researchers can use to define and execute repeatable data analysis pipelines on AWS Batch: AWS Step Functions , a native AWS service for workflow orchestration. 3rd party alternatives: Cromwell , a workflow execution system from the Broad Institute Nextflow , a reactive workflow framework and domain specific language (DSL) from the Comparative Bioinformatics group at the Barcelona Centre for Genomic Regulation (CRG) Help There are many more 3rd party alternatives. We are actively seeking out help to document them here!","title":"Workflow Orchestration"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html","text":"Optimizing AWS Batch Resources for Cost Effective Genomics Workflows DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Tasks in a workflow run in Docker containers. Each task contains a runtime definition that specifies the number of CPUs and amount of RAM allocated to the task. runtime { docker: \"biocontainers/plink1.9:v1.90b6.6-181012-1-deb_cv1\" memory: \"8 GB\" cpu: 2 } The amount of resource allocated to each container ultimately impacts the cost to run a workflow. Optimally allocating resources leads to cost efficiency. The following suggestions can help to \u201cright size\u201d your workflow. Optimize the longest running, and most parallel tasks first With AWS Batch you pay for what you use, so it makes most sense to focus on those tasks that run the longest as well as those that have the largest scatters (parallel tasks) as they will make up the majority of the workflow runtime and contribute most to the cost. Enable CloudWatch Container Insights CloudWatch container insights provides detailed monitoring of ECS clusters as well as containers running on those clusters. AWS Batch compute environments are managed ECS clusters, enabling container insights on those clusters will show how the cluster utilization during a workflow run. Currently, cluster insights can be enabled using the AWS CLI. To do this we need the name of the ECS cluster that is managed by the AWS Batch Compute Environment. From the AWS Batch console, identify the Batch Compute Environment(s) of interest 1. Click on the link of the environment and note the ECS Cluster Name 1. Enable cluster insights with the following command aws ecs update-cluster-settings --cluster **ECS-CLUSTER-NAME** --settings name=containerInsights,value=enabled Using Container Insights With Container Insights enabled metrics can be viewed in the CloudWatch console and used to determine if resources are under or over allocated. From the side bar of the CloudWatch console click Container Insights and View Performance Dashboards From the dropdown select ECS Clusters to view the Cluster utilization. Select a time range corresponding to a Cromwell Workflow run. Using the ECS Clusters Dashboard we can see the utilization of each EC2 worker in the cluster. The CPU utilization averages about 50% suggesting that the containers may not be efficiently packed (see below for tips on packing), the containers have too much CPU allocated or both. The memory utilization is very low so memory is over allocated. Container instance count shows the average number of EC2 workers hosting containers, and the task count is the number of tasks running in containers at that time. From the dropdown select ECS Tasks to view the container utilization. Select a time range corresponding to a Cromwell Workflow run. Using the ECS Tasks Dashboard we can gain insight into the average activity within the containers during the workflow. In this case we again see that memory is over allocated and CPU could potentially be reduced. Correlating tasks with metrics To most effectively use CloudWatch Container Insight metrics it is important to know the time at which each workflow task started and finished. You can get this information from the Metadata API of your Cromwell server. The following will return the metadata of the workflow run as a JSON object. curl -k -X GET \"https://my-cromwell-server/api/workflows/v1/WORKFLOW-ID/metadata\" Consider CPU and memory ratios EC2 workers for Cromwell AWS Batch compute environments are c , m , and r instance families that have vCPU to memory ratios of 1:2, 1:4 and 1:8 respectively. AWS Batch will attempt to fit containers to instances in the most optimal way depending on cost and size requirements. Given that a task requiring 16GB of RAM that could make use of all available CPUs, then to optimally pack the containers you should specify either 2, 4, or 8 vCPU. Other values could lead to inefficient packing meaning the resources of the EC2 container instance will be paid for but not optimally used. NOTE: Fully packing an instance can result in it becoming unresponsive if the tasks in the containers use 100% (or more if they start swapping) of the allocated resources. The instance may then be unresponsive to Batch and may time out. To avoid this, always allow for a little overhead. Split tasks that pipe output If a workflow task consists of a process that pipes STDOUT to another process then both processes will run in the same container and receive the same resources. If one task requires more resources than the other this might be inefficient and might be better divided into two tasks each with its own runtime configuration. Note that this will require the intermediate STDOUT to be written to a file and copied between containers so if this output is very large then keeping the processes in the same task may be more efficient. Finally, piping very large outputs requires a lot of memory so your container will need an appropriate allocation of memory. Use the most cost effective instance generation Fifth generation EC2 types ( c5 , m5 , r5 ) have a lower on demand price and have higher clock speeds than their 4th generation counterparts ( c4 , m4 , r4 ). Therefore, for on demand compute environments those instance types should be preferred. In spot compute environments we suggest using both 4th and 5th generation types as this increases the pool of available types meaning Batch will be able to choose the instance type that is cheapest and least likely to be interrupted. In addition, using all available availability zones for a AWS Batch compute environment will also give AWS Batch a wider selection of Spot instances to choose from.","title":"Cost Effective Workflows"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#optimizing-aws-batch-resources-for-cost-effective-genomics-workflows","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Tasks in a workflow run in Docker containers. Each task contains a runtime definition that specifies the number of CPUs and amount of RAM allocated to the task. runtime { docker: \"biocontainers/plink1.9:v1.90b6.6-181012-1-deb_cv1\" memory: \"8 GB\" cpu: 2 } The amount of resource allocated to each container ultimately impacts the cost to run a workflow. Optimally allocating resources leads to cost efficiency. The following suggestions can help to \u201cright size\u201d your workflow.","title":"Optimizing AWS Batch Resources for Cost Effective Genomics Workflows"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#optimize-the-longest-running-and-most-parallel-tasks-first","text":"With AWS Batch you pay for what you use, so it makes most sense to focus on those tasks that run the longest as well as those that have the largest scatters (parallel tasks) as they will make up the majority of the workflow runtime and contribute most to the cost.","title":"Optimize the longest running, and most parallel tasks first"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#enable-cloudwatch-container-insights","text":"CloudWatch container insights provides detailed monitoring of ECS clusters as well as containers running on those clusters. AWS Batch compute environments are managed ECS clusters, enabling container insights on those clusters will show how the cluster utilization during a workflow run. Currently, cluster insights can be enabled using the AWS CLI. To do this we need the name of the ECS cluster that is managed by the AWS Batch Compute Environment. From the AWS Batch console, identify the Batch Compute Environment(s) of interest 1. Click on the link of the environment and note the ECS Cluster Name 1. Enable cluster insights with the following command aws ecs update-cluster-settings --cluster **ECS-CLUSTER-NAME** --settings name=containerInsights,value=enabled","title":"Enable CloudWatch Container Insights"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#using-container-insights","text":"With Container Insights enabled metrics can be viewed in the CloudWatch console and used to determine if resources are under or over allocated. From the side bar of the CloudWatch console click Container Insights and View Performance Dashboards From the dropdown select ECS Clusters to view the Cluster utilization. Select a time range corresponding to a Cromwell Workflow run. Using the ECS Clusters Dashboard we can see the utilization of each EC2 worker in the cluster. The CPU utilization averages about 50% suggesting that the containers may not be efficiently packed (see below for tips on packing), the containers have too much CPU allocated or both. The memory utilization is very low so memory is over allocated. Container instance count shows the average number of EC2 workers hosting containers, and the task count is the number of tasks running in containers at that time. From the dropdown select ECS Tasks to view the container utilization. Select a time range corresponding to a Cromwell Workflow run. Using the ECS Tasks Dashboard we can gain insight into the average activity within the containers during the workflow. In this case we again see that memory is over allocated and CPU could potentially be reduced.","title":"Using Container Insights"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#correlating-tasks-with-metrics","text":"To most effectively use CloudWatch Container Insight metrics it is important to know the time at which each workflow task started and finished. You can get this information from the Metadata API of your Cromwell server. The following will return the metadata of the workflow run as a JSON object. curl -k -X GET \"https://my-cromwell-server/api/workflows/v1/WORKFLOW-ID/metadata\"","title":"Correlating tasks with metrics"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#consider-cpu-and-memory-ratios","text":"EC2 workers for Cromwell AWS Batch compute environments are c , m , and r instance families that have vCPU to memory ratios of 1:2, 1:4 and 1:8 respectively. AWS Batch will attempt to fit containers to instances in the most optimal way depending on cost and size requirements. Given that a task requiring 16GB of RAM that could make use of all available CPUs, then to optimally pack the containers you should specify either 2, 4, or 8 vCPU. Other values could lead to inefficient packing meaning the resources of the EC2 container instance will be paid for but not optimally used. NOTE: Fully packing an instance can result in it becoming unresponsive if the tasks in the containers use 100% (or more if they start swapping) of the allocated resources. The instance may then be unresponsive to Batch and may time out. To avoid this, always allow for a little overhead.","title":"Consider CPU and memory ratios"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#split-tasks-that-pipe-output","text":"If a workflow task consists of a process that pipes STDOUT to another process then both processes will run in the same container and receive the same resources. If one task requires more resources than the other this might be inefficient and might be better divided into two tasks each with its own runtime configuration. Note that this will require the intermediate STDOUT to be written to a file and copied between containers so if this output is very large then keeping the processes in the same task may be more efficient. Finally, piping very large outputs requires a lot of memory so your container will need an appropriate allocation of memory.","title":"Split tasks that pipe output"},{"location":"orchestration/cost-effective-workflows/cost-effective-workflows.html#use-the-most-cost-effective-instance-generation","text":"Fifth generation EC2 types ( c5 , m5 , r5 ) have a lower on demand price and have higher clock speeds than their 4th generation counterparts ( c4 , m4 , r4 ). Therefore, for on demand compute environments those instance types should be preferred. In spot compute environments we suggest using both 4th and 5th generation types as this increases the pool of available types meaning Batch will be able to choose the instance type that is cheapest and least likely to be interrupted. In addition, using all available availability zones for a AWS Batch compute environment will also give AWS Batch a wider selection of Spot instances to choose from.","title":"Use the most cost effective instance generation"},{"location":"orchestration/cromwell/cromwell-examples.html","text":"Cromwell Examples DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some example workflows you can use to test Cromwell on AWS. The curl commands assume that you have access to a Cromwell server via localhost:8000 . Simple Hello World This is a single file workflow. It simply echos \"Hello AWS!\" to stdout and exits. Workflow Definition simple-hello.wdl task echoHello{ command { echo \"Hello AWS!\" } runtime { docker: \"ubuntu:latest\" } } workflow printHelloAndGoodbye { call echoHello } Running the workflow To submit this workflow via curl use the following command: $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@/path/to/simple-hello.wdl\" You should receive a response like the following: {\"id\":\"104d9ade-6461-40e7-bc4e-227c3a49e98b\",\"status\":\"Submitted\"} If the workflow completes successfully, the server will log the following: 2018-09-21 04:07:42,928 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-7eefeeed-157e-4307-9267-9b4d716874e5 [UUID(7eefeeed)]: Workflow w complete. Final Outputs: { \"w.echo.f\": \"s3://aws-cromwell-test-us-east-1/cromwell-execution/w/7eefeeed-157e-4307-9267-9b4d716874e5/call-echo/echo-stdout.log\" } 2018-09-21 04:07:42,931 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowManagerActor WorkflowActor-7eefeeed-157e-4307-9267-9b4d716874e5 is in a terminal state: WorkflowSucceededState Call Caching If you submit the same job again Cromwell will find in the metadata database that the previous call to the echoHello task was completed successfully (a cache hit). Rather than submitting the job to AWS Batch the server will simply copy the previous result You can disable call caching on a single workflow by providing a JSON options file: { \"write_to_cache\": false, \"read_from_cache\": false } This file may be submitted along with the workflow: curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowOptions=@options.json\" Hello World with inputs This workflow is virtually the same as the single file workflow above, but uses an input file to define parameters in the workflow. Workflow Definition hello-aws.wdl task hello { String addressee command { echo \"Hello ${addressee}! Welcome to Cromwell . . . on AWS!\" } output { String message = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" } } workflow wf_hello { call hello output { hello.message } } Inputs hello-aws.json { \"wf_hello.hello.addressee\": \"World!\" } Running the workflow Submit this workflow using: $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@hello-aws.wdl\" \\ -F \"workflowInputs=@hello-aws.json\" Using data on S3 This workflow demonstrates how to use data from S3. First, create some data: $ curl \"https://baconipsum.com/api/?type=all-meat&paras=1&format=text\" > meats.txt and upload it to an S3 bucket accessible using the Cromwell server's IAM policy: $ aws s3 cp meats.txt s3://<your-bucket-name>/ Create the following wdl and input json files. Workflow Definition s3inputs.wdl task read_file { File file command { cat ${file} } output { String contents = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" } } workflow ReadFile { call read_file output { read_file.contents } } Inputs s3inputs.json { \"ReadFile.read_file.file\": \"s3://aws-cromwell-test-us-east-1/meats.txt\" } Running the workflow Submit the workflow via curl : $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@s3inputs.wdl\" \\ -F \"workflowInputs=@s3inputs.json\" If successful the server should log the following: 2018-09-21 05:04:15,478 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-1774c9a2-12bf-42ea-902d-3dbe2a70a116 [UUID(1774c9a2)]: Workflow ReadFile complete. Final Outputs: { \"ReadFile.read_file.contents\": \"Strip steak venison leberkas sausage fatback pork belly short ribs. Tail fatback prosciutto meatball sausage filet mignon tri-tip porchetta cupim doner boudin. Meatloaf jerky short loin turkey beef kielbasa kevin cupim burgdoggen short ribs spare ribs flank doner chuck. Cupim prosciutto jerky leberkas pork loin pastrami. Chuck ham pork loin, prosciutto filet mignon kevin brisket corned beef short loin shoulder jowl porchetta venison. Hamburger ham hock tail swine andouille beef ribs t-bone turducken tenderloin burgdoggen capicola frankfurter sirloin ham.\" } 2018-09-21 05:04:15,481 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowManagerActor WorkflowActor-1774c9a2-12bf-42ea-902d-3dbe2a70a116 is in a terminal state: WorkflowSucceededState Real-world example: HaplotypeCaller This example demonstrates how to use Cromwell with the AWS backend to run GATK4 HaplotypeCaller against public data in S3. The HaplotypeCaller tool is one of the primary steps in GATK best practices pipeline. The source for these files can be found in Cromwell's test suite on GitHub . Worflow Definition HaplotypeCaller.aws.wdl ## Copyright Broad Institute, 2017 ## ## This WDL workflow runs HaplotypeCaller from GATK4 in GVCF mode on a single sample ## according to the GATK Best Practices (June 2016), scattered across intervals. ## ## Requirements/expectations : ## - One analysis-ready BAM file for a single sample (as identified in RG:SM) ## - Set of variant calling intervals lists for the scatter, provided in a file ## ## Outputs : ## - One GVCF file and its index ## ## Cromwell version support ## - Successfully tested on v29 ## - Does not work on versions < v23 due to output syntax ## ## IMPORTANT NOTE: HaplotypeCaller in GATK4 is still in evaluation phase and should not ## be used in production until it has been fully vetted. In the meantime, use the GATK3 ## version for any production needs. ## ## Runtime parameters are optimized for Broad's Google Cloud Platform implementation. ## ## LICENSING : ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ## be subject to different licenses. Users are responsible for checking that they are ## authorized to run all programs before running this script. Please see the dockers ## for detailed licensing information pertaining to the included programs. # WORKFLOW DEFINITION workflow HaplotypeCallerGvcf_GATK4 { File input_bam File input_bam_index File ref_dict File ref_fasta File ref_fasta_index File scattered_calling_intervals_list String gatk_docker String gatk_path Array[File] scattered_calling_intervals = read_lines(scattered_calling_intervals_list) String sample_basename = basename(input_bam, \".bam\") String gvcf_name = sample_basename + \".g.vcf.gz\" String gvcf_index = sample_basename + \".g.vcf.gz.tbi\" # Call variants in parallel over grouped calling intervals scatter (interval_file in scattered_calling_intervals) { # Generate GVCF by interval call HaplotypeCaller { input: input_bam = input_bam, input_bam_index = input_bam_index, interval_list = interval_file, gvcf_name = gvcf_name, ref_dict = ref_dict, ref_fasta = ref_fasta, ref_fasta_index = ref_fasta_index, docker_image = gatk_docker, gatk_path = gatk_path } } # Merge per-interval GVCFs call MergeGVCFs { input: input_vcfs = HaplotypeCaller.output_gvcf, vcf_name = gvcf_name, vcf_index = gvcf_index, docker_image = gatk_docker, gatk_path = gatk_path } # Outputs that will be retained when execution is complete output { File output_merged_gvcf = MergeGVCFs.output_vcf File output_merged_gvcf_index = MergeGVCFs.output_vcf_index } } # TASK DEFINITIONS # HaplotypeCaller per-sample in GVCF mode task HaplotypeCaller { File input_bam File input_bam_index String gvcf_name File ref_dict File ref_fasta File ref_fasta_index File interval_list Int? interval_padding Float? contamination Int? max_alt_alleles String mem_size String docker_image String gatk_path String java_opt command { ${gatk_path} --java-options ${java_opt} \\ HaplotypeCaller \\ -R ${ref_fasta} \\ -I ${input_bam} \\ -O ${gvcf_name} \\ -L ${interval_list} \\ -ip ${default=100 interval_padding} \\ -contamination ${default=0 contamination} \\ --max-alternate-alleles ${default=3 max_alt_alleles} \\ -ERC GVCF } runtime { docker: docker_image memory: mem_size cpu: 1 } output { File output_gvcf = \"${gvcf_name}\" } } # Merge GVCFs generated per-interval for the same sample task MergeGVCFs { Array [File] input_vcfs String vcf_name String vcf_index String mem_size String docker_image String gatk_path String java_opt command { ${gatk_path} --java-options ${java_opt} \\ MergeVcfs \\ --INPUT=${sep=' --INPUT=' input_vcfs} \\ --OUTPUT=${vcf_name} } runtime { docker: docker_image memory: mem_size cpu: 1 } output { File output_vcf = \"${vcf_name}\" File output_vcf_index = \"${vcf_index}\" } } Inputs The inputs for this workflow reference public data on S3 that is hosted by AWS as part of the AWS Public Dataset Program . HaplotypeCaller.aws.json { \"##_COMMENT1\": \"INPUT BAM\", \"HaplotypeCallerGvcf_GATK4.input_bam\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bam\", \"HaplotypeCallerGvcf_GATK4.input_bam_index\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bai\", \"##_COMMENT2\": \"REFERENCE FILES\", \"HaplotypeCallerGvcf_GATK4.ref_dict\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dict\", \"HaplotypeCallerGvcf_GATK4.ref_fasta\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\", \"HaplotypeCallerGvcf_GATK4.ref_fasta_index\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\", \"##_COMMENT3\": \"INTERVALS\", \"HaplotypeCallerGvcf_GATK4.scattered_calling_intervals_list\": \"s3://gatk-test-data/intervals/hg38_wgs_scattered_calling_intervals.txt\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.interval_padding\": 100, \"##_COMMENT4\": \"DOCKERS\", \"HaplotypeCallerGvcf_GATK4.gatk_docker\": \"broadinstitute/gatk:4.0.0.0\", \"##_COMMENT5\": \"PATHS\", \"HaplotypeCallerGvcf_GATK4.gatk_path\": \"/gatk/gatk\", \"##_COMMENT6\": \"JAVA OPTIONS\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.java_opt\": \"-Xms8000m\", \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.java_opt\": \"-Xms8000m\", \"##_COMMENT7\": \"MEMORY ALLOCATION\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.mem_size\": \"10 GB\", \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.mem_size\": \"30 GB\", } Running the workflow Submit the workflow via curl : $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@HaplotypeCaller.aws.wdl\" \\ -F \"workflowInputs=@HaplotypeCaller.aws.json\" This workflow takes about 60-90min to complete.","title":"Examples"},{"location":"orchestration/cromwell/cromwell-examples.html#cromwell-examples","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some example workflows you can use to test Cromwell on AWS. The curl commands assume that you have access to a Cromwell server via localhost:8000 .","title":"Cromwell Examples"},{"location":"orchestration/cromwell/cromwell-examples.html#simple-hello-world","text":"This is a single file workflow. It simply echos \"Hello AWS!\" to stdout and exits.","title":"Simple Hello World"},{"location":"orchestration/cromwell/cromwell-examples.html#workflow-definition","text":"simple-hello.wdl task echoHello{ command { echo \"Hello AWS!\" } runtime { docker: \"ubuntu:latest\" } } workflow printHelloAndGoodbye { call echoHello }","title":"Workflow Definition"},{"location":"orchestration/cromwell/cromwell-examples.html#running-the-workflow","text":"To submit this workflow via curl use the following command: $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@/path/to/simple-hello.wdl\" You should receive a response like the following: {\"id\":\"104d9ade-6461-40e7-bc4e-227c3a49e98b\",\"status\":\"Submitted\"} If the workflow completes successfully, the server will log the following: 2018-09-21 04:07:42,928 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-7eefeeed-157e-4307-9267-9b4d716874e5 [UUID(7eefeeed)]: Workflow w complete. Final Outputs: { \"w.echo.f\": \"s3://aws-cromwell-test-us-east-1/cromwell-execution/w/7eefeeed-157e-4307-9267-9b4d716874e5/call-echo/echo-stdout.log\" } 2018-09-21 04:07:42,931 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowManagerActor WorkflowActor-7eefeeed-157e-4307-9267-9b4d716874e5 is in a terminal state: WorkflowSucceededState","title":"Running the workflow"},{"location":"orchestration/cromwell/cromwell-examples.html#call-caching","text":"If you submit the same job again Cromwell will find in the metadata database that the previous call to the echoHello task was completed successfully (a cache hit). Rather than submitting the job to AWS Batch the server will simply copy the previous result You can disable call caching on a single workflow by providing a JSON options file: { \"write_to_cache\": false, \"read_from_cache\": false } This file may be submitted along with the workflow: curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowOptions=@options.json\"","title":"Call Caching"},{"location":"orchestration/cromwell/cromwell-examples.html#hello-world-with-inputs","text":"This workflow is virtually the same as the single file workflow above, but uses an input file to define parameters in the workflow.","title":"Hello World with inputs"},{"location":"orchestration/cromwell/cromwell-examples.html#workflow-definition_1","text":"hello-aws.wdl task hello { String addressee command { echo \"Hello ${addressee}! Welcome to Cromwell . . . on AWS!\" } output { String message = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" } } workflow wf_hello { call hello output { hello.message } }","title":"Workflow Definition"},{"location":"orchestration/cromwell/cromwell-examples.html#inputs","text":"hello-aws.json { \"wf_hello.hello.addressee\": \"World!\" }","title":"Inputs"},{"location":"orchestration/cromwell/cromwell-examples.html#running-the-workflow_1","text":"Submit this workflow using: $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@hello-aws.wdl\" \\ -F \"workflowInputs=@hello-aws.json\"","title":"Running the workflow"},{"location":"orchestration/cromwell/cromwell-examples.html#using-data-on-s3","text":"This workflow demonstrates how to use data from S3. First, create some data: $ curl \"https://baconipsum.com/api/?type=all-meat&paras=1&format=text\" > meats.txt and upload it to an S3 bucket accessible using the Cromwell server's IAM policy: $ aws s3 cp meats.txt s3://<your-bucket-name>/ Create the following wdl and input json files.","title":"Using data on S3"},{"location":"orchestration/cromwell/cromwell-examples.html#workflow-definition_2","text":"s3inputs.wdl task read_file { File file command { cat ${file} } output { String contents = read_string(stdout()) } runtime { docker: \"ubuntu:latest\" } } workflow ReadFile { call read_file output { read_file.contents } }","title":"Workflow Definition"},{"location":"orchestration/cromwell/cromwell-examples.html#inputs_1","text":"s3inputs.json { \"ReadFile.read_file.file\": \"s3://aws-cromwell-test-us-east-1/meats.txt\" }","title":"Inputs"},{"location":"orchestration/cromwell/cromwell-examples.html#running-the-workflow_2","text":"Submit the workflow via curl : $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@s3inputs.wdl\" \\ -F \"workflowInputs=@s3inputs.json\" If successful the server should log the following: 2018-09-21 05:04:15,478 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - WorkflowExecutionActor-1774c9a2-12bf-42ea-902d-3dbe2a70a116 [UUID(1774c9a2)]: Workflow ReadFile complete. Final Outputs: { \"ReadFile.read_file.contents\": \"Strip steak venison leberkas sausage fatback pork belly short ribs. Tail fatback prosciutto meatball sausage filet mignon tri-tip porchetta cupim doner boudin. Meatloaf jerky short loin turkey beef kielbasa kevin cupim burgdoggen short ribs spare ribs flank doner chuck. Cupim prosciutto jerky leberkas pork loin pastrami. Chuck ham pork loin, prosciutto filet mignon kevin brisket corned beef short loin shoulder jowl porchetta venison. Hamburger ham hock tail swine andouille beef ribs t-bone turducken tenderloin burgdoggen capicola frankfurter sirloin ham.\" } 2018-09-21 05:04:15,481 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - WorkflowManagerActor WorkflowActor-1774c9a2-12bf-42ea-902d-3dbe2a70a116 is in a terminal state: WorkflowSucceededState","title":"Running the workflow"},{"location":"orchestration/cromwell/cromwell-examples.html#real-world-example-haplotypecaller","text":"This example demonstrates how to use Cromwell with the AWS backend to run GATK4 HaplotypeCaller against public data in S3. The HaplotypeCaller tool is one of the primary steps in GATK best practices pipeline. The source for these files can be found in Cromwell's test suite on GitHub .","title":"Real-world example: HaplotypeCaller"},{"location":"orchestration/cromwell/cromwell-examples.html#worflow-definition","text":"HaplotypeCaller.aws.wdl ## Copyright Broad Institute, 2017 ## ## This WDL workflow runs HaplotypeCaller from GATK4 in GVCF mode on a single sample ## according to the GATK Best Practices (June 2016), scattered across intervals. ## ## Requirements/expectations : ## - One analysis-ready BAM file for a single sample (as identified in RG:SM) ## - Set of variant calling intervals lists for the scatter, provided in a file ## ## Outputs : ## - One GVCF file and its index ## ## Cromwell version support ## - Successfully tested on v29 ## - Does not work on versions < v23 due to output syntax ## ## IMPORTANT NOTE: HaplotypeCaller in GATK4 is still in evaluation phase and should not ## be used in production until it has been fully vetted. In the meantime, use the GATK3 ## version for any production needs. ## ## Runtime parameters are optimized for Broad's Google Cloud Platform implementation. ## ## LICENSING : ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ## be subject to different licenses. Users are responsible for checking that they are ## authorized to run all programs before running this script. Please see the dockers ## for detailed licensing information pertaining to the included programs. # WORKFLOW DEFINITION workflow HaplotypeCallerGvcf_GATK4 { File input_bam File input_bam_index File ref_dict File ref_fasta File ref_fasta_index File scattered_calling_intervals_list String gatk_docker String gatk_path Array[File] scattered_calling_intervals = read_lines(scattered_calling_intervals_list) String sample_basename = basename(input_bam, \".bam\") String gvcf_name = sample_basename + \".g.vcf.gz\" String gvcf_index = sample_basename + \".g.vcf.gz.tbi\" # Call variants in parallel over grouped calling intervals scatter (interval_file in scattered_calling_intervals) { # Generate GVCF by interval call HaplotypeCaller { input: input_bam = input_bam, input_bam_index = input_bam_index, interval_list = interval_file, gvcf_name = gvcf_name, ref_dict = ref_dict, ref_fasta = ref_fasta, ref_fasta_index = ref_fasta_index, docker_image = gatk_docker, gatk_path = gatk_path } } # Merge per-interval GVCFs call MergeGVCFs { input: input_vcfs = HaplotypeCaller.output_gvcf, vcf_name = gvcf_name, vcf_index = gvcf_index, docker_image = gatk_docker, gatk_path = gatk_path } # Outputs that will be retained when execution is complete output { File output_merged_gvcf = MergeGVCFs.output_vcf File output_merged_gvcf_index = MergeGVCFs.output_vcf_index } } # TASK DEFINITIONS # HaplotypeCaller per-sample in GVCF mode task HaplotypeCaller { File input_bam File input_bam_index String gvcf_name File ref_dict File ref_fasta File ref_fasta_index File interval_list Int? interval_padding Float? contamination Int? max_alt_alleles String mem_size String docker_image String gatk_path String java_opt command { ${gatk_path} --java-options ${java_opt} \\ HaplotypeCaller \\ -R ${ref_fasta} \\ -I ${input_bam} \\ -O ${gvcf_name} \\ -L ${interval_list} \\ -ip ${default=100 interval_padding} \\ -contamination ${default=0 contamination} \\ --max-alternate-alleles ${default=3 max_alt_alleles} \\ -ERC GVCF } runtime { docker: docker_image memory: mem_size cpu: 1 } output { File output_gvcf = \"${gvcf_name}\" } } # Merge GVCFs generated per-interval for the same sample task MergeGVCFs { Array [File] input_vcfs String vcf_name String vcf_index String mem_size String docker_image String gatk_path String java_opt command { ${gatk_path} --java-options ${java_opt} \\ MergeVcfs \\ --INPUT=${sep=' --INPUT=' input_vcfs} \\ --OUTPUT=${vcf_name} } runtime { docker: docker_image memory: mem_size cpu: 1 } output { File output_vcf = \"${vcf_name}\" File output_vcf_index = \"${vcf_index}\" } }","title":"Worflow Definition"},{"location":"orchestration/cromwell/cromwell-examples.html#inputs_2","text":"The inputs for this workflow reference public data on S3 that is hosted by AWS as part of the AWS Public Dataset Program . HaplotypeCaller.aws.json { \"##_COMMENT1\": \"INPUT BAM\", \"HaplotypeCallerGvcf_GATK4.input_bam\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bam\", \"HaplotypeCallerGvcf_GATK4.input_bam_index\": \"s3://gatk-test-data/wgs_bam/NA12878_24RG_hg38/NA12878_24RG_small.hg38.bai\", \"##_COMMENT2\": \"REFERENCE FILES\", \"HaplotypeCallerGvcf_GATK4.ref_dict\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.dict\", \"HaplotypeCallerGvcf_GATK4.ref_fasta\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta\", \"HaplotypeCallerGvcf_GATK4.ref_fasta_index\": \"s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai\", \"##_COMMENT3\": \"INTERVALS\", \"HaplotypeCallerGvcf_GATK4.scattered_calling_intervals_list\": \"s3://gatk-test-data/intervals/hg38_wgs_scattered_calling_intervals.txt\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.interval_padding\": 100, \"##_COMMENT4\": \"DOCKERS\", \"HaplotypeCallerGvcf_GATK4.gatk_docker\": \"broadinstitute/gatk:4.0.0.0\", \"##_COMMENT5\": \"PATHS\", \"HaplotypeCallerGvcf_GATK4.gatk_path\": \"/gatk/gatk\", \"##_COMMENT6\": \"JAVA OPTIONS\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.java_opt\": \"-Xms8000m\", \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.java_opt\": \"-Xms8000m\", \"##_COMMENT7\": \"MEMORY ALLOCATION\", \"HaplotypeCallerGvcf_GATK4.HaplotypeCaller.mem_size\": \"10 GB\", \"HaplotypeCallerGvcf_GATK4.MergeGVCFs.mem_size\": \"30 GB\", }","title":"Inputs"},{"location":"orchestration/cromwell/cromwell-examples.html#running-the-workflow_3","text":"Submit the workflow via curl : $ curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@HaplotypeCaller.aws.wdl\" \\ -F \"workflowInputs=@HaplotypeCaller.aws.json\" This workflow takes about 60-90min to complete.","title":"Running the workflow"},{"location":"orchestration/cromwell/cromwell-overview.html","text":"Cromwell on AWS Batch DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Cromwell is a workflow management system for scientific workflows developed by the Broad Institute and supports job execution using AWS Batch . Requirements To get started using Cromwell on AWS you'll need the following setup in your AWS account: A VPC with at least 2 private subnets The Genomics Workflow Core Environment EC2 Instance as a Cromwell Server RDS Cluster for the Cromwell metadata database The following will help you deploy these components. VPC Cromwell uses a relational database for storing metadata information. In AWS you can use an RDS cluster for this. For security and availability is is recommended that your RDS cluster deploy into at least 2 private subnets. If the target VPC you want to deploy Cromwell into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow Genomics Workflow Core To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture. Cromwell Resources The following CloudFormation template will create a Cromwell server instance and, an RDS Aurora Serverless database cluster. Name Description Source Launch Stack Cromwell Resource Create resources needed to run Cromwell on AWS: an RDS Aurora database, an EC2 instance with Cromwell installed as a server, and an IAM instance profile cloud_download play_arrow Important The Namespace parameter in this template configures Cromwell and associates it with a specific Genomics Workflow Core. Once created, you can access the server instance in a web browser via the instance's public DNS name which can be found on the Outputs tab for the stack in the CloudFormation Console. There you should see Cromwell's SwaggerUI, which provides a simple web interface for submitting workflows. Info The server instance uses a self-signed certificate and is configured for HTTPS access. You may get a security warning from your web-browser when accessing it. In a production setting, it is recommended to install a certificate from a trusted authority. The CloudFormation template above also configures the server with integration to Amazon CloudWatch for monitoring Cromwell's log output and AWS Systems Manager . The private key that you referenced in the CloudFormation template allows SSH terminal access and performing any maintenance on the instance. In addition the server instance can be managed from AWS Systems Manager. Deployment Details Cromwell Database Cromwell uses a relational database to store workflow metadata and caching information. By default, Cromwell will use an in-memory database, which is sufficient for ephemeral, single workflow use. However, for more scalability and to fully take advantage of workflow caching capabilities, it is recommended to use a dedicated and persistent database that is separate from the instance Cromwell is running on. The CloudFormation template will deploy an RDS Aurora MySQL instance and configure the server to connect to this DB Cromwell server The CloudFormation template above launches an EC2 instance as a persistent Cromwell server. You can use this server as an endpoint for running multiple concurrent workflows. This instance needs the following: Java 8 The latest version of Cromwell with AWS Batch backend support (v52+) Permissions to read from all S3 buckets used for input and output data submit / describe / cancel / terminate jobs on AWS Batch queues These permissions are granted to the server instance via an instance profile . This allows an EC2 instance to assume an IAM role and call other AWS services on your behalf. The specific IAM policies used in the instance profile are shown below. Access to AWS Batch Lets the Cromwell server instance submit and get info about AWS Batch jobs. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CromwellServer-BatchPolicy\", \"Effect\": \"Allow\", \"Action\": [ \"batch:DescribeJobQueues\" \"batch:DeregisterJobDefinition\" \"batch:TerminateJob\" \"batch:DescribeJobs\" \"batch:CancelJob\" \"batch:SubmitJob\" \"batch:RegisterJobDefinition\" \"batch:DescribeJobDefinitions\" \"batch:ListJobs\" \"batch:DescribeComputeEnvironments\" ], \"Resource\": \"*\" } ] } If you want to further limit Cromwell's access to compute resources - e.g. to specific job queues - you can scope down the above policy as needed by explicitly specifying Resources . Access to S3 Lets the Cromwell server instance read and write data from/to S3. Specifically, Cromwell needs access to the return code files ( rc.txt ) generated by each job to track job status. In addition, this should also include any open dataset buckets you may need to read from for your workflows since Cromwell will need to perform HeadObject and ListBucket operations when determining job inputs for tasks. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CromwellServer-S3Policy\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::<bucket-name>\", \"arn:aws:s3:::<bucket-name>/*\", ] } ] } Configuring Cromwell to use AWS Batch The following is an example *.conf file to use the AWSBackend . // cromwell.conf include required(classpath(\"application\")) webservice { interface = localhost port = 8000 } aws { application-name = \"cromwell\" auths = [{ name = \"default\" scheme = \"default\" }] region = \"<your-region>\" } database { profile = \"slick.jdbc.MySQLProfile$\" db { driver = \"com.mysql.cj.jdbc.Driver\" url = \"<db-url>\" user = \"cromwell\" password = \"<cromwell_password>\" connectionTimeout = 5000 } } call-caching { enabled = true invalidate-bad-cache-results = true } engine { filesystems { s3 { auth = \"default\" } } } backend { default = \"AWSBATCH\" providers { AWSBATCH { actor-factory = \"cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory\" config { numSubmitAttempts = 10 numCreateDefinitionAttempts = 10 root = \"s3://<your-s3-bucket-name>\" auth = \"default\" default-runtime-attributes { queueArn = \"<your-queue-arn>\" , scriptBucketName = \"<your-bucket-name>\" } filesystems { s3 { auth = \"default\" duplication-strategy: [ \"hard-link\", \"soft-link\", \"copy\" ] } } } } } } The above file uses the default credential provider chain for authorization. Replace the following with values appropriate for your account and workload: <your region> : the AWS region your S3 bucket and AWS Batch environment are deployed into - e.g. us-east-1 <db-url> : the JDBC url of the Cromwell metadata database <cromwell-password> : the password of the cromwell user in the metadata database. This value can also be supplied as a Java command line variable. <your-s3-bucket-name> : the name of the S3 bucket you will use for inputs and outputs from tasks in the workflow. <your-queue-arn> : the Amazon Resource Name of the AWS Batch queue you want to use for your tasks. Accessing the Cromwell server The Cromwell EC2 instance may be accessed using the AWS Session Manager via console or in the terminal with the command aws ssm start-session --target <instance-id> . Please note that by default this will log you in as user ec2-user in the directory /usr/bin . You may prefer to become the ec2-user with the command sudo su - ec2-user which will switch you to that user's home directory. Stop / Start / Restart the Cromwell service The CloudFormation template above installs Cromwell as a service under the control of supervisorctl . If you need to make changes to the cromwell.conf file you will want to restart the service so that configuration changes are included. supervisorctl restart cromwell-server supervisorctl start and supervisorctl stop are also supported. Running a workflow To submit a workflow to your Cromwell server, you can use any of the following: Cromwell's SwaggerUI in a web-browser a REST client like Insomnia or Postman the command line with curl for example: curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowInputs=@inputs.json\" \\ -F \"workflowOptions=@options.json\" Workflow logs After submitting a workflow, you can monitor the progress of tasks via the AWS Batch console. Cromwell server logs are captured in the cromwell_server log group and the logs of the AWS Batch jobs that run each task in the workflow can be found in the /aws/batch/jobs CloudWatch log group. The next section provides some examples of running Crommwell on AWS. Cost optimizing workflows Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Overview"},{"location":"orchestration/cromwell/cromwell-overview.html#cromwell-on-aws-batch","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Cromwell is a workflow management system for scientific workflows developed by the Broad Institute and supports job execution using AWS Batch .","title":"Cromwell on AWS Batch"},{"location":"orchestration/cromwell/cromwell-overview.html#requirements","text":"To get started using Cromwell on AWS you'll need the following setup in your AWS account: A VPC with at least 2 private subnets The Genomics Workflow Core Environment EC2 Instance as a Cromwell Server RDS Cluster for the Cromwell metadata database The following will help you deploy these components.","title":"Requirements"},{"location":"orchestration/cromwell/cromwell-overview.html#vpc","text":"Cromwell uses a relational database for storing metadata information. In AWS you can use an RDS cluster for this. For security and availability is is recommended that your RDS cluster deploy into at least 2 private subnets. If the target VPC you want to deploy Cromwell into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow","title":"VPC"},{"location":"orchestration/cromwell/cromwell-overview.html#genomics-workflow-core","text":"To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture.","title":"Genomics Workflow Core"},{"location":"orchestration/cromwell/cromwell-overview.html#cromwell-resources","text":"The following CloudFormation template will create a Cromwell server instance and, an RDS Aurora Serverless database cluster. Name Description Source Launch Stack Cromwell Resource Create resources needed to run Cromwell on AWS: an RDS Aurora database, an EC2 instance with Cromwell installed as a server, and an IAM instance profile cloud_download play_arrow Important The Namespace parameter in this template configures Cromwell and associates it with a specific Genomics Workflow Core. Once created, you can access the server instance in a web browser via the instance's public DNS name which can be found on the Outputs tab for the stack in the CloudFormation Console. There you should see Cromwell's SwaggerUI, which provides a simple web interface for submitting workflows. Info The server instance uses a self-signed certificate and is configured for HTTPS access. You may get a security warning from your web-browser when accessing it. In a production setting, it is recommended to install a certificate from a trusted authority. The CloudFormation template above also configures the server with integration to Amazon CloudWatch for monitoring Cromwell's log output and AWS Systems Manager . The private key that you referenced in the CloudFormation template allows SSH terminal access and performing any maintenance on the instance. In addition the server instance can be managed from AWS Systems Manager.","title":"Cromwell Resources"},{"location":"orchestration/cromwell/cromwell-overview.html#deployment-details","text":"","title":"Deployment Details"},{"location":"orchestration/cromwell/cromwell-overview.html#cromwell-database","text":"Cromwell uses a relational database to store workflow metadata and caching information. By default, Cromwell will use an in-memory database, which is sufficient for ephemeral, single workflow use. However, for more scalability and to fully take advantage of workflow caching capabilities, it is recommended to use a dedicated and persistent database that is separate from the instance Cromwell is running on. The CloudFormation template will deploy an RDS Aurora MySQL instance and configure the server to connect to this DB","title":"Cromwell Database"},{"location":"orchestration/cromwell/cromwell-overview.html#cromwell-server","text":"The CloudFormation template above launches an EC2 instance as a persistent Cromwell server. You can use this server as an endpoint for running multiple concurrent workflows. This instance needs the following: Java 8 The latest version of Cromwell with AWS Batch backend support (v52+) Permissions to read from all S3 buckets used for input and output data submit / describe / cancel / terminate jobs on AWS Batch queues These permissions are granted to the server instance via an instance profile . This allows an EC2 instance to assume an IAM role and call other AWS services on your behalf. The specific IAM policies used in the instance profile are shown below.","title":"Cromwell server"},{"location":"orchestration/cromwell/cromwell-overview.html#access-to-aws-batch","text":"Lets the Cromwell server instance submit and get info about AWS Batch jobs. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CromwellServer-BatchPolicy\", \"Effect\": \"Allow\", \"Action\": [ \"batch:DescribeJobQueues\" \"batch:DeregisterJobDefinition\" \"batch:TerminateJob\" \"batch:DescribeJobs\" \"batch:CancelJob\" \"batch:SubmitJob\" \"batch:RegisterJobDefinition\" \"batch:DescribeJobDefinitions\" \"batch:ListJobs\" \"batch:DescribeComputeEnvironments\" ], \"Resource\": \"*\" } ] } If you want to further limit Cromwell's access to compute resources - e.g. to specific job queues - you can scope down the above policy as needed by explicitly specifying Resources .","title":"Access to AWS Batch"},{"location":"orchestration/cromwell/cromwell-overview.html#access-to-s3","text":"Lets the Cromwell server instance read and write data from/to S3. Specifically, Cromwell needs access to the return code files ( rc.txt ) generated by each job to track job status. In addition, this should also include any open dataset buckets you may need to read from for your workflows since Cromwell will need to perform HeadObject and ListBucket operations when determining job inputs for tasks. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CromwellServer-S3Policy\", \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::<bucket-name>\", \"arn:aws:s3:::<bucket-name>/*\", ] } ] }","title":"Access to S3"},{"location":"orchestration/cromwell/cromwell-overview.html#configuring-cromwell-to-use-aws-batch","text":"The following is an example *.conf file to use the AWSBackend . // cromwell.conf include required(classpath(\"application\")) webservice { interface = localhost port = 8000 } aws { application-name = \"cromwell\" auths = [{ name = \"default\" scheme = \"default\" }] region = \"<your-region>\" } database { profile = \"slick.jdbc.MySQLProfile$\" db { driver = \"com.mysql.cj.jdbc.Driver\" url = \"<db-url>\" user = \"cromwell\" password = \"<cromwell_password>\" connectionTimeout = 5000 } } call-caching { enabled = true invalidate-bad-cache-results = true } engine { filesystems { s3 { auth = \"default\" } } } backend { default = \"AWSBATCH\" providers { AWSBATCH { actor-factory = \"cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory\" config { numSubmitAttempts = 10 numCreateDefinitionAttempts = 10 root = \"s3://<your-s3-bucket-name>\" auth = \"default\" default-runtime-attributes { queueArn = \"<your-queue-arn>\" , scriptBucketName = \"<your-bucket-name>\" } filesystems { s3 { auth = \"default\" duplication-strategy: [ \"hard-link\", \"soft-link\", \"copy\" ] } } } } } } The above file uses the default credential provider chain for authorization. Replace the following with values appropriate for your account and workload: <your region> : the AWS region your S3 bucket and AWS Batch environment are deployed into - e.g. us-east-1 <db-url> : the JDBC url of the Cromwell metadata database <cromwell-password> : the password of the cromwell user in the metadata database. This value can also be supplied as a Java command line variable. <your-s3-bucket-name> : the name of the S3 bucket you will use for inputs and outputs from tasks in the workflow. <your-queue-arn> : the Amazon Resource Name of the AWS Batch queue you want to use for your tasks.","title":"Configuring Cromwell to use AWS Batch"},{"location":"orchestration/cromwell/cromwell-overview.html#accessing-the-cromwell-server","text":"The Cromwell EC2 instance may be accessed using the AWS Session Manager via console or in the terminal with the command aws ssm start-session --target <instance-id> . Please note that by default this will log you in as user ec2-user in the directory /usr/bin . You may prefer to become the ec2-user with the command sudo su - ec2-user which will switch you to that user's home directory.","title":"Accessing the Cromwell server"},{"location":"orchestration/cromwell/cromwell-overview.html#stop-start-restart-the-cromwell-service","text":"The CloudFormation template above installs Cromwell as a service under the control of supervisorctl . If you need to make changes to the cromwell.conf file you will want to restart the service so that configuration changes are included. supervisorctl restart cromwell-server supervisorctl start and supervisorctl stop are also supported.","title":"Stop / Start / Restart the Cromwell service"},{"location":"orchestration/cromwell/cromwell-overview.html#running-a-workflow","text":"To submit a workflow to your Cromwell server, you can use any of the following: Cromwell's SwaggerUI in a web-browser a REST client like Insomnia or Postman the command line with curl for example: curl -X POST \"http://localhost:8000/api/workflows/v1\" \\ -H \"accept: application/json\" \\ -F \"workflowSource=@workflow.wdl\" \\ -F \"workflowInputs=@inputs.json\" \\ -F \"workflowOptions=@options.json\"","title":"Running a workflow"},{"location":"orchestration/cromwell/cromwell-overview.html#workflow-logs","text":"After submitting a workflow, you can monitor the progress of tasks via the AWS Batch console. Cromwell server logs are captured in the cromwell_server log group and the logs of the AWS Batch jobs that run each task in the workflow can be found in the /aws/batch/jobs CloudWatch log group. The next section provides some examples of running Crommwell on AWS.","title":"Workflow logs"},{"location":"orchestration/cromwell/cromwell-overview.html#cost-optimizing-workflows","text":"Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Cost optimizing workflows"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html","text":"Cromwell Troubleshooting DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some common errors that we have seen and suggested solutions S3 Access Denied (403) Possible Cause(s) A 403 error from S3 indicates that Cromwell is trying to access an S3 object that it doesn't have permission to. Following the priciple of \"least access\" Cromwell uses an IAM EC2 instance role that grants it read and write access to the S3 bucket you specified in the CloudFormation deployment and read only access to the gatk-test-data and broad-references S3 buckets. If your workflow references other S3 objects ( even ones in your account ) you will need to allow this via changes to the IAM role. Similarly if a step in your workflow attempts to write to another bucket you will need to add the appropriate permissions. Suggested Solution(s) Add read access to additional buckets by attaching a policy to the Cromwell server's IAM EC2 instance role with content similar to: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::bucket-a\", \"arn:aws:s3:::bucket-a/*\", \"arn:aws:s3:::another-bucket\", \"arn:aws:s3:::another-bucket/*\" ] } ] } The exact name of the role will be unique and generated by CloudFormation, however it will contain the words \"CromwellServer\" and it will be the role attached to the EC2 running the Cromwell server. S3 File Not Found (404) Possible Cause(s) A file required by the workflow cannot be found at the specified S3 Path. Your workflow inputs might have the incorrect path OR an expected file was not created by the previous step. Suggested Solution(s) Check the paths of inputs and that the expected file exists at that path. If the file name is something like <previousTaskName>-rc.txt the previous task failed before it was able to write out the result code. Inspect the stderr.txt and stdout.txt of the previous step for possible reasons. Cromwell Server OutOfMemory errors Possible Cause(s) Out of memory errors on the Cromwell Server are typically the result of the JVM running out of memory while attempting to keep track of multiple workflows or workflows with very large scatter steps. Suggested Solutions Consider upgrading the server instance type to one with more RAM. Investigate tuning Cromwell's job-control limits to find a configuration that appropriately restricts the number of queued Akka messages. Consider increasing the maximum instance RAM available to the JVM. Our Cloudformation templates this set to 85% ( -XX:MaxRAMPercentage=85.0 ) allowing some head room for the OS. On larger instance types you may be able to increase this further. Ensure you are not using an in memory database on the server instance. Our cloudformation templates configure a separate Aurora MySQL cluster to avoid this. Cromwell Task (Container) OutOfMemory errors Possible Cause(s) Individual tasks from a workflow run in docker containers on AWS Batch. If those containers have insufficient RAM for the task they can fail. Some older applications (including older versions of the JVM) do not always respect the memory limits imposed by the container and may think they have resources they cannot use. Suggested solutions Assign more memory to the task in the runtime: {} stanza of the WDL or if the task application allows use command line or configuration parameters to appropriately limit memory. For tasks executed by the JVM investigate -Xmx and -XX:MaxRAMPercentage parameters. Cromwell submitted AWS Batch jobs hang in 'Runnable' state Possible Causes The resources requested by the task exceed the largest size of instance available in your AWS Batch Compute Environment Batch worker EC2 instances are not able to join the Compute Environments ECS cluster Suggested solutions Reduce the resources required by your task to less than the maximum CPU and memory of the largest instance type allowed in your Batch Compute Environment. In your EC2 console determine if any gwf-core workers have started. If they have then ensure they have a route to the internet (for example, does your subnet have a NAT gateway). Worker nodes require access to the internet so that required dependencies can be downloaded by the worker nodes at startup time. If this process fails then Docker will not start, the ecs-agent will not run, and the systems manager will also not run. In addition, the node will also not be able to communicate with the AWS Batch service.","title":"Trouble Shooting"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#cromwell-troubleshooting","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some common errors that we have seen and suggested solutions","title":"Cromwell Troubleshooting"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#s3-access-denied-403","text":"","title":"S3 Access Denied (403)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#possible-causes","text":"A 403 error from S3 indicates that Cromwell is trying to access an S3 object that it doesn't have permission to. Following the priciple of \"least access\" Cromwell uses an IAM EC2 instance role that grants it read and write access to the S3 bucket you specified in the CloudFormation deployment and read only access to the gatk-test-data and broad-references S3 buckets. If your workflow references other S3 objects ( even ones in your account ) you will need to allow this via changes to the IAM role. Similarly if a step in your workflow attempts to write to another bucket you will need to add the appropriate permissions.","title":"Possible Cause(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#suggested-solutions","text":"Add read access to additional buckets by attaching a policy to the Cromwell server's IAM EC2 instance role with content similar to: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::bucket-a\", \"arn:aws:s3:::bucket-a/*\", \"arn:aws:s3:::another-bucket\", \"arn:aws:s3:::another-bucket/*\" ] } ] } The exact name of the role will be unique and generated by CloudFormation, however it will contain the words \"CromwellServer\" and it will be the role attached to the EC2 running the Cromwell server.","title":"Suggested Solution(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#s3-file-not-found-404","text":"","title":"S3 File Not Found (404)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#possible-causes_1","text":"A file required by the workflow cannot be found at the specified S3 Path. Your workflow inputs might have the incorrect path OR an expected file was not created by the previous step.","title":"Possible Cause(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#suggested-solutions_1","text":"Check the paths of inputs and that the expected file exists at that path. If the file name is something like <previousTaskName>-rc.txt the previous task failed before it was able to write out the result code. Inspect the stderr.txt and stdout.txt of the previous step for possible reasons.","title":"Suggested Solution(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#cromwell-server-outofmemory-errors","text":"","title":"Cromwell Server OutOfMemory errors"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#possible-causes_2","text":"Out of memory errors on the Cromwell Server are typically the result of the JVM running out of memory while attempting to keep track of multiple workflows or workflows with very large scatter steps.","title":"Possible Cause(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#suggested-solutions_2","text":"Consider upgrading the server instance type to one with more RAM. Investigate tuning Cromwell's job-control limits to find a configuration that appropriately restricts the number of queued Akka messages. Consider increasing the maximum instance RAM available to the JVM. Our Cloudformation templates this set to 85% ( -XX:MaxRAMPercentage=85.0 ) allowing some head room for the OS. On larger instance types you may be able to increase this further. Ensure you are not using an in memory database on the server instance. Our cloudformation templates configure a separate Aurora MySQL cluster to avoid this.","title":"Suggested Solutions"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#cromwell-task-container-outofmemory-errors","text":"","title":"Cromwell Task (Container) OutOfMemory errors"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#possible-causes_3","text":"Individual tasks from a workflow run in docker containers on AWS Batch. If those containers have insufficient RAM for the task they can fail. Some older applications (including older versions of the JVM) do not always respect the memory limits imposed by the container and may think they have resources they cannot use.","title":"Possible Cause(s)"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#suggested-solutions_3","text":"Assign more memory to the task in the runtime: {} stanza of the WDL or if the task application allows use command line or configuration parameters to appropriately limit memory. For tasks executed by the JVM investigate -Xmx and -XX:MaxRAMPercentage parameters.","title":"Suggested solutions"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#cromwell-submitted-aws-batch-jobs-hang-in-runnable-state","text":"","title":"Cromwell submitted AWS Batch jobs hang in 'Runnable' state"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#possible-causes_4","text":"The resources requested by the task exceed the largest size of instance available in your AWS Batch Compute Environment Batch worker EC2 instances are not able to join the Compute Environments ECS cluster","title":"Possible Causes"},{"location":"orchestration/cromwell/cromwell-trouble-shooting.html#suggested-solutions_4","text":"Reduce the resources required by your task to less than the maximum CPU and memory of the largest instance type allowed in your Batch Compute Environment. In your EC2 console determine if any gwf-core workers have started. If they have then ensure they have a route to the internet (for example, does your subnet have a NAT gateway). Worker nodes require access to the internet so that required dependencies can be downloaded by the worker nodes at startup time. If this process fails then Docker will not start, the ecs-agent will not run, and the systems manager will also not run. In addition, the node will also not be able to communicate with the AWS Batch service.","title":"Suggested solutions"},{"location":"orchestration/nextflow/nextflow-overview.html","text":"Nextflow on AWS Batch DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Nextflow is a reactive workflow framework and domain specific language (DSL) developed by the Comparative Bioinformatics group at the Barcelona Centre for Genomic Regulation (CRG) that enables scalable and reproducible scientific workflows using software containers. Nextflow can be run either locally or on a dedicated EC2 instance. The latter is preferred if you have long running workflows - with the caveat that you are responsible for stopping the instance when your workflow is complete. The architecture presented in this guide demonstrates how you can run Nextflow using AWS Batch in a managed and cost effective fashion. Requirements To get started using Nextflow on AWS you'll need the following setup in your AWS account: A VPC with at least 2 subnets (preferrably ones that are private ) The Genomics Workflow Core Environment A containerized nextflow executable with a custom entrypoint script that uses AWS Batch supplied environment variables for configuration information A Batch Job Definition that runs a Nextflow head node An IAM Role for the Nextflow head node job that allows it to submit AWS Batch jobs (optional) An S3 Bucket to store your Nextflow session cache The following will help you deploy these components VPC If you are handling sensitive data in your Nextflow pipelines, we recommend using at least 2 private subnets for AWS Batch compute jobs. EC2 instances launched into private subnets do not have public IP addresses, and therefore cannot be directly accessed from the public internet. They can still retain internet access from within the VPC - e.g. to pull source code, retrive public datasets, or install required softwre - if networking is configured appropriately. If the target VPC you want to deploy Nextflow into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow Genomics Workflow Core To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture. Nextflow Resources The the following CloudFormation template will create a nextflow container image, Batch Job Definition, and an IAM Role for a Nextflow head node job: Name Description Source Launch Stack Nextflow Resources Create Nextflow specific resources needed to run on AWS: an S3 Bucket for nextflow workflow scripts, Nextflow container, AWS Batch Job Definition for a Nextflow head node, and an IAM role for the nextflow head node job cloud_download play_arrow Deployment Details Nextflow container For AWS Batch to run Nextflow as a Batch Job, it needs to be containerized. The container image for nextflow built by the Nextflow Resources CloudFormation template includes capabilities to automatically configure Nextflow and run workflow scripts in S3. If you want to add specialized capabilities or require a particular version of Nextflow, you can modify the source code to best suit your needs. To create such a container, you can use a Dockerfile like the one below: # use the upstream nextflow container as a base image ARG VERSION=latest FROM nextflow/nextflow:${VERSION} AS build FROM amazonlinux:2 AS final COPY --from=build /usr/local/bin/nextflow /usr/bin/nextflow RUN yum update -y \\ && yum install -y \\ curl \\ hostname \\ java \\ unzip \\ && yum clean -y all RUN rm -rf /var/cache/yum # install awscli v2 RUN curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" \\ && unzip -q /tmp/awscliv2.zip -d /tmp \\ && /tmp/aws/install -b /usr/bin \\ && rm -rf /tmp/aws* ENV JAVA_HOME /usr/lib/jvm/jre-openjdk/ # invoke nextflow once to download dependencies RUN nextflow -version # install a custom entrypoint script that handles being run within an AWS Batch Job COPY nextflow.aws.sh /opt/bin/nextflow.aws.sh RUN chmod +x /opt/bin/nextflow.aws.sh WORKDIR /opt/work ENTRYPOINT [\"/opt/bin/nextflow.aws.sh\"] Note If you are trying to keep your container image as small as possible, keep in mind that Nextflow relies on basic linux tools such as awk , bash , ps , date , sed , grep , egrep , and tail which may need to be installed on extra minimalist base images like alpine . An example entrypoint script for the container is shown below and demonstrates how to wrap a nextflow run ... command so that it can be run as an AWS Batch Job. The script takes command line arguments that are passed in by AWS Batch during job submission. The first parameter should be a Nextflow \"project\", and any additional parameters are passed along to the Nextflow executable. Nextflow supports pulling projects directly from Git repositories. You can extend this behavior to allow projects to be specified as an S3 URI - a bucket and folder therein where you have staged your Nextflow scripts and supporting files (like additional config files). This is useful if you publish tested pipelines (e.g. via a continuous integration workflow) as static artifacts in S3. For details on how to do this see the full source code for the script . The script automatically configures Nextflow to use AWS Batch using environment variables defined in the Nextflow Job Definition. These include: NF_WORKDIR - an S3 URI used for to store intermediate and final workflow outputs NF_LOGSDIR - an S3 URI used to store workflow session cache and logs NF_JOB_QUEUE - the AWS Batch Job Queue that workflow processes will be submitted to Importantly, the script also handles restoration/preservation of the workflow session cache to enable using previously computed results via the -resume flag. #!/bin/bash NEXTFLOW_PROJECT=$1 # first argument is a project shift NEXTFLOW_PARAMS=\"$@\" # additional arguments are nextflow options (e.g. -resume) or workflow parameters # Create the default config using environment variables # passed into the container by AWS Batch NF_CONFIG=~/.nextflow/config cat << EOF > $NF_CONFIG workDir = \"$NF_WORKDIR\" process.executor = \"awsbatch\" process.queue = \"$NF_JOB_QUEUE\" aws.batch.cliPath = \"/home/ec2-user/miniconda/bin/aws\" EOF echo \"=== CONFIGURATION ===\" cat ~/.nextflow/config # stage in session cache # .nextflow directory holds all session information for the current and past runs. # it should be `sync`'d with an s3 uri, so that runs from previous sessions can be # resumed echo \"== Restoring Session Cache ==\" aws s3 sync --only-show-errors $NF_LOGSDIR/.nextflow .nextflow function preserve_session() { # stage out session cache if [ -d .nextflow ]; then echo \"== Preserving Session Cache ==\" aws s3 sync --only-show-errors .nextflow $NF_LOGSDIR/.nextflow fi # .nextflow.log file has more detailed logging from the workflow session and is # nominally unique per session. # # when run locally, .nextflow.logs are automatically rotated # when syncing to S3 uniquely identify logs by the AWS Batch Job ID if [ -f .nextflow.log ]; then echo \"== Preserving Session Log ==\" aws s3 cp --only-show-errors .nextflow.log $NF_LOGSDIR/.nextflow.log.$AWS_BATCH_JOB_ID fi } # set a trap so that session information is preserved when the container exits trap preserve_session EXIT echo \"== Running Workflow ==\" echo \"nextflow run $NEXTFLOW_PROJECT $NEXTFLOW_PARAMS\" nextflow run $NEXTFLOW_PROJECT $NEXTFLOW_PARAMS Note AWS_BATCH_JOB_ID is one of several environment variables that are automatically provided to all AWS Batch jobs. The NF_WORKDIR , NF_LOGSDIR , and NF_JOB_QUEUE variables are ones set by the Batch Job Definition ( see below ). Job instance AWS CLI Nextflow uses the AWS CLI to stage input and output data for tasks. The AWS CLI can either be installed in the task container or on the host instance that task containers run on. Adding the AWS CLI to an existing containerized tool requires rebuilding the image to include it. Assuming your tool's container image is based on CentOS, this would require a Dockerfile like so: FROM myoriginalcontainer RUN yum install -y awscli ENTRYPOINT [\"mytool\"] If you have many tools in your pipeline, rebuilding all of their images and keeping them up to date may not be ideal. Using a version installed on the host instance means you can use pre-existing containers. To leveraage this, the AWS CLI must be self contained and not rely on system shared libraries. The AWS CLI v2 is packaged this way. The Launch Template deployed by the Core Environment handles installing the AWS CLI v2 on instances launched by AWS Batch. Batch job definition An AWS Batch Job Definition to run a containerized Nextflow is shown below. { \"jobDefinitionName\": \"nextflow\", \"jobDefinitionArn\": \"arn:aws:batch:<region>:<account-number>:job-definition/nextflow:1\", \"type\": \"container\", \"parameters\": {}, \"containerProperties\": { \"image\": \"<account-number>.dkr.ecr.<region>.amazonaws.com/nextflow:latest\", \"vcpus\": 2, \"memory\": 1024, \"command\": [], \"jobRoleArn\": \"<nextflowJobRoleArn>\", \"volumes\": [], \"environment\": [ { \"name\": \"NF_LOGSDIR\", \"value\": \"s3://<bucket>/_nextflow/logs\" }, { \"name\": \"NF_JOB_QUEUE\", \"value\": \"<jobQueueArn>\" }, { \"name\": \"NF_WORKDIR\", \"value\": \"s3://<bucket>/_nextflow/runs\" } ], \"mountPoints\": [], \"ulimits\": [], \"resourceRequirements\": [] } } The <nextflowJobRoleArn> is a reference to the IAM Role required by Nextflow head jobs and is described below. Nextflow IAM Role Nextflow needs to be able to create Batch Job Defintions, submit and cancel Batch Jobs, and read workflow logs and session information from S3. These permissions are provided via a Job Role associated with the Job Definition. Policies for this role would look like the following: Nextflow-Batch-Access This policy gives full access to AWS Batch. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"batch:*\" ], \"Resource\": \"*\", \"Effect\": \"Allow\" } ] } Nextflow-S3Bucket-Access This policy gives full access to the buckets used to store workflow data and Nextflow session metadata. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<nextflow-bucket-name>\", \"arn:aws:s3:::<nextflow-bucket-name>/*\", \"arn:aws:s3:::<data-bucket-name>\", \"arn:aws:s3:::<data-bucket-name>/*\" ], \"Effect\": \"Allow\" } ] } Nextflow specific S3 Bucket Because running as a container will be an ephemeral process, the containerized version of nextflow stores workflow session information in S3 using paths described by NF_WORKDIR and NF_LOGSDIR environment variables. These allow you to use Nextflow's -resume flag to restart a workflow that was previously interrupted at the step it left off at. This bucket can be independent of the S3 bucket used to store workflow input and output data if necessary. Workflow process definitions The process definitions in Nextflow scripts should include a couple key parts for running on AWS Batch: the container directive cpus and memory directives to define resource that will be used by Batch Jobs !!! Note: The container image used to run a process needs to be capable of running the AWS CLI. It doesn't have to contain the CLI but it does need shared libraries from glib.c such as libz.so.1 which may not be present in very minimal images. An example definition for a simple \"Hello World\" process is shown below: texts = Channel.from(\"AWS\", \"Nextflow\") process hello { // directives // a container images is required container \"ubuntu:latest\" // compute resources for the Batch Job cpus 1 memory '512 MB' input: val text from texts output: file 'hello.txt' \"\"\" echo \"Hello $text\" > hello.txt \"\"\" } For each process in your workflow, Nextflow will create a corresponding Batch Job Definition, which it will re-use for subsequent workflow runs. The process defined above will create a Batch Job Definition called nf-ubuntu-latest that looks like: { \"jobDefinitionName\": \"nf-ubuntu-latest\", \"jobDefinitionArn\": \"arn:aws:batch:<region>:<account-number>:job-definition/nf-ubuntu-latest:1\", \"revision\": 1, \"status\": \"ACTIVE\", \"type\": \"container\", \"parameters\": { \"nf-token\": \"43869867b5fbae16fa7cfeb5ea2c3522\" }, \"containerProperties\": { \"image\": \"ubuntu:latest\", \"vcpus\": 1, \"memory\": 1024, \"command\": [ \"true\" ], \"volumes\": [ { \"host\": { \"sourcePath\": \"/opt/aws-cli\" }, \"name\": \"aws-cli\" } ], \"environment\": [], \"mountPoints\": [ { \"containerPath\": \"/opt/aws-cli\", \"readOnly\": true, \"sourceVolume\": \"aws-cli\" } ], \"ulimits\": [] } } You can also use the aws.batch.volumes config option to define additional volumes and mount points so that your processes can have access to directories on the host instance. Running workflows To run a workflow you submit a nextflow Batch job to the appropriate Batch Job Queue via: the AWS Batch Console the command line with the AWS CLI This is what starting a workflow via the AWS CLI would look like using a modified fork of Nextflow's built-in \"hello-world\" workflow: aws batch submit-job \\ --job-name nf-hello \\ --job-queue <queue-name> \\ --job-definition nextflow-<nextflow-stack-namespace> \\ --container-overrides command=wleepang/hello !!! Note: The reason for the modification was purely to make use of a container that could provide libraries (such as libz.so.1 ) needed to run the AWS CLI which is responsible for marshalling files to and from S3 After submitting a workflow, you can monitor the progress of tasks via the AWS Batch console. For the \"Hello World\" workflow above you will see five jobs run in Batch - one for the head node, and one for each Channel text as it goes through the hello process. For a more complex example, you can try the following, which will run a demo RNASeq workflow against data in the 1000 Genomes AWS Public Dataset : aws batch submit-job \\ --job-name nf-core-rnaseq \\ --job-queue <queue-name> \\ --job-definition nextflow-<nextflow-stack-namespace> \\ --container-overrides command=rnaseq-nf,\\ \"--reads\",\"'s3://1000genomes/phase3/data/HG00243/sequence_read/SRR*_{1,2}.filt.fastq.gz'\",\\ \"--genome\",\"GRCh37\",\\ \"--skip_qc\" For the example \"rnaseq\" workflow you will see 5 jobs run in Batch over the course of a couple hours - the head node will last the whole duration of the pipeline while the others will stop once their step is complete. You can look at the CloudWatch logs for the head node job to monitor workflow progress. Note the additional single quotes wrapping the 1000genomes path. In both of the examples above, submitting workflows is an asynchronous task allowing you to quickly move on to other tasks. Importantly, AWS Batch handled scaling up all the compute needed to run workflow jobs, and when the workflow was complete, AWS Batch scaled all compute resources back down. Cost optimizing workflows Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Overview"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-on-aws-batch","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. Nextflow is a reactive workflow framework and domain specific language (DSL) developed by the Comparative Bioinformatics group at the Barcelona Centre for Genomic Regulation (CRG) that enables scalable and reproducible scientific workflows using software containers. Nextflow can be run either locally or on a dedicated EC2 instance. The latter is preferred if you have long running workflows - with the caveat that you are responsible for stopping the instance when your workflow is complete. The architecture presented in this guide demonstrates how you can run Nextflow using AWS Batch in a managed and cost effective fashion.","title":"Nextflow on AWS Batch"},{"location":"orchestration/nextflow/nextflow-overview.html#requirements","text":"To get started using Nextflow on AWS you'll need the following setup in your AWS account: A VPC with at least 2 subnets (preferrably ones that are private ) The Genomics Workflow Core Environment A containerized nextflow executable with a custom entrypoint script that uses AWS Batch supplied environment variables for configuration information A Batch Job Definition that runs a Nextflow head node An IAM Role for the Nextflow head node job that allows it to submit AWS Batch jobs (optional) An S3 Bucket to store your Nextflow session cache The following will help you deploy these components","title":"Requirements"},{"location":"orchestration/nextflow/nextflow-overview.html#vpc","text":"If you are handling sensitive data in your Nextflow pipelines, we recommend using at least 2 private subnets for AWS Batch compute jobs. EC2 instances launched into private subnets do not have public IP addresses, and therefore cannot be directly accessed from the public internet. They can still retain internet access from within the VPC - e.g. to pull source code, retrive public datasets, or install required softwre - if networking is configured appropriately. If the target VPC you want to deploy Nextflow into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow","title":"VPC"},{"location":"orchestration/nextflow/nextflow-overview.html#genomics-workflow-core","text":"To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture.","title":"Genomics Workflow Core"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-resources","text":"The the following CloudFormation template will create a nextflow container image, Batch Job Definition, and an IAM Role for a Nextflow head node job: Name Description Source Launch Stack Nextflow Resources Create Nextflow specific resources needed to run on AWS: an S3 Bucket for nextflow workflow scripts, Nextflow container, AWS Batch Job Definition for a Nextflow head node, and an IAM role for the nextflow head node job cloud_download play_arrow","title":"Nextflow Resources"},{"location":"orchestration/nextflow/nextflow-overview.html#deployment-details","text":"","title":"Deployment Details"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-container","text":"For AWS Batch to run Nextflow as a Batch Job, it needs to be containerized. The container image for nextflow built by the Nextflow Resources CloudFormation template includes capabilities to automatically configure Nextflow and run workflow scripts in S3. If you want to add specialized capabilities or require a particular version of Nextflow, you can modify the source code to best suit your needs. To create such a container, you can use a Dockerfile like the one below: # use the upstream nextflow container as a base image ARG VERSION=latest FROM nextflow/nextflow:${VERSION} AS build FROM amazonlinux:2 AS final COPY --from=build /usr/local/bin/nextflow /usr/bin/nextflow RUN yum update -y \\ && yum install -y \\ curl \\ hostname \\ java \\ unzip \\ && yum clean -y all RUN rm -rf /var/cache/yum # install awscli v2 RUN curl -s \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"/tmp/awscliv2.zip\" \\ && unzip -q /tmp/awscliv2.zip -d /tmp \\ && /tmp/aws/install -b /usr/bin \\ && rm -rf /tmp/aws* ENV JAVA_HOME /usr/lib/jvm/jre-openjdk/ # invoke nextflow once to download dependencies RUN nextflow -version # install a custom entrypoint script that handles being run within an AWS Batch Job COPY nextflow.aws.sh /opt/bin/nextflow.aws.sh RUN chmod +x /opt/bin/nextflow.aws.sh WORKDIR /opt/work ENTRYPOINT [\"/opt/bin/nextflow.aws.sh\"] Note If you are trying to keep your container image as small as possible, keep in mind that Nextflow relies on basic linux tools such as awk , bash , ps , date , sed , grep , egrep , and tail which may need to be installed on extra minimalist base images like alpine . An example entrypoint script for the container is shown below and demonstrates how to wrap a nextflow run ... command so that it can be run as an AWS Batch Job. The script takes command line arguments that are passed in by AWS Batch during job submission. The first parameter should be a Nextflow \"project\", and any additional parameters are passed along to the Nextflow executable. Nextflow supports pulling projects directly from Git repositories. You can extend this behavior to allow projects to be specified as an S3 URI - a bucket and folder therein where you have staged your Nextflow scripts and supporting files (like additional config files). This is useful if you publish tested pipelines (e.g. via a continuous integration workflow) as static artifacts in S3. For details on how to do this see the full source code for the script . The script automatically configures Nextflow to use AWS Batch using environment variables defined in the Nextflow Job Definition. These include: NF_WORKDIR - an S3 URI used for to store intermediate and final workflow outputs NF_LOGSDIR - an S3 URI used to store workflow session cache and logs NF_JOB_QUEUE - the AWS Batch Job Queue that workflow processes will be submitted to Importantly, the script also handles restoration/preservation of the workflow session cache to enable using previously computed results via the -resume flag. #!/bin/bash NEXTFLOW_PROJECT=$1 # first argument is a project shift NEXTFLOW_PARAMS=\"$@\" # additional arguments are nextflow options (e.g. -resume) or workflow parameters # Create the default config using environment variables # passed into the container by AWS Batch NF_CONFIG=~/.nextflow/config cat << EOF > $NF_CONFIG workDir = \"$NF_WORKDIR\" process.executor = \"awsbatch\" process.queue = \"$NF_JOB_QUEUE\" aws.batch.cliPath = \"/home/ec2-user/miniconda/bin/aws\" EOF echo \"=== CONFIGURATION ===\" cat ~/.nextflow/config # stage in session cache # .nextflow directory holds all session information for the current and past runs. # it should be `sync`'d with an s3 uri, so that runs from previous sessions can be # resumed echo \"== Restoring Session Cache ==\" aws s3 sync --only-show-errors $NF_LOGSDIR/.nextflow .nextflow function preserve_session() { # stage out session cache if [ -d .nextflow ]; then echo \"== Preserving Session Cache ==\" aws s3 sync --only-show-errors .nextflow $NF_LOGSDIR/.nextflow fi # .nextflow.log file has more detailed logging from the workflow session and is # nominally unique per session. # # when run locally, .nextflow.logs are automatically rotated # when syncing to S3 uniquely identify logs by the AWS Batch Job ID if [ -f .nextflow.log ]; then echo \"== Preserving Session Log ==\" aws s3 cp --only-show-errors .nextflow.log $NF_LOGSDIR/.nextflow.log.$AWS_BATCH_JOB_ID fi } # set a trap so that session information is preserved when the container exits trap preserve_session EXIT echo \"== Running Workflow ==\" echo \"nextflow run $NEXTFLOW_PROJECT $NEXTFLOW_PARAMS\" nextflow run $NEXTFLOW_PROJECT $NEXTFLOW_PARAMS Note AWS_BATCH_JOB_ID is one of several environment variables that are automatically provided to all AWS Batch jobs. The NF_WORKDIR , NF_LOGSDIR , and NF_JOB_QUEUE variables are ones set by the Batch Job Definition ( see below ).","title":"Nextflow container"},{"location":"orchestration/nextflow/nextflow-overview.html#job-instance-aws-cli","text":"Nextflow uses the AWS CLI to stage input and output data for tasks. The AWS CLI can either be installed in the task container or on the host instance that task containers run on. Adding the AWS CLI to an existing containerized tool requires rebuilding the image to include it. Assuming your tool's container image is based on CentOS, this would require a Dockerfile like so: FROM myoriginalcontainer RUN yum install -y awscli ENTRYPOINT [\"mytool\"] If you have many tools in your pipeline, rebuilding all of their images and keeping them up to date may not be ideal. Using a version installed on the host instance means you can use pre-existing containers. To leveraage this, the AWS CLI must be self contained and not rely on system shared libraries. The AWS CLI v2 is packaged this way. The Launch Template deployed by the Core Environment handles installing the AWS CLI v2 on instances launched by AWS Batch.","title":"Job instance AWS CLI"},{"location":"orchestration/nextflow/nextflow-overview.html#batch-job-definition","text":"An AWS Batch Job Definition to run a containerized Nextflow is shown below. { \"jobDefinitionName\": \"nextflow\", \"jobDefinitionArn\": \"arn:aws:batch:<region>:<account-number>:job-definition/nextflow:1\", \"type\": \"container\", \"parameters\": {}, \"containerProperties\": { \"image\": \"<account-number>.dkr.ecr.<region>.amazonaws.com/nextflow:latest\", \"vcpus\": 2, \"memory\": 1024, \"command\": [], \"jobRoleArn\": \"<nextflowJobRoleArn>\", \"volumes\": [], \"environment\": [ { \"name\": \"NF_LOGSDIR\", \"value\": \"s3://<bucket>/_nextflow/logs\" }, { \"name\": \"NF_JOB_QUEUE\", \"value\": \"<jobQueueArn>\" }, { \"name\": \"NF_WORKDIR\", \"value\": \"s3://<bucket>/_nextflow/runs\" } ], \"mountPoints\": [], \"ulimits\": [], \"resourceRequirements\": [] } } The <nextflowJobRoleArn> is a reference to the IAM Role required by Nextflow head jobs and is described below.","title":"Batch job definition"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-iam-role","text":"Nextflow needs to be able to create Batch Job Defintions, submit and cancel Batch Jobs, and read workflow logs and session information from S3. These permissions are provided via a Job Role associated with the Job Definition. Policies for this role would look like the following:","title":"Nextflow IAM Role"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-batch-access","text":"This policy gives full access to AWS Batch. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"batch:*\" ], \"Resource\": \"*\", \"Effect\": \"Allow\" } ] }","title":"Nextflow-Batch-Access"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-s3bucket-access","text":"This policy gives full access to the buckets used to store workflow data and Nextflow session metadata. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::<nextflow-bucket-name>\", \"arn:aws:s3:::<nextflow-bucket-name>/*\", \"arn:aws:s3:::<data-bucket-name>\", \"arn:aws:s3:::<data-bucket-name>/*\" ], \"Effect\": \"Allow\" } ] }","title":"Nextflow-S3Bucket-Access"},{"location":"orchestration/nextflow/nextflow-overview.html#nextflow-specific-s3-bucket","text":"Because running as a container will be an ephemeral process, the containerized version of nextflow stores workflow session information in S3 using paths described by NF_WORKDIR and NF_LOGSDIR environment variables. These allow you to use Nextflow's -resume flag to restart a workflow that was previously interrupted at the step it left off at. This bucket can be independent of the S3 bucket used to store workflow input and output data if necessary.","title":"Nextflow specific S3 Bucket"},{"location":"orchestration/nextflow/nextflow-overview.html#workflow-process-definitions","text":"The process definitions in Nextflow scripts should include a couple key parts for running on AWS Batch: the container directive cpus and memory directives to define resource that will be used by Batch Jobs !!! Note: The container image used to run a process needs to be capable of running the AWS CLI. It doesn't have to contain the CLI but it does need shared libraries from glib.c such as libz.so.1 which may not be present in very minimal images. An example definition for a simple \"Hello World\" process is shown below: texts = Channel.from(\"AWS\", \"Nextflow\") process hello { // directives // a container images is required container \"ubuntu:latest\" // compute resources for the Batch Job cpus 1 memory '512 MB' input: val text from texts output: file 'hello.txt' \"\"\" echo \"Hello $text\" > hello.txt \"\"\" } For each process in your workflow, Nextflow will create a corresponding Batch Job Definition, which it will re-use for subsequent workflow runs. The process defined above will create a Batch Job Definition called nf-ubuntu-latest that looks like: { \"jobDefinitionName\": \"nf-ubuntu-latest\", \"jobDefinitionArn\": \"arn:aws:batch:<region>:<account-number>:job-definition/nf-ubuntu-latest:1\", \"revision\": 1, \"status\": \"ACTIVE\", \"type\": \"container\", \"parameters\": { \"nf-token\": \"43869867b5fbae16fa7cfeb5ea2c3522\" }, \"containerProperties\": { \"image\": \"ubuntu:latest\", \"vcpus\": 1, \"memory\": 1024, \"command\": [ \"true\" ], \"volumes\": [ { \"host\": { \"sourcePath\": \"/opt/aws-cli\" }, \"name\": \"aws-cli\" } ], \"environment\": [], \"mountPoints\": [ { \"containerPath\": \"/opt/aws-cli\", \"readOnly\": true, \"sourceVolume\": \"aws-cli\" } ], \"ulimits\": [] } } You can also use the aws.batch.volumes config option to define additional volumes and mount points so that your processes can have access to directories on the host instance.","title":"Workflow process definitions"},{"location":"orchestration/nextflow/nextflow-overview.html#running-workflows","text":"To run a workflow you submit a nextflow Batch job to the appropriate Batch Job Queue via: the AWS Batch Console the command line with the AWS CLI This is what starting a workflow via the AWS CLI would look like using a modified fork of Nextflow's built-in \"hello-world\" workflow: aws batch submit-job \\ --job-name nf-hello \\ --job-queue <queue-name> \\ --job-definition nextflow-<nextflow-stack-namespace> \\ --container-overrides command=wleepang/hello !!! Note: The reason for the modification was purely to make use of a container that could provide libraries (such as libz.so.1 ) needed to run the AWS CLI which is responsible for marshalling files to and from S3 After submitting a workflow, you can monitor the progress of tasks via the AWS Batch console. For the \"Hello World\" workflow above you will see five jobs run in Batch - one for the head node, and one for each Channel text as it goes through the hello process. For a more complex example, you can try the following, which will run a demo RNASeq workflow against data in the 1000 Genomes AWS Public Dataset : aws batch submit-job \\ --job-name nf-core-rnaseq \\ --job-queue <queue-name> \\ --job-definition nextflow-<nextflow-stack-namespace> \\ --container-overrides command=rnaseq-nf,\\ \"--reads\",\"'s3://1000genomes/phase3/data/HG00243/sequence_read/SRR*_{1,2}.filt.fastq.gz'\",\\ \"--genome\",\"GRCh37\",\\ \"--skip_qc\" For the example \"rnaseq\" workflow you will see 5 jobs run in Batch over the course of a couple hours - the head node will last the whole duration of the pipeline while the others will stop once their step is complete. You can look at the CloudWatch logs for the head node job to monitor workflow progress. Note the additional single quotes wrapping the 1000genomes path. In both of the examples above, submitting workflows is an asynchronous task allowing you to quickly move on to other tasks. Importantly, AWS Batch handled scaling up all the compute needed to run workflow jobs, and when the workflow was complete, AWS Batch scaled all compute resources back down.","title":"Running workflows"},{"location":"orchestration/nextflow/nextflow-overview.html#cost-optimizing-workflows","text":"Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Cost optimizing workflows"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html","text":"Nextflow Troubleshooting DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some common errors that we have seen and suggested solutions Job Logs say there is an error in AWS CLI while loading shared libraries Possible Cause(s) Nextflow on AWS Batch relies on the process containers being able to use the AWS CLI (which is mounted from the container host). Very minimal container images such as Alpine do not contain the glibc libraries needed by the AWS CLI. Suggested Solution(s) Modify your image to include or mount these dependencies Use an image (or build from a base) that already contains these such as ubuntu:latest AWS credentials not working when set in the environment Possible Cause(s) You are using a local run of nextflow with temporary federated or IAM role credentials that use the AWS_SESSION_TOKEN in addition to AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Nextflow does not look for the AWS_SESSION_TOKEN environment variable as detailed at nextflow/issues/2839 Suggested Solution(s) Instead of using local credentials consider using an IAM role associated to an EC2 instance or ECS container where the Nextflow binary runs from. This will not require setting any local credentials and remove the need to update a session token. If you are using just using Nextflow locally for testing purposes, you can set credentials in a local nextflow.config which does support the AWS_SESSION_TOKEN aws { accessKey = 'XXXXXXXXXXXXXXXX' secretKey = 'XXXXXXXXXXXXXXXX' sessionToken = 'XXXXXXXXXXXXXXX' } N.B. If you set the sessionToken in the Nextflow config it will expire and will need to be updated. This expiry time will depend on the configuration of credentials generation within your account. Container start errors CannotStartContainerError: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/env-execute\": stat /usr/local/env-execute: no such file or directory: un Possible Cause(s) Nextflow on AWS Batch relies on the process containers being able to use a number of scripts that are mounted to the container. If references to these are wrong or do not exist then the tasks will not start. Suggested Solution(s) If using the provided image setup with no changes, check the path specified for the aws-cli in your nextflow.config is set to aws.batch.cliPath = '/opt/aws-cli/bin/aws' Check the target S3 bucket created in the set-up has the following path: bucket-name/<namespace>-ecs-additions/SourceStag/ and that content is present. This location should contain a zip file that has the following in it: . \u251c\u2500\u2500 awscli-shim.sh \u251c\u2500\u2500 ecs-additions-common.sh \u251c\u2500\u2500 ecs-additions-cromwell.sh \u251c\u2500\u2500 ecs-additions-nextflow.sh \u251c\u2500\u2500 ecs-additions-step-functions.sh \u251c\u2500\u2500 ecs-logs-collector.sh \u251c\u2500\u2500 fetch_and_run.sh \u251c\u2500\u2500 get-amazon-ebs-autoscale.sh \u2514\u2500\u2500 provision.sh If this is missing check that <namespace>-ecs-additions exists and ran successfully in AWS Codepipeline and rerun if failures are present.","title":"Trouble Shooting"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#nextflow-troubleshooting","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The following are some common errors that we have seen and suggested solutions","title":"Nextflow Troubleshooting"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#job-logs-say-there-is-an-error-in-aws-cli-while-loading-shared-libraries","text":"","title":"Job Logs say there is an error in AWS CLI while loading shared libraries"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#possible-causes","text":"Nextflow on AWS Batch relies on the process containers being able to use the AWS CLI (which is mounted from the container host). Very minimal container images such as Alpine do not contain the glibc libraries needed by the AWS CLI.","title":"Possible Cause(s)"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#suggested-solutions","text":"Modify your image to include or mount these dependencies Use an image (or build from a base) that already contains these such as ubuntu:latest","title":"Suggested Solution(s)"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#aws-credentials-not-working-when-set-in-the-environment","text":"","title":"AWS credentials not working when set in the environment"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#possible-causes_1","text":"You are using a local run of nextflow with temporary federated or IAM role credentials that use the AWS_SESSION_TOKEN in addition to AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Nextflow does not look for the AWS_SESSION_TOKEN environment variable as detailed at nextflow/issues/2839","title":"Possible Cause(s)"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#suggested-solutions_1","text":"Instead of using local credentials consider using an IAM role associated to an EC2 instance or ECS container where the Nextflow binary runs from. This will not require setting any local credentials and remove the need to update a session token. If you are using just using Nextflow locally for testing purposes, you can set credentials in a local nextflow.config which does support the AWS_SESSION_TOKEN aws { accessKey = 'XXXXXXXXXXXXXXXX' secretKey = 'XXXXXXXXXXXXXXXX' sessionToken = 'XXXXXXXXXXXXXXX' } N.B. If you set the sessionToken in the Nextflow config it will expire and will need to be updated. This expiry time will depend on the configuration of credentials generation within your account.","title":"Suggested Solution(s)"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#container-start-errors","text":"CannotStartContainerError: Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/env-execute\": stat /usr/local/env-execute: no such file or directory: un","title":"Container start errors"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#possible-causes_2","text":"Nextflow on AWS Batch relies on the process containers being able to use a number of scripts that are mounted to the container. If references to these are wrong or do not exist then the tasks will not start.","title":"Possible Cause(s)"},{"location":"orchestration/nextflow/nextflow-trouble-shooting.html#suggested-solutions_2","text":"If using the provided image setup with no changes, check the path specified for the aws-cli in your nextflow.config is set to aws.batch.cliPath = '/opt/aws-cli/bin/aws' Check the target S3 bucket created in the set-up has the following path: bucket-name/<namespace>-ecs-additions/SourceStag/ and that content is present. This location should contain a zip file that has the following in it: . \u251c\u2500\u2500 awscli-shim.sh \u251c\u2500\u2500 ecs-additions-common.sh \u251c\u2500\u2500 ecs-additions-cromwell.sh \u251c\u2500\u2500 ecs-additions-nextflow.sh \u251c\u2500\u2500 ecs-additions-step-functions.sh \u251c\u2500\u2500 ecs-logs-collector.sh \u251c\u2500\u2500 fetch_and_run.sh \u251c\u2500\u2500 get-amazon-ebs-autoscale.sh \u2514\u2500\u2500 provision.sh If this is missing check that <namespace>-ecs-additions exists and ran successfully in AWS Codepipeline and rerun if failures are present.","title":"Suggested Solution(s)"},{"location":"orchestration/step-functions/step-functions-examples.html","text":"Step Functions Workflow Examples THIS IS A STUB this was created from this file .","title":"Step Functions Workflow Examples"},{"location":"orchestration/step-functions/step-functions-examples.html#step-functions-workflow-examples","text":"THIS IS A STUB this was created from this file .","title":"Step Functions Workflow Examples"},{"location":"orchestration/step-functions/step-functions-overview.html","text":"AWS Step Functions For Genomics Workflows DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The AWS Step Functions service allows you to orchestrate other AWS services, such as Lambda, Batch, SNS, and Glue, making it easy to coordinate the components of distributed applications as a series of steps in a visual workflow. In the context of genomics workflows, the combination of AWS Step Functions with Batch and Lambda constitutes a robust, scalable, and serverless task orchestration solution. Requirements To get started using AWS Step Functions for genomics workflows you'll need the following setup in your AWS account: A VPC with at least 2 subnets (preferrably ones that are private ) The Genomics Workflow Core Environment Containerized tools for your workflow steps like BWA-MEM, Samtools, BCFtools custom entrypoint scripts that uses AWS Batch supplied environment variables for configuration and data handling A Batch Job Definitions for your tools An IAM Role for AWS Step Functions that allows it to submit AWS Batch jobs The following will help you deploy these components VPC If you are handling sensitive data in your genomics pipelines, we recommend using at least 2 private subnets for AWS Batch compute jobs. EC2 instances launched into private subnets do not have public IP addresses, and therefore cannot be directly accessed from the public internet. They can still retain internet access from within the VPC - e.g. to pull source code, retrive public datasets, or install required softwre - if networking is configured appropriately. If the target VPC you want to deploy into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow Genomics Workflow Core To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture. Step Functions Resources The the following CloudFormation template will create an AWS Step Functions State Machine that defines an example variant calling workflow using BWA-MEM, Samtools, and BCFtools; container images and AWS Batch Job Definitions for the tooling; and an IAM Role that allows AWS Step Functions to call AWS Batch during State Machine executions: Name Description Source Launch Stack AWS Step Functions Resources Create a Step Functions State Machine, Batch Job Definitions, and container images to run an example genomics workflow cloud_download play_arrow Deployment Details AWS Step Functions Execution Role An AWS Step Functions Execution role is an IAM role that allows Step Functions to execute other AWS services via the state machine. This can be created automatically during the \"first-run\" experience in the AWS Step Functions console when you create your first state machine. The policy attached to the role will depend on the specifc tasks you incorporate into your state machine. State machines that use AWS Batch for job execution and send events to CloudWatch should have an Execution role with the following inline policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"enable submitting batch jobs\", \"Effect\": \"Allow\", \"Action\": [ \"batch:SubmitJob\", \"batch:DescribeJobs\", \"batch:TerminateJob\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"events:PutTargets\", \"events:PutRule\", \"events:DescribeRule\" ], \"Resource\": [ \"arn:aws:events:<region>:<account-number>:rule/StepFunctionsGetEventsForBatchJobsRule\" ] } ] } For more complex workflows that use nested workflows or require more complex input parsing, you need to add additional permissions for executing Step Functions State Machines and invoking Lambda functions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"enable calling lambda functions\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\" ], \"Resource\": \"*\" }, { \"Sid\": \"enable calling other step functions\", \"Effect\": \"Allow\", \"Action\": [ \"states:StartExecution\" ], \"Resource\": \"*\" }, ... ] } Note All Resource values in the policy statements above can be scoped to be more specific if needed. Step Functions State Machines Workflows in AWS Step Functions are built using Amazon States Language (ASL), a declarative, JSON-based, structured language used to define a \"state-machine\". An AWS Step Functions State-Machine is a collection of states that can do work (Task states), determine which states to transition to next (Choice states), stop an execution with an error (Fail states), and so on. Building workflows with AWS Step Functions The overall structure of a state-machine looks like the following: { \"Comment\": \"Description of you state-machine\", \"StartAt\": \"FirstState\", \"States\": { \"FirstState\": { \"Type\": \"<state-type>\", \"Next\": \"<name of next state>\" }, \"State1\" : { ... }, ... \"StateN\" : { ... }, \"LastState\": { ... \"End\": true } } } A simple \"Hello World\" state-machine looks like this: { \"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\", \"StartAt\": \"HelloWorld\", \"States\": { \"HelloWorld\": { \"Type\": \"Pass\", \"Result\": \"Hello World!\", \"End\": true } } ASL supports several task types and simple structures that can be combined to form a wide variety of complex workflows. More detailed coverage of ASL state types and structures is provided in the Step Functions ASL documentation . Batch Job Definitions AWS Batch Job Definitions are used to define compute resource requirements and parameter defaults for an AWS Batch Job. These are then referenced in state machine Task states by their respective ARNs. An example Job Definition for the bwa-mem sequence aligner is shown below: { \"jobDefinitionName\": \"bwa-mem\", \"type\": \"container\", \"parameters\": { \"threads\": \"8\" }, \"containerProperties\": { \"image\": \"<dockerhub-user>/bwa-mem:latest\", \"vcpus\": 8, \"memory\": 32000, \"command\": [ \"bwa\", \"mem\", \"-t\", \"Ref::threads\", \"-p\", \"reference.fasta\", \"sample_1.fastq.gz\" ], \"volumes\": [ { \"host\": { \"sourcePath\": \"/scratch\" }, \"name\": \"scratch\" }, { \"host\": { \"sourcePath\": \"/opt/miniconda\" }, \"name\": \"aws-cli\" } ], \"environment\": [ { \"name\": \"REFERENCE_URI\", \"value\": \"s3://<bucket-name>/reference/*\" }, { \"name\": \"INPUT_DATA_URI\", \"value\": \"s3://<bucket-name>/<sample-name>/fastq/*.fastq.gz\" }, { \"name\": \"OUTPUT_DATA_URI\", \"value\": \"s3://<bucket-name>/<sample-name>/aligned\" } ], \"mountPoints\": [ { \"containerPath\": \"/opt/work\", \"sourceVolume\": \"scratch\" }, { \"containerPath\": \"/opt/miniconda\", \"sourceVolume\": \"aws-cli\" } ], \"ulimits\": [] } } There are three key parts of the above definition to take note of. Command and Parameters The command is a list of strings that will be sent to the container. This is the same as the ... arguments that you would provide to a docker run mycontainer ... command. Parameters are placeholders that you define whose values are substituted when a job is submitted. In the case above a threads parameter is defined with a default value of 8 . The job definition's command references this parameter with Ref::threads . Note Parameter references in the command list must be separate strings - concatenation with other parameter references or static values is not allowed. Environment Environment defines a set of environment variables that will be available for the container. For example, you can define environment variables used by the container entrypoint script to identify data it needs to stage in. Volumes and Mount Points Together, volumes and mountPoints define what you would provide as using a -v hostpath:containerpath option to a docker run command. These can be used to map host directories with resources (e.g. data or tools) used by all containers. In the example above, a scratch volume is mapped so that the container can utilize a larger disk on the host. Also, a version of the AWS CLI installed with conda is mapped into the container - enabling the container to have access to it (e.g. so it can transfer data from S3 and back) with out explicitly building in. State Machine Batch Job Tasks AWS Step Functions has built-in integration with AWS Batch (and several other services ), and provides snippets of code to make developing your state-machine tasks easier. The corresponding state machine state for the bwa-mem Job definition above would look like the following: \"BwaMemTask\": { \"Type\": \"Task\", \"InputPath\": \"$\", \"ResultPath\": \"$.bwa-mem.status\", \"Resource\": \"arn:aws:states:::batch:submitJob.sync\", \"Parameters\": { \"JobDefinition\": \"arn:aws:batch:<region>:<account>:job-definition/bwa-mem:1\", \"JobName\": \"bwa-mem\", \"JobQueue\": \"<queue-arn>\", \"Parameters.$\": \"$.bwa-mem.parameters\", \"Environment\": [ {\"Name\": \"REFERENCE_URI\", \"Value.$\": \"$.bwa-mem.environment.REFERENCE_URI\"}, {\"Name\": \"INPUT_DATA_URI\", \"Value.$\": \"$.bwa-mem.environment.INPUT_DATA_URI\"}, {\"Name\": \"OUTPUT_DATA_URI\", \"Value.$\": \"$.bwa-mem.environment.OUTPUT_DATA_URI\"} ] }, \"Next\": \"NEXT_TASK_NAME\" } Inputs to a state machine that uses the above BwaMemTask would look like this: { \"bwa-mem\": { \"parameters\": { \"threads\": 8 }, \"environment\": { \"REFERENCE_URI\": \"s3://<bucket-name/><sample-name>/reference/*\", \"INPUT_DATA_URI\": \"s3://<bucket-name/><sample-name>/fastq/*.fastq.gz\", \"OUTPUT_DATA_URI\": \"s3://<bucket-name/><sample-name>/aligned\" } }, ... } When the Task state completes Step Functions will add information to a new status key under bwa-mem in the JSON object. The complete object will be passed on to the next state in the workflow. Example state machine The example workflow is a simple secondary analysis pipeline that converts raw FASTQ files into VCFs with variants called for a list of chromosomes. It uses the following open source based tools: bwa-mem : Burrows-Wheeler Aligner for aligning short sequence reads to a reference genome samtools : S equence A lignment M apping library for indexing and sorting aligned reads bcftools : B inary (V)ariant C all F ormat library for determining variants in sample reads relative to a reference genome Read alignment, sorting, and indexing occur sequentially by Step Functions Task States. Variant calls for chromosomes occur in parallel using a Step Functions Map State and sub-Task States therein. All tasks submit AWS Batch Jobs to perform computational work using containerized versions of the tools listed above. The tooling containers used by the workflow use a generic entrypoint script that wraps the underlying tool and handles S3 data staging. It uses the AWS CLI to transfer objects and environment variables to identify data inputs and outputs to stage. Running the workflow When the stack above completes, go to the outputs tab and copy the JSON string provided in StateMachineInput . The input JSON will look like the following, but with the values for queue and JOB_OUTPUT_PREFIX prepopulated with resource names specific to the stack created by the CloudFormation template above: { \"params\": { \"__comment__\": { \"replace values for `queue` and `environment.JOB_OUTPUT_PREFIX` with values that match your resources\": { \"queue\": \"Name or ARN of the AWS Batch Job Queue the workflow will use by default.\", \"environment.JOB_OUTPUT_PREFIX\": \"S3 URI (e.g. s3://bucket/prefix) you are using for workflow inputs and outputs.\" }, }, \"queue\": \"default\", \"environment\": { \"REFERENCE_NAME\": \"Homo_sapiens_assembly38\", \"SAMPLE_ID\": \"NIST7035\", \"SOURCE_DATA_PREFIX\": \"s3://aws-batch-genomics-shared/secondary-analysis/example-files/fastq\", \"JOB_OUTPUT_PREFIX\": \"s3://YOUR-BUCKET-NAME/PREFIX\", \"JOB_AWS_CLI_PATH\": \"/opt/miniconda/bin\" }, \"chromosomes\": [ \"chr19\", \"chr20\", \"chr21\", \"chr22\" ] } } Next head to the AWS Step Functions console and select the state-machine that was created. Click the \"Start Execution\" button. In the dialog that appears, paste the input JSON into the \"Input\" field, and click the \"Start Execution\" button. (A unique execution ID will be automatically generated). You will be taken to the execution tracking page where you can monitor the progress of your workflow. The example workflow references a small demo dataset and takes approximately 20-30 minutes to complete. Cost optimizing workflows Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Overview"},{"location":"orchestration/step-functions/step-functions-overview.html#aws-step-functions-for-genomics-workflows","text":"DEPRECATION NOTICE This site and related code are no longer actively maintained. This site will be disabled and the underlying Github repository will be archived on 2023-07-31 . This allows all code and assets presented here to remain publicly available for historical reference purposes only. For more up to date solutions to running Genomics workflows on AWS checkout: Amazon Omics - a fully managed service for storing, processing, and querying genomic, transcriptomic, and other omics data into insights. Omics Workflows provides fully managed execution of pre-packaged Ready2Run workflows or private workflows you create using WDL or Nextflow. Amazon Genomics CLI - an open source tool that automates deploying and running workflow engines in AWS. AGC uses the same architectural patterns described here (i.e. operating workflow engines with AWS Batch). It provides support for running WDL, Nextflow, Snakemake, and CWL based workflows. The AWS Step Functions service allows you to orchestrate other AWS services, such as Lambda, Batch, SNS, and Glue, making it easy to coordinate the components of distributed applications as a series of steps in a visual workflow. In the context of genomics workflows, the combination of AWS Step Functions with Batch and Lambda constitutes a robust, scalable, and serverless task orchestration solution.","title":"AWS Step Functions For Genomics Workflows"},{"location":"orchestration/step-functions/step-functions-overview.html#requirements","text":"To get started using AWS Step Functions for genomics workflows you'll need the following setup in your AWS account: A VPC with at least 2 subnets (preferrably ones that are private ) The Genomics Workflow Core Environment Containerized tools for your workflow steps like BWA-MEM, Samtools, BCFtools custom entrypoint scripts that uses AWS Batch supplied environment variables for configuration and data handling A Batch Job Definitions for your tools An IAM Role for AWS Step Functions that allows it to submit AWS Batch jobs The following will help you deploy these components","title":"Requirements"},{"location":"orchestration/step-functions/step-functions-overview.html#vpc","text":"If you are handling sensitive data in your genomics pipelines, we recommend using at least 2 private subnets for AWS Batch compute jobs. EC2 instances launched into private subnets do not have public IP addresses, and therefore cannot be directly accessed from the public internet. They can still retain internet access from within the VPC - e.g. to pull source code, retrive public datasets, or install required softwre - if networking is configured appropriately. If the target VPC you want to deploy into already has this, you can skip ahead. If not, you can use the CloudFormation template below, which uses the AWS VPC Quickstart , to create one meeting these requirements. Name Description Source Launch Stack VPC (Optional) Creates a new Virtual Private Cloud to use for your genomics workflow resources. cloud_download play_arrow","title":"VPC"},{"location":"orchestration/step-functions/step-functions-overview.html#genomics-workflow-core","text":"To launch the Genomics Workflow Core in your AWS account, use the CloudFormation template below. Name Description Source Launch Stack Genomics Workflow Core Create EC2 Launch Templates, AWS Batch Job Queues and Compute Environments, a secure Amazon S3 bucket, and IAM policies and roles within an existing VPC. NOTE: You must provide VPC ID, and subnet IDs . cloud_download play_arrow The core is agnostic of the workflow orchestrator you intended to use, and can be installed multiple times in your account if needed (e.g. for use by different projects). Each installation uses a Namespace value to group resources accordingly. By default, the Namespace is set to the stack name, which must be unique within an AWS region. See the Core Environment For more details on the core's architecture.","title":"Genomics Workflow Core"},{"location":"orchestration/step-functions/step-functions-overview.html#step-functions-resources","text":"The the following CloudFormation template will create an AWS Step Functions State Machine that defines an example variant calling workflow using BWA-MEM, Samtools, and BCFtools; container images and AWS Batch Job Definitions for the tooling; and an IAM Role that allows AWS Step Functions to call AWS Batch during State Machine executions: Name Description Source Launch Stack AWS Step Functions Resources Create a Step Functions State Machine, Batch Job Definitions, and container images to run an example genomics workflow cloud_download play_arrow","title":"Step Functions Resources"},{"location":"orchestration/step-functions/step-functions-overview.html#deployment-details","text":"","title":"Deployment Details"},{"location":"orchestration/step-functions/step-functions-overview.html#aws-step-functions-execution-role","text":"An AWS Step Functions Execution role is an IAM role that allows Step Functions to execute other AWS services via the state machine. This can be created automatically during the \"first-run\" experience in the AWS Step Functions console when you create your first state machine. The policy attached to the role will depend on the specifc tasks you incorporate into your state machine. State machines that use AWS Batch for job execution and send events to CloudWatch should have an Execution role with the following inline policy: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"enable submitting batch jobs\", \"Effect\": \"Allow\", \"Action\": [ \"batch:SubmitJob\", \"batch:DescribeJobs\", \"batch:TerminateJob\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": [ \"events:PutTargets\", \"events:PutRule\", \"events:DescribeRule\" ], \"Resource\": [ \"arn:aws:events:<region>:<account-number>:rule/StepFunctionsGetEventsForBatchJobsRule\" ] } ] } For more complex workflows that use nested workflows or require more complex input parsing, you need to add additional permissions for executing Step Functions State Machines and invoking Lambda functions: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"enable calling lambda functions\", \"Effect\": \"Allow\", \"Action\": [ \"lambda:InvokeFunction\" ], \"Resource\": \"*\" }, { \"Sid\": \"enable calling other step functions\", \"Effect\": \"Allow\", \"Action\": [ \"states:StartExecution\" ], \"Resource\": \"*\" }, ... ] } Note All Resource values in the policy statements above can be scoped to be more specific if needed.","title":"AWS Step Functions Execution Role"},{"location":"orchestration/step-functions/step-functions-overview.html#step-functions-state-machines","text":"Workflows in AWS Step Functions are built using Amazon States Language (ASL), a declarative, JSON-based, structured language used to define a \"state-machine\". An AWS Step Functions State-Machine is a collection of states that can do work (Task states), determine which states to transition to next (Choice states), stop an execution with an error (Fail states), and so on.","title":"Step Functions State Machines"},{"location":"orchestration/step-functions/step-functions-overview.html#building-workflows-with-aws-step-functions","text":"The overall structure of a state-machine looks like the following: { \"Comment\": \"Description of you state-machine\", \"StartAt\": \"FirstState\", \"States\": { \"FirstState\": { \"Type\": \"<state-type>\", \"Next\": \"<name of next state>\" }, \"State1\" : { ... }, ... \"StateN\" : { ... }, \"LastState\": { ... \"End\": true } } } A simple \"Hello World\" state-machine looks like this: { \"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\", \"StartAt\": \"HelloWorld\", \"States\": { \"HelloWorld\": { \"Type\": \"Pass\", \"Result\": \"Hello World!\", \"End\": true } } ASL supports several task types and simple structures that can be combined to form a wide variety of complex workflows. More detailed coverage of ASL state types and structures is provided in the Step Functions ASL documentation .","title":"Building workflows with AWS Step Functions"},{"location":"orchestration/step-functions/step-functions-overview.html#batch-job-definitions","text":"AWS Batch Job Definitions are used to define compute resource requirements and parameter defaults for an AWS Batch Job. These are then referenced in state machine Task states by their respective ARNs. An example Job Definition for the bwa-mem sequence aligner is shown below: { \"jobDefinitionName\": \"bwa-mem\", \"type\": \"container\", \"parameters\": { \"threads\": \"8\" }, \"containerProperties\": { \"image\": \"<dockerhub-user>/bwa-mem:latest\", \"vcpus\": 8, \"memory\": 32000, \"command\": [ \"bwa\", \"mem\", \"-t\", \"Ref::threads\", \"-p\", \"reference.fasta\", \"sample_1.fastq.gz\" ], \"volumes\": [ { \"host\": { \"sourcePath\": \"/scratch\" }, \"name\": \"scratch\" }, { \"host\": { \"sourcePath\": \"/opt/miniconda\" }, \"name\": \"aws-cli\" } ], \"environment\": [ { \"name\": \"REFERENCE_URI\", \"value\": \"s3://<bucket-name>/reference/*\" }, { \"name\": \"INPUT_DATA_URI\", \"value\": \"s3://<bucket-name>/<sample-name>/fastq/*.fastq.gz\" }, { \"name\": \"OUTPUT_DATA_URI\", \"value\": \"s3://<bucket-name>/<sample-name>/aligned\" } ], \"mountPoints\": [ { \"containerPath\": \"/opt/work\", \"sourceVolume\": \"scratch\" }, { \"containerPath\": \"/opt/miniconda\", \"sourceVolume\": \"aws-cli\" } ], \"ulimits\": [] } } There are three key parts of the above definition to take note of. Command and Parameters The command is a list of strings that will be sent to the container. This is the same as the ... arguments that you would provide to a docker run mycontainer ... command. Parameters are placeholders that you define whose values are substituted when a job is submitted. In the case above a threads parameter is defined with a default value of 8 . The job definition's command references this parameter with Ref::threads . Note Parameter references in the command list must be separate strings - concatenation with other parameter references or static values is not allowed. Environment Environment defines a set of environment variables that will be available for the container. For example, you can define environment variables used by the container entrypoint script to identify data it needs to stage in. Volumes and Mount Points Together, volumes and mountPoints define what you would provide as using a -v hostpath:containerpath option to a docker run command. These can be used to map host directories with resources (e.g. data or tools) used by all containers. In the example above, a scratch volume is mapped so that the container can utilize a larger disk on the host. Also, a version of the AWS CLI installed with conda is mapped into the container - enabling the container to have access to it (e.g. so it can transfer data from S3 and back) with out explicitly building in.","title":"Batch Job Definitions"},{"location":"orchestration/step-functions/step-functions-overview.html#state-machine-batch-job-tasks","text":"AWS Step Functions has built-in integration with AWS Batch (and several other services ), and provides snippets of code to make developing your state-machine tasks easier. The corresponding state machine state for the bwa-mem Job definition above would look like the following: \"BwaMemTask\": { \"Type\": \"Task\", \"InputPath\": \"$\", \"ResultPath\": \"$.bwa-mem.status\", \"Resource\": \"arn:aws:states:::batch:submitJob.sync\", \"Parameters\": { \"JobDefinition\": \"arn:aws:batch:<region>:<account>:job-definition/bwa-mem:1\", \"JobName\": \"bwa-mem\", \"JobQueue\": \"<queue-arn>\", \"Parameters.$\": \"$.bwa-mem.parameters\", \"Environment\": [ {\"Name\": \"REFERENCE_URI\", \"Value.$\": \"$.bwa-mem.environment.REFERENCE_URI\"}, {\"Name\": \"INPUT_DATA_URI\", \"Value.$\": \"$.bwa-mem.environment.INPUT_DATA_URI\"}, {\"Name\": \"OUTPUT_DATA_URI\", \"Value.$\": \"$.bwa-mem.environment.OUTPUT_DATA_URI\"} ] }, \"Next\": \"NEXT_TASK_NAME\" } Inputs to a state machine that uses the above BwaMemTask would look like this: { \"bwa-mem\": { \"parameters\": { \"threads\": 8 }, \"environment\": { \"REFERENCE_URI\": \"s3://<bucket-name/><sample-name>/reference/*\", \"INPUT_DATA_URI\": \"s3://<bucket-name/><sample-name>/fastq/*.fastq.gz\", \"OUTPUT_DATA_URI\": \"s3://<bucket-name/><sample-name>/aligned\" } }, ... } When the Task state completes Step Functions will add information to a new status key under bwa-mem in the JSON object. The complete object will be passed on to the next state in the workflow.","title":"State Machine Batch Job Tasks"},{"location":"orchestration/step-functions/step-functions-overview.html#example-state-machine","text":"The example workflow is a simple secondary analysis pipeline that converts raw FASTQ files into VCFs with variants called for a list of chromosomes. It uses the following open source based tools: bwa-mem : Burrows-Wheeler Aligner for aligning short sequence reads to a reference genome samtools : S equence A lignment M apping library for indexing and sorting aligned reads bcftools : B inary (V)ariant C all F ormat library for determining variants in sample reads relative to a reference genome Read alignment, sorting, and indexing occur sequentially by Step Functions Task States. Variant calls for chromosomes occur in parallel using a Step Functions Map State and sub-Task States therein. All tasks submit AWS Batch Jobs to perform computational work using containerized versions of the tools listed above. The tooling containers used by the workflow use a generic entrypoint script that wraps the underlying tool and handles S3 data staging. It uses the AWS CLI to transfer objects and environment variables to identify data inputs and outputs to stage.","title":"Example state machine"},{"location":"orchestration/step-functions/step-functions-overview.html#running-the-workflow","text":"When the stack above completes, go to the outputs tab and copy the JSON string provided in StateMachineInput . The input JSON will look like the following, but with the values for queue and JOB_OUTPUT_PREFIX prepopulated with resource names specific to the stack created by the CloudFormation template above: { \"params\": { \"__comment__\": { \"replace values for `queue` and `environment.JOB_OUTPUT_PREFIX` with values that match your resources\": { \"queue\": \"Name or ARN of the AWS Batch Job Queue the workflow will use by default.\", \"environment.JOB_OUTPUT_PREFIX\": \"S3 URI (e.g. s3://bucket/prefix) you are using for workflow inputs and outputs.\" }, }, \"queue\": \"default\", \"environment\": { \"REFERENCE_NAME\": \"Homo_sapiens_assembly38\", \"SAMPLE_ID\": \"NIST7035\", \"SOURCE_DATA_PREFIX\": \"s3://aws-batch-genomics-shared/secondary-analysis/example-files/fastq\", \"JOB_OUTPUT_PREFIX\": \"s3://YOUR-BUCKET-NAME/PREFIX\", \"JOB_AWS_CLI_PATH\": \"/opt/miniconda/bin\" }, \"chromosomes\": [ \"chr19\", \"chr20\", \"chr21\", \"chr22\" ] } } Next head to the AWS Step Functions console and select the state-machine that was created. Click the \"Start Execution\" button. In the dialog that appears, paste the input JSON into the \"Input\" field, and click the \"Start Execution\" button. (A unique execution ID will be automatically generated). You will be taken to the execution tracking page where you can monitor the progress of your workflow. The example workflow references a small demo dataset and takes approximately 20-30 minutes to complete.","title":"Running the workflow"},{"location":"orchestration/step-functions/step-functions-overview.html#cost-optimizing-workflows","text":"Optimizing the allocation of resources to your workflows can help you to reduce costs","title":"Cost optimizing workflows"}]}